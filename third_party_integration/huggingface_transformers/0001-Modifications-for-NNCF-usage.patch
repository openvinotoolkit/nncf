From d333a563fd28c538f93217af3d7556141d19d51b Mon Sep 17 00:00:00 2001
From: skholkin <kholkinsd@gmail.com>
Date: Fri, 4 Mar 2022 14:03:05 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 examples/pytorch/language-modeling/run_clm.py |  75 ++++++++---
 examples/pytorch/question-answering/run_qa.py |  63 ++++++++--
 .../pytorch/text-classification/run_glue.py   | 107 ++++++++++++++--
 .../pytorch/text-classification/run_xnli.py   |  70 +++++++++--
 .../pytorch/token-classification/run_ner.py   | 118 ++++++++++++++----
 nncf_bert_config_conll.json                   |  44 +++++++
 nncf_bert_config_mrpc.json                    |  42 +++++++
 nncf_bert_config_squad.json                   |  44 +++++++
 nncf_bert_config_squad_kd.json                |  50 ++++++++
 ...config_squad_magnitude_sparsity_cubic.json |  31 +++++
 nncf_bert_config_xnli.json                    |  36 ++++++
 nncf_distilbert_config_sst2.json              |  34 +++++
 nncf_gpt2_config_wikitext_hw_config.json      |  58 +++++++++
 nncf_mobilebert_config_squad_int8.json        |  46 +++++++
 nncf_roberta_config_mnli.json                 |  36 ++++++
 src/transformers/file_utils.py                |   1 +
 src/transformers/modeling_utils.py            |  29 ++++-
 src/transformers/trainer.py                   |  74 +++++++++--
 src/transformers/trainer_callback.py          |   2 +
 src/transformers/training_args.py             |   6 +
 20 files changed, 893 insertions(+), 73 deletions(-)
 create mode 100644 nncf_bert_config_conll.json
 create mode 100644 nncf_bert_config_mrpc.json
 create mode 100644 nncf_bert_config_squad.json
 create mode 100644 nncf_bert_config_squad_kd.json
 create mode 100644 nncf_bert_config_squad_magnitude_sparsity_cubic.json
 create mode 100644 nncf_bert_config_xnli.json
 create mode 100644 nncf_distilbert_config_sst2.json
 create mode 100644 nncf_gpt2_config_wikitext_hw_config.json
 create mode 100644 nncf_mobilebert_config_squad_int8.json
 create mode 100644 nncf_roberta_config_mnli.json

diff --git a/examples/pytorch/language-modeling/run_clm.py b/examples/pytorch/language-modeling/run_clm.py
index 0c002deeb..f9e4f4171 100755
--- a/examples/pytorch/language-modeling/run_clm.py
+++ b/examples/pytorch/language-modeling/run_clm.py
@@ -29,6 +29,8 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import onnx
+import torch
 from datasets import load_dataset
 
 import transformers
@@ -48,7 +50,12 @@ from transformers.testing_utils import CaptureLogger
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
+from transformers.trainer import get_train_dataloader_for_init
 
+from nncf import NNCFConfig
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -334,21 +341,7 @@ def main():
             "You can do it from another script, save it, and load it from here, using --tokenizer_name."
         )
 
-    if model_args.model_name_or_path:
-        model = AutoModelForCausalLM.from_pretrained(
-            model_args.model_name_or_path,
-            from_tf=bool(".ckpt" in model_args.model_name_or_path),
-            config=config,
-            cache_dir=model_args.cache_dir,
-            revision=model_args.model_revision,
-            use_auth_token=True if model_args.use_auth_token else None,
-        )
-    else:
-        model = AutoModelForCausalLM.from_config(config)
-        n_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())
-        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")
 
-    model.resize_token_embeddings(len(tokenizer))
 
     # Preprocessing the datasets.
     # First we tokenize all the texts.
@@ -444,6 +437,59 @@ def main():
         if data_args.max_eval_samples is not None:
             eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset,
+                                                             default_data_collator)
+
+            class WikitextInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(WikitextInitializingDataLoader(train_dataloader)),
+                BNAdaptationInitArgs(WikitextInitializingDataLoader(train_dataloader)),
+            ])
+
+    if model_args.model_name_or_path:
+        retval = AutoModelForCausalLM.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+            nncf_config=nncf_config,
+            nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+        )
+    else:
+        retval = AutoModelForCausalLM.from_config(config)
+        n_params = sum(dict((p.data_ptr(), p.numel()) for p in retval.parameters()).values())
+        logger.info(f"Training new model from scratch - Total size={n_params / 2 ** 20:.2f}M params")
+
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    model.resize_token_embeddings(len(tokenizer))
+
+    if training_args.to_onnx:
+        if nncf_config is not None:
+           compression_ctrl.export_model(training_args.to_onnx)
+        else:
+           model.to('cpu')
+           dummy_tensor = torch.ones([1, config.n_positions], dtype=torch.long)
+           onnx.export(model, dummy_tensor, training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -453,6 +499,7 @@ def main():
         tokenizer=tokenizer,
         # Data collator will default to DataCollatorWithPadding, so we change it.
         data_collator=default_data_collator,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index b8559bb72..0d15131c2 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -25,6 +25,7 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import torch
 from datasets import load_dataset, load_metric
 
 import transformers
@@ -41,11 +42,19 @@ from transformers import (
     default_data_collator,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
 from utils_qa import postprocess_qa_predictions
 
+from torch import onnx
+
+from nncf import NNCFConfig
+from nncf.torch.initialization import PTInitializingDataLoader
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -298,14 +307,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForQuestionAnswering.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -555,6 +556,51 @@ def main():
     def compute_metrics(p: EvalPrediction):
         return metric.compute(predictions=p.predictions, references=p.label_ids)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset, data_collator)
+            class SquadInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(SquadInitializingDataloader(train_dataloader)),
+                BNAdaptationInitArgs(SquadInitializingDataloader(train_dataloader)),
+            ])
+
+    retval = AutoModelForQuestionAnswering.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 384], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = QuestionAnsweringTrainer(
         model=model,
@@ -566,6 +612,7 @@ def main():
         data_collator=data_collator,
         post_process_function=post_processing_function,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/pytorch/text-classification/run_glue.py b/examples/pytorch/text-classification/run_glue.py
index 6e66af923..bfb9503f5 100755
--- a/examples/pytorch/text-classification/run_glue.py
+++ b/examples/pytorch/text-classification/run_glue.py
@@ -28,6 +28,10 @@ import numpy as np
 from datasets import load_dataset, load_metric
 
 import transformers
+from nncf import NNCFConfig
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -41,6 +45,7 @@ from transformers import (
     default_data_collator,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
@@ -327,14 +332,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Preprocessing the raw_datasets
     if data_args.task_name is not None:
@@ -360,12 +357,12 @@ def main():
     # Some models have set the order of the labels to use, so let's make sure we do use it.
     label_to_id = None
     if (
-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
+        config.label2id != PretrainedConfig(num_labels=num_labels).label2id
         and data_args.task_name is not None
         and not is_regression
     ):
         # Some have all caps in their config, some don't.
-        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
+        label_name_to_id = {k.lower(): v for k, v in config.label2id.items()}
         if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
             label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}
         else:
@@ -378,8 +375,8 @@ def main():
         label_to_id = {v: i for i, v in enumerate(label_list)}
 
     if label_to_id is not None:
-        model.config.label2id = label_to_id
-        model.config.id2label = {id: label for label, id in config.label2id.items()}
+        config.label2id = label_to_id
+        config.id2label = {id: label for label, id in config.label2id.items()}
 
     if data_args.max_seq_length > tokenizer.model_max_length:
         logger.warning(
@@ -414,6 +411,87 @@ def main():
         if data_args.max_train_samples is not None:
             train_dataset = train_dataset.select(range(data_args.max_train_samples))
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args,
+                                                             train_dataset,
+                                                             data_collator=default_data_collator)
+
+            class SST2InitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "labels": dataloader_output["labels"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "input_ids": dataloader_output["input_ids"]
+                    }
+
+            class MRPCInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "labels": dataloader_output["labels"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "input_ids": dataloader_output["input_ids"],
+                        "token_type_ids": dataloader_output["token_type_ids"]
+                    }
+
+            class MNLIInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "labels": dataloader_output["labels"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "input_ids": dataloader_output["input_ids"]
+                    }
+
+            if data_args.task_name == "sst2":
+                initializing_data_loader_cls = SST2InitializingDataLoader
+            elif data_args.task_name == "mrpc":
+                initializing_data_loader_cls = MRPCInitializingDataLoader
+            elif data_args.task_name == "mnli":
+                initializing_data_loader_cls = MNLIInitializingDataLoader
+            initializing_data_loader = initializing_data_loader_cls(train_dataloader)
+            nncf_config.register_extra_structs([QuantizationRangeInitArgs(initializing_data_loader),
+                                                BNAdaptationInitArgs(initializing_data_loader)])
+
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            import torch
+            from torch import onnx
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor),
+                        training_args.to_onnx, opset_version=10)
+
     if training_args.do_eval:
         if "validation" not in raw_datasets and "validation_matched" not in raw_datasets:
             raise ValueError("--do_eval requires a validation dataset")
@@ -471,8 +549,13 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+    if nncf_config is not None:
+        if not (training_args.local_rank == -1 or training_args.no_cuda):
+            compression_ctrl.distributed()
+
     # Training
     if training_args.do_train:
         checkpoint = None
diff --git a/examples/pytorch/text-classification/run_xnli.py b/examples/pytorch/text-classification/run_xnli.py
index d7e5dfd83..6947317f8 100755
--- a/examples/pytorch/text-classification/run_xnli.py
+++ b/examples/pytorch/text-classification/run_xnli.py
@@ -26,9 +26,15 @@ from typing import Optional
 
 import datasets
 import numpy as np
+import torch
 from datasets import load_dataset, load_metric
 
 import transformers
+from nncf import NNCFConfig
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.torch import register_default_init_args
+from nncf.torch.initialization import PTInitializingDataLoader
+
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -41,6 +47,7 @@ from transformers import (
     default_data_collator,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
@@ -250,14 +257,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Preprocessing the datasets
     # Padding strategy
@@ -331,6 +330,56 @@ def main():
     else:
         data_collator = None
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args,
+                                                             train_dataset,
+                                                             data_collator=data_collator)
+
+            class KwargBasedInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            initializing_data_loader = KwargBasedInitializingDataloader(train_dataloader)
+            nncf_config = register_default_init_args(nncf_config, initializing_data_loader)
+
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, training_args.max_seq_length], dtype=torch.long)
+            torch.onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -340,8 +389,13 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+    if nncf_config is not None:
+        if not (training_args.local_rank == -1 or training_args.no_cuda):
+            compression_ctrl.distributed()
+
     # Training
     if training_args.do_train:
         checkpoint = None
diff --git a/examples/pytorch/token-classification/run_ner.py b/examples/pytorch/token-classification/run_ner.py
index 65c26cd9e..f3cf0f857 100755
--- a/examples/pytorch/token-classification/run_ner.py
+++ b/examples/pytorch/token-classification/run_ner.py
@@ -22,30 +22,39 @@ Fine-tuning the library models for token classification.
 import logging
 import os
 import sys
-from dataclasses import dataclass, field
+from copy import deepcopy
+from dataclasses import dataclass
+from dataclasses import field
+from typing import List
 from typing import Optional
 
 import datasets
 import numpy as np
-from datasets import ClassLabel, load_dataset, load_metric
-
+import torch
 import transformers
-from transformers import (
-    AutoConfig,
-    AutoModelForTokenClassification,
-    AutoTokenizer,
-    DataCollatorForTokenClassification,
-    HfArgumentParser,
-    PreTrainedTokenizerFast,
-    Trainer,
-    TrainingArguments,
-    set_seed,
-)
+from datasets import ClassLabel
+from datasets import load_dataset
+from datasets import load_metric
+from nncf import NNCFConfig
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
+from packaging import version
+from torch import onnx
+from transformers import AutoConfig
+from transformers import AutoModelForTokenClassification
+from transformers import AutoTokenizer
+from transformers import DataCollatorForTokenClassification
+from transformers import HfArgumentParser
+from transformers import PreTrainedTokenizerFast
+from transformers import Trainer
+from transformers import TrainingArguments
+from transformers import set_seed
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
 
-
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
 
@@ -177,6 +186,16 @@ class DataTrainingArguments:
         self.task_name = self.task_name.lower()
 
 
+def filter_columns(dataset, keep_columns: List[str], remove_columns: List[str]):
+    if version.parse(datasets.__version__) < version.parse("1.4.0"):
+        dataset.set_format(
+            type=dataset.format["type"], columns=keep_columns, format_kwargs=dataset.format["format_kwargs"]
+        )
+        return dataset
+    else:
+        return dataset.remove_columns(remove_columns)
+
+
 def main():
     # See all possible arguments in src/transformers/training_args.py
     # or by passing the --help flag to this script.
@@ -331,14 +350,7 @@ def main():
             use_auth_token=True if model_args.use_auth_token else None,
         )
 
-    model = AutoModelForTokenClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
+
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -432,6 +444,65 @@ def main():
     # Data collator
     data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataset_for_init = deepcopy(train_dataset)
+
+            train_dataset_for_init = filter_columns(train_dataset_for_init,
+                                                    keep_columns=['labels', 'input_ids', 'attention_mask',
+                                                                  'token_type_ids'],
+                                                    remove_columns=['ner_tags', 'pos_tags', 'tokens', 'id',
+                                                                    'chunk_tags'])
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset_for_init, data_collator)
+
+            class ConllInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "input_ids": dataloader_output["input_ids"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "token_type_ids": dataloader_output["token_type_ids"],
+                    }
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(ConllInitializingDataloader(train_dataloader)),
+                BNAdaptationInitArgs(ConllInitializingDataloader(train_dataloader)),
+            ])
+
+    retval = AutoModelForTokenClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx,
+                        opset_version=10)
+
     # Metrics
     metric = load_metric("seqeval")
 
@@ -477,6 +548,7 @@ def main():
         tokenizer=tokenizer,
         data_collator=data_collator,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/nncf_bert_config_conll.json b/nncf_bert_config_conll.json
new file mode 100644
index 000000000..0f73fe368
--- /dev/null
+++ b/nncf_bert_config_conll.json
@@ -0,0 +1,44 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 200
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true,
+            "per_channel": false
+        }
+    }
+}
diff --git a/nncf_bert_config_mrpc.json b/nncf_bert_config_mrpc.json
new file mode 100644
index 000000000..f4ecbeeed
--- /dev/null
+++ b/nncf_bert_config_mrpc.json
@@ -0,0 +1,42 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 64,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 200
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "per_channel": false
+        }
+    }
+}
diff --git a/nncf_bert_config_squad.json b/nncf_bert_config_squad.json
new file mode 100644
index 000000000..12e0440f7
--- /dev/null
+++ b/nncf_bert_config_squad.json
@@ -0,0 +1,44 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 200
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true,
+            "per_channel": false
+        }
+    }
+}
diff --git a/nncf_bert_config_squad_kd.json b/nncf_bert_config_squad_kd.json
new file mode 100644
index 000000000..f5872a0a2
--- /dev/null
+++ b/nncf_bert_config_squad_kd.json
@@ -0,0 +1,50 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": [{
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 200
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true,
+            "per_channel": false
+        }
+    },
+    {
+        "algorithm": "knowledge_distillation",
+        "type": "softmax",
+        "temperature": 3
+    }
+    ]
+}
diff --git a/nncf_bert_config_squad_magnitude_sparsity_cubic.json b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
new file mode 100644
index 000000000..b4452e8d4
--- /dev/null
+++ b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
@@ -0,0 +1,31 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "magnitude_sparsity",
+        "params": {
+            "schedule": "polynomial",
+            "power": 3,
+            "sparsity_init": 0.0,
+            "sparsity_target": 0.8,
+            "sparsity_target_epoch": 40,
+            "sparsity_freeze_epoch": 60,
+            "update_per_optimizer_step": true,
+            "steps_per_epoch": 1109,
+            "weight_importance": "abs"
+        },
+        "ignored_scopes": ["{re}.*NNCFEmbedding"]
+    }
+}
diff --git a/nncf_bert_config_xnli.json b/nncf_bert_config_xnli.json
new file mode 100644
index 000000000..a21a522fc
--- /dev/null
+++ b/nncf_bert_config_xnli.json
@@ -0,0 +1,36 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 96
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "{re}BertIntermediate\\[intermediate\\]/__mul___0",
+            "{re}BertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric"
+        }
+    }
+}
diff --git a/nncf_distilbert_config_sst2.json b/nncf_distilbert_config_sst2.json
new file mode 100644
index 000000000..868735016
--- /dev/null
+++ b/nncf_distilbert_config_sst2.json
@@ -0,0 +1,34 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "mean_percentile"
+            }
+        },
+        "ignored_scopes": [
+            "{re}TransformerBlock\\[[0-9]*\\]/FFN\\[ffn\\]/__mul___0",
+            "{re}TransformerBlock\\[[0-9]*\\]/FFN\\[ffn\\]/NNCFLinear\\[lin1\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_gpt2_config_wikitext_hw_config.json b/nncf_gpt2_config_wikitext_hw_config.json
new file mode 100644
index 000000000..4b2376133
--- /dev/null
+++ b/nncf_gpt2_config_wikitext_hw_config.json
@@ -0,0 +1,58 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 1024],
+            "type": "long"
+        }
+    ],
+    "hw_config_type": "cpu",
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 16,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            }
+        },
+        "ignored_scopes": [
+             //gelu_new with fusing into previous GEMM
+            "{re}.*MLP\\[mlp\\]/__rmul___0",
+            "{re}.*MLP\\[mlp\\]/__add___0",
+            "{re}.*MLP\\[mlp\\]/__rmul___1",
+            "{re}.*MLP\\[mlp\\]/tanh_0",
+            "{re}.*MLP\\[mlp\\]/__radd___0",
+            "{re}.*MLP\\[mlp\\]/__mul___0",
+
+            // Intermediate embedding sum results
+            "GPT2LMHeadModel/GPT2Model[transformer]/__add___0",
+            "GPT2LMHeadModel/GPT2Model[transformer]/__add___1",
+
+            // Scaling in attention
+            "{re}.*Attention\\[attn\\]/__truediv___0",
+
+            // Pre-LayerNorm additions
+            "{re}.*Block\\[[0-9]*\\]/__add___0",
+            "{re}.*Block\\[[0-9]*\\]/__add___1",
+
+            // Final LayerNorm inputs
+            "GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]",
+
+            // LM head
+            "GPT2LMHeadModel/NNCFLinear[lm_head]"
+        ],
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_mobilebert_config_squad_int8.json b/nncf_mobilebert_config_squad_int8.json
new file mode 100644
index 000000000..89504d2c7
--- /dev/null
+++ b/nncf_mobilebert_config_squad_int8.json
@@ -0,0 +1,46 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 64,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            }
+        },
+        "ignored_scopes": ["{re}MobileBertSelfAttention\\[self\\]/__add___0",
+            "{re}MobileBertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"],
+        "activations":
+        {
+            "mode": "symmetric",
+            "ignored_scopes": [
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___0",
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___1",
+            "{re}MobileBertOutput\\[output\\]/__add___0",
+            "{re}NoNorm\\[LayerNorm\\]/__mul___0"]
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_roberta_config_mnli.json b/nncf_roberta_config_mnli.json
new file mode 100644
index 000000000..edbe0f84d
--- /dev/null
+++ b/nncf_roberta_config_mnli.json
@@ -0,0 +1,36 @@
+{
+    "input_info": [
+        {
+            "keyword": "input_ids",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        },
+        {
+            "keyword": "attention_mask",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 24
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]",
+            "RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[dense]"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "asymmetric"
+        }
+    }
+}
diff --git a/src/transformers/file_utils.py b/src/transformers/file_utils.py
index bc2b29354..f547fc708 100644
--- a/src/transformers/file_utils.py
+++ b/src/transformers/file_utils.py
@@ -244,6 +244,7 @@ SESSION_ID = uuid4().hex
 DISABLE_TELEMETRY = os.getenv("DISABLE_TELEMETRY", False) in ENV_VARS_TRUE_VALUES
 
 WEIGHTS_NAME = "pytorch_model.bin"
+NNCF_PT_STATE_NAME = "nncf_state.bin"
 TF2_WEIGHTS_NAME = "tf_model.h5"
 TF_WEIGHTS_NAME = "model.ckpt"
 FLAX_WEIGHTS_NAME = "flax_model.msgpack"
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index b34637b02..29ae259d6 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -23,8 +23,10 @@ from dataclasses import dataclass
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
 import torch
+from nncf.torch import create_compressed_model
 from torch import Tensor, device, nn
 from torch.nn import CrossEntropyLoss
+from transformers.file_utils import NNCF_PT_STATE_NAME
 
 from .activations import get_activation
 from .configuration_utils import PretrainedConfig
@@ -923,6 +925,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         state_dict: Optional[dict] = None,
         save_function: Callable = torch.save,
         push_to_hub: bool = False,
+        nncf_compression_state: Dict = None,
         **kwargs,
     ):
         """
@@ -994,6 +997,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         output_model_file = os.path.join(save_directory, WEIGHTS_NAME)
         save_function(state_dict, output_model_file)
 
+        if nncf_compression_state is not None:
+            nncf_state_output_file = os.path.join(save_directory, NNCF_PT_STATE_NAME)
+            save_function(nncf_compression_state, nncf_state_output_file)
+
         logger.info(f"Model weights saved in {output_model_file}")
 
         if push_to_hub:
@@ -1166,6 +1173,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         from_auto_class = kwargs.pop("_from_auto", False)
         _fast_init = kwargs.pop("_fast_init", True)
         torch_dtype = kwargs.pop("torch_dtype", None)
+        nncf_config = kwargs.pop("nncf_config", None)
+        nncf_eval = kwargs.pop("nncf_eval", False)
 
         from_pt = not (from_tf | from_flax)
 
@@ -1349,6 +1358,11 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 )
                 raise
         elif from_pt:
+            if nncf_config is not None and nncf_eval:
+                compression_algo_controller, model = create_compressed_model(model, nncf_config,
+                                                                             compression_state=state_dict)
+                return compression_algo_controller, model
+
             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_state_dict_into_model(
                 model,
                 state_dict,
@@ -1363,6 +1377,18 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         # Set model in evaluation mode to deactivate DropOut modules by default
         model.eval()
 
+        if nncf_config is not None:
+            compression_state = None
+
+            compression_state_file = os.path.join(pretrained_model_name_or_path, NNCF_PT_STATE_NAME)
+            if os.path.isfile(compression_state_file):
+                compression_state = torch.load(compression_state_file)
+            else:
+                compression_state = None
+            compression_algo_controller, model = create_compressed_model(model, nncf_config,
+                                                                         compression_state=compression_state)
+            return compression_algo_controller, model
+
         if output_loading_info:
             loading_info = {
                 "missing_keys": missing_keys,
@@ -1562,7 +1588,8 @@ PreTrainedModel.push_to_hub.__doc__ = PreTrainedModel.push_to_hub.__doc__.format
     object="model", object_class="AutoModel", object_files="model checkpoint"
 )
 
-
+import nncf
+@nncf.torch.register_module()
 class Conv1D(nn.Module):
     """
     1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index d6e0d51c4..dfb3147f9 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -30,6 +30,7 @@ from logging import StreamHandler
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
 
+from nncf.torch.nncf_network import NNCFNetwork
 from tqdm.auto import tqdm
 
 
@@ -47,6 +48,8 @@ from .integrations import (  # isort: split
 
 import numpy as np
 import torch
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from packaging import version
 from torch import nn
 from torch.utils.data.dataloader import DataLoader
@@ -185,6 +188,29 @@ if TYPE_CHECKING:
 logger = logging.get_logger(__name__)
 
 
+def get_train_dataloader_for_init(args, train_dataset, data_collator=None):
+    from torch.utils.data import RandomSampler
+    from torch.utils.data import DistributedSampler
+    train_sampler = (
+        RandomSampler(train_dataset)
+        if args.local_rank == -1
+        else DistributedSampler(train_dataset)
+    )
+
+    if data_collator is None:
+        from transformers.data.data_collator import default_data_collator
+        data_collator = default_data_collator
+
+    from torch.utils.data import DataLoader
+    data_loader = DataLoader(
+        train_dataset,
+        batch_size=args.train_batch_size,
+        sampler=train_sampler,
+        collate_fn=data_collator,
+        drop_last=args.dataloader_drop_last,
+    )
+    return data_loader
+
 class Trainer:
     """
     Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.
@@ -274,12 +300,15 @@ class Trainer:
         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
         callbacks: Optional[List[TrainerCallback]] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
+        compression_ctrl: PTCompressionAlgorithmController = None
     ):
         if args is None:
             output_dir = "tmp_trainer"
             logger.info(f"No `TrainingArguments` passed, using `output_dir={output_dir}`.")
             args = TrainingArguments(output_dir=output_dir)
         self.args = args
+
+        self.compression_ctrl = compression_ctrl
         # Seed must be set before instantiating the model when using model
         set_seed(self.args.seed)
         self.hp_name = None
@@ -510,7 +539,10 @@ class Trainer:
             return dataset
         if self._signature_columns is None:
             # Inspect model forward signature to keep only the arguments it accepts.
-            signature = inspect.signature(self.model.forward)
+            if isinstance(self.model, NNCFNetwork):
+                signature = inspect.signature(self.model.get_nncf_wrapped_model().forward)
+            else:
+                signature = inspect.signature(self.model.forward)
             self._signature_columns = list(signature.parameters.keys())
             # Labels may be named label or label_ids, the default data collator handles that.
             self._signature_columns += ["label", "label_ids"]
@@ -972,6 +1004,9 @@ class Trainer:
                 find_unused_parameters = not getattr(model.config, "gradient_checkpointing", False)
             else:
                 find_unused_parameters = True
+
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
             model = nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank],
@@ -1231,11 +1266,13 @@ class Trainer:
                     break
 
         for epoch in range(epochs_trained, num_train_epochs):
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.scheduler.epoch_step()
+                print(self.compression_ctrl.statistics().to_str())
             if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                 train_dataloader.sampler.set_epoch(epoch)
             elif isinstance(train_dataloader.dataset, IterableDatasetShard):
                 train_dataloader.dataset.set_epoch(epoch)
-
             if is_torch_tpu_available():
                 parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)
                 epoch_iterator = parallel_loader
@@ -1275,9 +1312,11 @@ class Trainer:
                 ):
                     # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.
                     with model.no_sync():
-                        tr_loss += self.training_step(model, inputs)
+                        curr_loss = self.training_step(model, inputs)
                 else:
-                    tr_loss += self.training_step(model, inputs)
+                    curr_loss = self.training_step(model, inputs)
+
+                tr_loss += curr_loss
                 self.current_flos += float(self.floating_point_ops(inputs))
 
                 # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps
@@ -1311,6 +1350,8 @@ class Trainer:
                             )
 
                     # Optimizer step
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.scheduler.step()
                     optimizer_was_run = True
                     if self.deepspeed:
                         pass  # called outside the loop
@@ -1331,6 +1372,7 @@ class Trainer:
                     model.zero_grad()
                     self.state.global_step += 1
                     self.state.epoch = epoch + (step + 1) / steps_in_epoch
+                    self.state.curr_loss = curr_loss.cpu().detach().item()
                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
 
                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
@@ -1426,6 +1468,13 @@ class Trainer:
             logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
             logs["learning_rate"] = self._get_learning_rate()
 
+            if self.compression_ctrl is not None:
+                logs["compression_loss"] = self.compression_ctrl.loss().item()
+                compression_stats = self.compression_ctrl.statistics()
+                for key, value in prepare_for_tensorboard(compression_stats).items():
+                    logs["compression/statistics/{0}".format(key)] = value
+                print(compression_stats.to_str())
+
             self._total_loss_scalar += tr_loss_scalar
             self._globalstep_last_logged = self.state.global_step
             self.store_flos()
@@ -1779,6 +1828,10 @@ class Trainer:
             # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`
             loss = loss / self.args.gradient_accumulation_steps
 
+        if self.compression_ctrl is not None:
+            compression_loss = self.compression_ctrl.loss()
+            loss += compression_loss
+
         if self.use_amp:
             self.scaler.scale(loss).backward()
         elif self.use_apex:
@@ -1917,13 +1970,20 @@ class Trainer:
         output_dir = output_dir if output_dir is not None else self.args.output_dir
         os.makedirs(output_dir, exist_ok=True)
         logger.info(f"Saving model checkpoint to {output_dir}")
+
+
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         if not isinstance(self.model, PreTrainedModel):
-            if isinstance(unwrap_model(self.model), PreTrainedModel):
+            unwrapped_model = unwrap_model(self.model)
+            if isinstance(unwrapped_model, NNCFNetwork):
+                is_pretrained = isinstance(unwrapped_model.get_nncf_wrapped_model(), PreTrainedModel)
+            else:
+                is_pretrained = isinstance(unwrapped_model, PreTrainedModel)
+            if is_pretrained:
                 if state_dict is None:
-                    state_dict = self.model.state_dict()
-                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)
+                    state_dict = unwrapped_model.state_dict()
+                unwrapped_model.save_pretrained(output_dir, state_dict=state_dict)
             else:
                 logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                 if state_dict is None:
diff --git a/src/transformers/trainer_callback.py b/src/transformers/trainer_callback.py
index 023418b3f..9962462ac 100644
--- a/src/transformers/trainer_callback.py
+++ b/src/transformers/trainer_callback.py
@@ -459,6 +459,8 @@ class ProgressCallback(TrainerCallback):
     def on_step_end(self, args, state, control, **kwargs):
         if state.is_local_process_zero:
             self.training_bar.update(state.global_step - self.current_step)
+            if hasattr(state, "curr_loss"):
+                self.training_bar.set_postfix(loss=state.curr_loss)
             self.current_step = state.global_step
 
     def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 404d92a22..97d8d4483 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -625,6 +625,12 @@ class TrainingArguments:
         metadata={"help": "Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer"},
     )
 
+    nncf_config: str = field(default=None,
+                             metadata={"help": "NNCF configuration .json file for compression-enabled training"})
+
+    to_onnx: str = field(default=None,
+                         metadata={"help": "Name of the ONNX model file to export the model to."})
+
     def __post_init__(self):
         # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
         # This needs to happen before any call to self.device or self.n_gpu.
-- 
2.17.1

