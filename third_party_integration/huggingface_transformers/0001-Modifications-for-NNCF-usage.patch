From 82ce3063a27a11074529313df6255a69f6cf189f Mon Sep 17 00:00:00 2001
From: Nikolay Lyalyushkin <nikolay.lyalyushkin@intel.com>
Date: Tue, 18 Oct 2022 22:08:16 +0200
Subject: [PATCH] Modifications for NNCF usage

---
 examples/pytorch/language-modeling/run_clm.py |  77 +++++++++---
 examples/pytorch/question-answering/run_qa.py |  63 ++++++++--
 .../pytorch/text-classification/run_glue.py   | 112 +++++++++++++++---
 .../pytorch/text-classification/run_xnli.py   |  71 +++++++++--
 .../pytorch/token-classification/run_ner.py   | 106 ++++++++++++++---
 nncf_bert_config_conll.json                   |  44 +++++++
 nncf_bert_config_mrpc.json                    |  42 +++++++
 nncf_bert_config_squad.json                   |  44 +++++++
 ...config_squad_magnitude_sparsity_cubic.json |  31 +++++
 nncf_bert_config_xnli.json                    |  38 ++++++
 nncf_distilbert_config_sst2.json              |  33 ++++++
 nncf_gpt2_config_wikitext_hw_config.json      |  49 ++++++++
 nncf_mobilebert_config_squad_int8.json        |  49 ++++++++
 nncf_roberta_config_mnli.json                 |  35 ++++++
 src/transformers/modeling_utils.py            |  24 ++++
 src/transformers/pytorch_utils.py             |   3 +-
 src/transformers/trainer.py                   |  72 ++++++++++-
 src/transformers/training_args.py             |   6 +
 src/transformers/utils/__init__.py            |   1 +
 19 files changed, 827 insertions(+), 73 deletions(-)
 create mode 100644 nncf_bert_config_conll.json
 create mode 100644 nncf_bert_config_mrpc.json
 create mode 100644 nncf_bert_config_squad.json
 create mode 100644 nncf_bert_config_squad_magnitude_sparsity_cubic.json
 create mode 100644 nncf_bert_config_xnli.json
 create mode 100644 nncf_distilbert_config_sst2.json
 create mode 100644 nncf_gpt2_config_wikitext_hw_config.json
 create mode 100644 nncf_mobilebert_config_squad_int8.json
 create mode 100644 nncf_roberta_config_mnli.json

diff --git a/examples/pytorch/language-modeling/run_clm.py b/examples/pytorch/language-modeling/run_clm.py
index fe03cde7c..3f8cd6d37 100755
--- a/examples/pytorch/language-modeling/run_clm.py
+++ b/examples/pytorch/language-modeling/run_clm.py
@@ -30,6 +30,8 @@ from itertools import chain
 from typing import Optional
 
 import datasets
+import onnx
+import torch
 from datasets import load_dataset
 
 import evaluate
@@ -51,7 +53,12 @@ from transformers.testing_utils import CaptureLogger
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
+from transformers.trainer import get_train_dataloader_for_init
 
+from nncf import NNCFConfig
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.23.0")
@@ -373,22 +380,6 @@ def main():
             "You can do it from another script, save it, and load it from here, using --tokenizer_name."
         )
 
-    if model_args.model_name_or_path:
-        model = AutoModelForCausalLM.from_pretrained(
-            model_args.model_name_or_path,
-            from_tf=bool(".ckpt" in model_args.model_name_or_path),
-            config=config,
-            cache_dir=model_args.cache_dir,
-            revision=model_args.model_revision,
-            use_auth_token=True if model_args.use_auth_token else None,
-        )
-    else:
-        model = AutoModelForCausalLM.from_config(config)
-        n_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())
-        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")
-
-    model.resize_token_embeddings(len(tokenizer))
-
     # Preprocessing the datasets.
     # First we tokenize all the texts.
     if training_args.do_train:
@@ -503,6 +494,59 @@ def main():
             preds = preds[:, :-1].reshape(-1)
             return metric.compute(predictions=preds, references=labels)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset,
+                                                             default_data_collator)
+
+            class WikitextInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(WikitextInitializingDataLoader(train_dataloader)),
+                BNAdaptationInitArgs(WikitextInitializingDataLoader(train_dataloader)),
+            ])
+
+    if model_args.model_name_or_path:
+        retval = AutoModelForCausalLM.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+            nncf_config=nncf_config,
+            nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+        )
+    else:
+        retval = AutoModelForCausalLM.from_config(config)
+        n_params = sum(dict((p.data_ptr(), p.numel()) for p in retval.parameters()).values())
+        logger.info(f"Training new model from scratch - Total size={n_params / 2 ** 20:.2f}M params")
+
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    model.resize_token_embeddings(len(tokenizer))
+
+    if training_args.to_onnx:
+        if nncf_config is not None:
+           compression_ctrl.export_model(training_args.to_onnx)
+        else:
+           model.to('cpu')
+           dummy_tensor = torch.ones([1, config.n_positions], dtype=torch.long)
+           onnx.export(model, dummy_tensor, training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -516,6 +560,7 @@ def main():
         preprocess_logits_for_metrics=preprocess_logits_for_metrics
         if training_args.do_eval and not is_torch_tpu_available()
         else None,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index 1240623b5..b6136d6e3 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -25,6 +25,7 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import torch
 from datasets import load_dataset
 
 import evaluate
@@ -42,11 +43,19 @@ from transformers import (
     default_data_collator,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
 from utils_qa import postprocess_qa_predictions
 
+from torch import onnx
+
+from nncf import NNCFConfig
+from nncf.torch.initialization import PTInitializingDataLoader
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.23.0")
@@ -327,14 +336,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForQuestionAnswering.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -599,6 +600,51 @@ def main():
     def compute_metrics(p: EvalPrediction):
         return metric.compute(predictions=p.predictions, references=p.label_ids)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset, data_collator)
+            class SquadInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(SquadInitializingDataloader(train_dataloader)),
+                BNAdaptationInitArgs(SquadInitializingDataloader(train_dataloader)),
+            ])
+
+    retval = AutoModelForQuestionAnswering.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 384], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = QuestionAnsweringTrainer(
         model=model,
@@ -610,6 +656,7 @@ def main():
         data_collator=data_collator,
         post_process_function=post_processing_function,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/pytorch/text-classification/run_glue.py b/examples/pytorch/text-classification/run_glue.py
index 3eb423f08..f675e7ab4 100755
--- a/examples/pytorch/text-classification/run_glue.py
+++ b/examples/pytorch/text-classification/run_glue.py
@@ -29,6 +29,10 @@ from datasets import load_dataset
 
 import evaluate
 import transformers
+from nncf import NNCFConfig
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -42,6 +46,7 @@ from transformers import (
     default_data_collator,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
@@ -366,15 +371,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
-    )
 
     # Preprocessing the raw_datasets
     if data_args.task_name is not None:
@@ -400,12 +396,12 @@ def main():
     # Some models have set the order of the labels to use, so let's make sure we do use it.
     label_to_id = None
     if (
-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
+        config.label2id != PretrainedConfig(num_labels=num_labels).label2id
         and data_args.task_name is not None
         and not is_regression
     ):
         # Some have all caps in their config, some don't.
-        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
+        label_name_to_id = {k.lower(): v for k, v in config.label2id.items()}
         if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
             label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}
         else:
@@ -418,11 +414,11 @@ def main():
         label_to_id = {v: i for i, v in enumerate(label_list)}
 
     if label_to_id is not None:
-        model.config.label2id = label_to_id
-        model.config.id2label = {id: label for label, id in config.label2id.items()}
+        config.label2id = label_to_id
+        config.id2label = {id: label for label, id in config.label2id.items()}
     elif data_args.task_name is not None and not is_regression:
-        model.config.label2id = {l: i for i, l in enumerate(label_list)}
-        model.config.id2label = {id: label for label, id in config.label2id.items()}
+        config.label2id = {l: i for i, l in enumerate(label_list)}
+        config.id2label = {id: label for label, id in config.label2id.items()}
 
     if data_args.max_seq_length > tokenizer.model_max_length:
         logger.warning(
@@ -458,6 +454,87 @@ def main():
             max_train_samples = min(len(train_dataset), data_args.max_train_samples)
             train_dataset = train_dataset.select(range(max_train_samples))
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args,
+                                                             train_dataset,
+                                                             data_collator=default_data_collator)
+
+            class SST2InitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "labels": dataloader_output["labels"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "input_ids": dataloader_output["input_ids"]
+                    }
+
+            class MRPCInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "labels": dataloader_output["labels"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "input_ids": dataloader_output["input_ids"],
+                        "token_type_ids": dataloader_output["token_type_ids"]
+                    }
+
+            class MNLIInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "labels": dataloader_output["labels"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "input_ids": dataloader_output["input_ids"]
+                    }
+
+            if data_args.task_name == "sst2":
+                initializing_data_loader_cls = SST2InitializingDataLoader
+            elif data_args.task_name == "mrpc":
+                initializing_data_loader_cls = MRPCInitializingDataLoader
+            elif data_args.task_name == "mnli":
+                initializing_data_loader_cls = MNLIInitializingDataLoader
+            initializing_data_loader = initializing_data_loader_cls(train_dataloader)
+            nncf_config.register_extra_structs([QuantizationRangeInitArgs(initializing_data_loader),
+                                                BNAdaptationInitArgs(initializing_data_loader)])
+
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            import torch
+            from torch import onnx
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor),
+                        training_args.to_onnx, opset_version=10)
+
     if training_args.do_eval:
         if "validation" not in raw_datasets and "validation_matched" not in raw_datasets:
             raise ValueError("--do_eval requires a validation dataset")
@@ -518,8 +595,13 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+    if nncf_config is not None:
+        if not (training_args.local_rank == -1 or training_args.no_cuda):
+            compression_ctrl.distributed()
+
     # Training
     if training_args.do_train:
         checkpoint = None
diff --git a/examples/pytorch/text-classification/run_xnli.py b/examples/pytorch/text-classification/run_xnli.py
index 55523edfc..68a3ebe41 100755
--- a/examples/pytorch/text-classification/run_xnli.py
+++ b/examples/pytorch/text-classification/run_xnli.py
@@ -26,10 +26,16 @@ from typing import Optional
 
 import datasets
 import numpy as np
+import torch
 from datasets import load_dataset
 
 import evaluate
 import transformers
+from nncf import NNCFConfig
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.torch import register_default_init_args
+from nncf.torch.initialization import PTInitializingDataLoader
+
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -42,6 +48,7 @@ from transformers import (
     default_data_collator,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
@@ -282,15 +289,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
-    )
 
     # Preprocessing the datasets
     # Padding strategy
@@ -367,6 +365,56 @@ def main():
     else:
         data_collator = None
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args,
+                                                             train_dataset,
+                                                             data_collator=data_collator)
+
+            class KwargBasedInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            initializing_data_loader = KwargBasedInitializingDataloader(train_dataloader)
+            nncf_config = register_default_init_args(nncf_config, initializing_data_loader)
+
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, training_args.max_seq_length], dtype=torch.long)
+            torch.onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -376,8 +424,13 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+    if nncf_config is not None:
+        if not (training_args.local_rank == -1 or training_args.no_cuda):
+            compression_ctrl.distributed()
+
     # Training
     if training_args.do_train:
         checkpoint = None
diff --git a/examples/pytorch/token-classification/run_ner.py b/examples/pytorch/token-classification/run_ner.py
index 52cbbb87b..30f150580 100755
--- a/examples/pytorch/token-classification/run_ner.py
+++ b/examples/pytorch/token-classification/run_ner.py
@@ -22,15 +22,24 @@ Fine-tuning the library models for token classification.
 import logging
 import os
 import sys
+from copy import deepcopy
 from dataclasses import dataclass, field
 from typing import Optional
+from typing import List
 
 import datasets
 import numpy as np
 from datasets import ClassLabel, load_dataset
 
 import evaluate
+import torch
 import transformers
+from nncf import NNCFConfig
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
+from packaging import version
+from torch import onnx
 from transformers import (
     AutoConfig,
     AutoModelForTokenClassification,
@@ -43,6 +52,7 @@ from transformers import (
     TrainingArguments,
     set_seed,
 )
+from transformers.trainer import get_train_dataloader_for_init
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version
@@ -204,6 +214,16 @@ class DataTrainingArguments:
         self.task_name = self.task_name.lower()
 
 
+def filter_columns(dataset, keep_columns: List[str], remove_columns: List[str]):
+    if version.parse(datasets.__version__) < version.parse("1.4.0"):
+        dataset.set_format(
+            type=dataset.format["type"], columns=keep_columns, format_kwargs=dataset.format["format_kwargs"]
+        )
+        return dataset
+    else:
+        return dataset.remove_columns(remove_columns)
+
+
 def main():
     # See all possible arguments in src/transformers/training_args.py
     # or by passing the --help flag to this script.
@@ -366,16 +386,6 @@ def main():
             use_auth_token=True if model_args.use_auth_token else None,
         )
 
-    model = AutoModelForTokenClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
-    )
-
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
@@ -385,25 +395,25 @@ def main():
         )
 
     # Model has labels -> use them.
-    if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
-        if list(sorted(model.config.label2id.keys())) == list(sorted(label_list)):
+    if config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
+        if list(sorted(config.label2id.keys())) == list(sorted(label_list)):
             # Reorganize `label_list` to match the ordering of the model.
             if labels_are_int:
-                label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}
-                label_list = [model.config.id2label[i] for i in range(num_labels)]
+                label_to_id = {i: int(config.label2id[l]) for i, l in enumerate(label_list)}
+                label_list = [config.id2label[i] for i in range(num_labels)]
             else:
-                label_list = [model.config.id2label[i] for i in range(num_labels)]
+                label_list = [config.id2label[i] for i in range(num_labels)]
                 label_to_id = {l: i for i, l in enumerate(label_list)}
         else:
             logger.warning(
                 "Your model seems to have been trained with labels, but they don't match the dataset: ",
-                f"model labels: {list(sorted(model.config.label2id.keys()))}, dataset labels:"
+                f"model labels: {list(sorted(config.label2id.keys()))}, dataset labels:"
                 f" {list(sorted(label_list))}.\nIgnoring the model labels as a result.",
             )
 
     # Set the correspondences label/ID inside the model config
-    model.config.label2id = {l: i for i, l in enumerate(label_list)}
-    model.config.id2label = {i: l for i, l in enumerate(label_list)}
+    config.label2id = {l: i for i, l in enumerate(label_list)}
+    config.id2label = {i: l for i, l in enumerate(label_list)}
 
     # Map that sends B-Xxx label to its I-Xxx counterpart
     b_to_i_label = []
@@ -504,6 +514,65 @@ def main():
     # Data collator
     data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataset_for_init = deepcopy(train_dataset)
+
+            train_dataset_for_init = filter_columns(train_dataset_for_init,
+                                                    keep_columns=['labels', 'input_ids', 'attention_mask',
+                                                                  'token_type_ids'],
+                                                    remove_columns=['ner_tags', 'pos_tags', 'tokens', 'id',
+                                                                    'chunk_tags'])
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset_for_init, data_collator)
+
+            class ConllInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), {
+                        "input_ids": dataloader_output["input_ids"],
+                        "attention_mask": dataloader_output["attention_mask"],
+                        "token_type_ids": dataloader_output["token_type_ids"],
+                    }
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(ConllInitializingDataloader(train_dataloader)),
+                BNAdaptationInitArgs(ConllInitializingDataloader(train_dataloader)),
+            ])
+
+    retval = AutoModelForTokenClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx,
+                        opset_version=10)
+
     # Metrics
     metric = evaluate.load("seqeval")
 
@@ -549,6 +618,7 @@ def main():
         tokenizer=tokenizer,
         data_collator=data_collator,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/nncf_bert_config_conll.json b/nncf_bert_config_conll.json
new file mode 100644
index 000000000..bf7c88ebb
--- /dev/null
+++ b/nncf_bert_config_conll.json
@@ -0,0 +1,44 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true,
+            "per_channel": false
+        }
+    }
+}
diff --git a/nncf_bert_config_mrpc.json b/nncf_bert_config_mrpc.json
new file mode 100644
index 000000000..425d89d76
--- /dev/null
+++ b/nncf_bert_config_mrpc.json
@@ -0,0 +1,42 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 64,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "per_channel": false
+        }
+    }
+}
diff --git a/nncf_bert_config_squad.json b/nncf_bert_config_squad.json
new file mode 100644
index 000000000..2a055de17
--- /dev/null
+++ b/nncf_bert_config_squad.json
@@ -0,0 +1,44 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true,
+            "per_channel": false
+        }
+    }
+}
diff --git a/nncf_bert_config_squad_magnitude_sparsity_cubic.json b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
new file mode 100644
index 000000000..b4452e8d4
--- /dev/null
+++ b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
@@ -0,0 +1,31 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "magnitude_sparsity",
+        "params": {
+            "schedule": "polynomial",
+            "power": 3,
+            "sparsity_init": 0.0,
+            "sparsity_target": 0.8,
+            "sparsity_target_epoch": 40,
+            "sparsity_freeze_epoch": 60,
+            "update_per_optimizer_step": true,
+            "steps_per_epoch": 1109,
+            "weight_importance": "abs"
+        },
+        "ignored_scopes": ["{re}.*NNCFEmbedding"]
+    }
+}
diff --git a/nncf_bert_config_xnli.json b/nncf_bert_config_xnli.json
new file mode 100644
index 000000000..92b95db1c
--- /dev/null
+++ b/nncf_bert_config_xnli.json
@@ -0,0 +1,38 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 96
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "{re}BertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric"
+        }
+    }
+}
diff --git a/nncf_distilbert_config_sst2.json b/nncf_distilbert_config_sst2.json
new file mode 100644
index 000000000..dc140ab39
--- /dev/null
+++ b/nncf_distilbert_config_sst2.json
@@ -0,0 +1,33 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "mean_percentile"
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_gpt2_config_wikitext_hw_config.json b/nncf_gpt2_config_wikitext_hw_config.json
new file mode 100644
index 000000000..55173b25b
--- /dev/null
+++ b/nncf_gpt2_config_wikitext_hw_config.json
@@ -0,0 +1,49 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 1024],
+            "type": "long"
+        }
+    ],
+    "hw_config_type": "cpu",
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 16,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "ignored_scopes": [
+            // Intermediate embedding sum results
+            "GPT2LMHeadModel/GPT2Model[transformer]/__add___0",
+
+            // Scaling in attention
+            "{re}.*Attention\\[attn\\]/__truediv___0",
+
+            // Pre-LayerNorm additions
+            "{re}.*Block\\[[0-9]*\\]/__add___0",
+            "{re}.*Block\\[[0-9]*\\]/__add___1",
+
+            // LM head
+            "GPT2LMHeadModel/NNCFLinear[lm_head]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_mobilebert_config_squad_int8.json b/nncf_mobilebert_config_squad_int8.json
new file mode 100644
index 000000000..4d0e84edf
--- /dev/null
+++ b/nncf_mobilebert_config_squad_int8.json
@@ -0,0 +1,49 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 64,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "ignored_scopes": ["{re}MobileBertSelfAttention\\[self\\]/__add___0",
+            "{re}MobileBertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"],
+        "activations":
+        {
+            "mode": "symmetric",
+            "ignored_scopes": [
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___0",
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___1",
+            "{re}MobileBertOutput\\[output\\]/__add___0",
+            "{re}NoNorm\\[LayerNorm\\]/__mul___0"]
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_roberta_config_mnli.json b/nncf_roberta_config_mnli.json
new file mode 100644
index 000000000..ff2d462f6
--- /dev/null
+++ b/nncf_roberta_config_mnli.json
@@ -0,0 +1,35 @@
+{
+    "input_info": [
+        {
+            "keyword": "input_ids",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        },
+        {
+            "keyword": "attention_mask",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 24
+            },
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            }
+        },
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "asymmetric"
+        }
+    }
+}
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 5f4fccd33..7f4cdb3d8 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -27,10 +27,12 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import torch
+from nncf.torch import create_compressed_model
 from packaging import version
 from torch import Tensor, device, nn
 from torch.nn import CrossEntropyLoss
 
+from transformers.utils import NNCF_PT_STATE_NAME
 from transformers.utils.hub import convert_file_size_to_int, get_checkpoint_shard_files
 from transformers.utils.import_utils import is_sagemaker_mp_enabled
 
@@ -1497,6 +1499,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         push_to_hub: bool = False,
         max_shard_size: Union[int, str] = "10GB",
         safe_serialization: bool = False,
+        nncf_compression_state: Dict = None,
         **kwargs,
     ):
         """
@@ -1620,6 +1623,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
             else:
                 save_function(shard, os.path.join(save_directory, shard_file))
 
+        if nncf_compression_state is not None:
+            nncf_state_output_file = os.path.join(save_directory, NNCF_PT_STATE_NAME)
+            save_function(nncf_compression_state, nncf_state_output_file)
+
         if index is None:
             logger.info(f"Model weights saved in {os.path.join(save_directory, WEIGHTS_NAME)}")
         else:
@@ -1901,6 +1908,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         load_in_8bit_skip_modules = kwargs.pop("load_in_8bit_skip_modules", None)
         subfolder = kwargs.pop("subfolder", "")
         commit_hash = kwargs.pop("_commit_hash", None)
+        nncf_config = kwargs.pop("nncf_config", None)
+        nncf_eval = kwargs.pop("nncf_eval", False)
 
         if trust_remote_code is True:
             logger.warning(
@@ -2321,6 +2330,11 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
             if dtype_orig is not None:
                 torch.set_default_dtype(dtype_orig)
 
+            if nncf_config is not None and nncf_eval:
+                compression_algo_controller, model = create_compressed_model(model, nncf_config,
+                                                                             compression_state=state_dict)
+                return compression_algo_controller, model
+
             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
                 model,
                 state_dict,
@@ -2344,6 +2358,16 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         # Set model in evaluation mode to deactivate DropOut modules by default
         model.eval()
 
+        if nncf_config is not None:
+            compression_state = None
+            compression_state_file = os.path.join(pretrained_model_name_or_path, NNCF_PT_STATE_NAME)
+            if os.path.isfile(compression_state_file):
+                compression_state = torch.load(compression_state_file)
+
+            compression_algo_controller, model = create_compressed_model(model, nncf_config,
+                                                                         compression_state=compression_state)
+            return compression_algo_controller, model
+
         # Dispatch model with hooks on all devices if necessary
         if device_map is not None:
             dispatch_model(model, device_map=device_map, offload_dir=offload_folder)
diff --git a/src/transformers/pytorch_utils.py b/src/transformers/pytorch_utils.py
index d94e049b5..09c99d4dd 100644
--- a/src/transformers/pytorch_utils.py
+++ b/src/transformers/pytorch_utils.py
@@ -87,7 +87,8 @@ def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0)
         new_layer.bias.requires_grad = True
     return new_layer
 
-
+import nncf
+@nncf.torch.register_module()
 class Conv1D(nn.Module):
     """
     1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 214e7a978..530e80048 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -33,7 +33,7 @@ from pathlib import Path
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
 
 from tqdm.auto import tqdm
-
+from nncf.torch.nncf_network import NNCFNetwork
 
 # Integrations must be imported before ML frameworks:
 from .integrations import (  # isort: split
@@ -55,6 +55,8 @@ import numpy as np
 import torch
 import torch.distributed as dist
 from packaging import version
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from torch import nn
 from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler
 from torch.utils.data.distributed import DistributedSampler
@@ -206,6 +208,30 @@ SCHEDULER_NAME = "scheduler.pt"
 SCALER_NAME = "scaler.pt"
 
 
+def get_train_dataloader_for_init(args, train_dataset, data_collator=None):
+    from torch.utils.data import RandomSampler
+    from torch.utils.data import DistributedSampler
+    train_sampler = (
+        RandomSampler(train_dataset)
+        if args.local_rank == -1
+        else DistributedSampler(train_dataset)
+    )
+
+    if data_collator is None:
+        from transformers.data.data_collator import default_data_collator
+        data_collator = default_data_collator
+
+    from torch.utils.data import DataLoader
+    data_loader = DataLoader(
+        train_dataset,
+        batch_size=args.train_batch_size,
+        sampler=train_sampler,
+        collate_fn=data_collator,
+        drop_last=args.dataloader_drop_last,
+    )
+    return data_loader
+
+
 class Trainer:
     """
     Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.
@@ -304,12 +330,15 @@ class Trainer:
         callbacks: Optional[List[TrainerCallback]] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
         preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None,
+        compression_ctrl: PTCompressionAlgorithmController = None
     ):
         if args is None:
             output_dir = "tmp_trainer"
             logger.info(f"No `TrainingArguments` passed, using `output_dir={output_dir}`.")
             args = TrainingArguments(output_dir=output_dir)
         self.args = args
+
+        self.compression_ctrl = compression_ctrl
         # Seed must be set before instantiating the model when using model
         enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)
         self.hp_name = None
@@ -623,7 +652,10 @@ class Trainer:
         self.current_flos = 0
         self.hp_search_backend = None
         self.use_tune_checkpoints = False
-        default_label_names = find_labels(self.model.__class__)
+        model_class = self.model.__class__
+        if isinstance(self.model, NNCFNetwork):
+            model_class = self.model.get_nncf_wrapped_model().__class__
+        default_label_names = find_labels(model_class)
         self.label_names = default_label_names if self.args.label_names is None else self.args.label_names
         self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)
 
@@ -708,7 +740,10 @@ class Trainer:
     def _set_signature_columns_if_needed(self):
         if self._signature_columns is None:
             # Inspect model forward signature to keep only the arguments it accepts.
-            signature = inspect.signature(self.model.forward)
+            if isinstance(self.model, NNCFNetwork):
+                signature = inspect.signature(self.model.get_nncf_wrapped_model().forward)
+            else:
+                signature = inspect.signature(self.model.forward)
             self._signature_columns = list(signature.parameters.keys())
             # Labels may be named label or label_ids, the default data collator handles that.
             self._signature_columns += list(set(["label", "label_ids"] + self.label_names))
@@ -1409,6 +1444,8 @@ class Trainer:
 
             if self.args.ddp_bucket_cap_mb is not None:
                 kwargs["bucket_cap_mb"] = self.args.ddp_bucket_cap_mb
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
             model = nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank] if self.args._n_gpu != 0 else None,
@@ -1687,6 +1724,9 @@ class Trainer:
                     _ = list(train_dataloader.sampler)
 
         for epoch in range(epochs_trained, num_train_epochs):
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.scheduler.epoch_step()
+                print(self.compression_ctrl.statistics().to_str())
             if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                 train_dataloader.sampler.set_epoch(epoch)
             elif hasattr(train_dataloader, "dataset") and isinstance(train_dataloader.dataset, IterableDatasetShard):
@@ -1790,6 +1830,8 @@ class Trainer:
                             )
 
                     # Optimizer step
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.scheduler.step()
                     optimizer_was_run = True
                     if self.deepspeed:
                         pass  # called outside the loop
@@ -1814,6 +1856,7 @@ class Trainer:
                     model.zero_grad()
                     self.state.global_step += 1
                     self.state.epoch = epoch + (step + 1) / steps_in_epoch
+                    self.state.curr_loss = tr_loss_step.cpu().detach().item()
                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
 
                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
@@ -2033,6 +2076,14 @@ class Trainer:
             logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
             logs["learning_rate"] = self._get_learning_rate()
 
+            if self.compression_ctrl is not None:
+                logs["compression_loss"] = self.compression_ctrl.loss().item()
+                compression_stats = self.compression_ctrl.statistics()
+                for key, value in prepare_for_tensorboard(compression_stats).items():
+                    logs["compression/statistics/{0}".format(key)] = value
+                print(compression_stats.to_str())
+
+
             self._total_loss_scalar += tr_loss_scalar
             self._globalstep_last_logged = self.state.global_step
             self.store_flos()
@@ -2492,6 +2543,10 @@ class Trainer:
             # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`
             loss = loss / self.args.gradient_accumulation_steps
 
+        if self.compression_ctrl is not None:
+            compression_loss = self.compression_ctrl.loss()
+            loss += compression_loss
+
         if self.do_grad_scaling:
             self.scaler.scale(loss).backward()
         elif self.use_apex:
@@ -2657,10 +2712,15 @@ class Trainer:
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         if not isinstance(self.model, PreTrainedModel):
-            if isinstance(unwrap_model(self.model), PreTrainedModel):
+            unwrapped_model = unwrap_model(self.model)
+            if isinstance(unwrapped_model, NNCFNetwork):
+                is_pretrained = isinstance(unwrapped_model.get_nncf_wrapped_model(), PreTrainedModel)
+            else:
+                is_pretrained = isinstance(unwrapped_model, PreTrainedModel)
+            if is_pretrained:
                 if state_dict is None:
-                    state_dict = self.model.state_dict()
-                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)
+                    state_dict = unwrapped_model.state_dict()
+                unwrapped_model.save_pretrained(output_dir, state_dict=state_dict)
             else:
                 logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                 if state_dict is None:
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 170315fe2..daa497c02 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -993,6 +993,12 @@ class TrainingArguments:
         },
     )
 
+    nncf_config: str = field(default=None,
+                             metadata={"help": "NNCF configuration .json file for compression-enabled training"})
+
+    to_onnx: str = field(default=None,
+                         metadata={"help": "Name of the ONNX model file to export the model to."})
+
     def __post_init__(self):
         # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
         # This needs to happen before any call to self.device or self.n_gpu.
diff --git a/src/transformers/utils/__init__.py b/src/transformers/utils/__init__.py
index 2269f2254..2f3082293 100644
--- a/src/transformers/utils/__init__.py
+++ b/src/transformers/utils/__init__.py
@@ -154,6 +154,7 @@ from .import_utils import (
 
 
 WEIGHTS_NAME = "pytorch_model.bin"
+NNCF_PT_STATE_NAME = "nncf_state.bin"
 WEIGHTS_INDEX_NAME = "pytorch_model.bin.index.json"
 TF2_WEIGHTS_NAME = "tf_model.h5"
 TF2_WEIGHTS_INDEX_NAME = "tf_model.h5.index.json"
-- 
2.25.1

