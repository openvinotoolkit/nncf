From 24e975fb879f8b5b6b590e83d0ffb8e0359c11dc Mon Sep 17 00:00:00 2001
From: Vasily Shamporov <vasily.shamporov@intel.com>
Date: Fri, 29 Nov 2019 18:33:05 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 .../run_language_modeling.py                  |  78 +++++++++--
 examples/question-answering/run_squad.py      | 128 ++++++++++++++++--
 examples/text-classification/run_glue.py      |  38 +++++-
 examples/text-classification/run_xnli.py      | 127 +++++++++++++++--
 nncf_bert_config_squad.json                   |  46 +++++++
 ...config_squad_magnitude_sparsity_cubic.json |  31 +++++
 nncf_bert_config_xnli.json                    |  36 +++++
 nncf_distilbert_config_sst2.json              |  33 +++++
 nncf_gpt2_config_wikitext_hw_config.json      |  58 ++++++++
 nncf_mobilebert_config_squad_int8.json        |  46 +++++++
 nncf_roberta_config_mnli.json                 |  36 +++++
 src/transformers/modeling_gpt2.py             |   5 +-
 src/transformers/modeling_utils.py            |  26 +++-
 src/transformers/tokenization_utils_base.py   |   2 +-
 src/transformers/trainer.py                   |  65 ++++++++-
 src/transformers/training_args.py             |   8 ++
 16 files changed, 708 insertions(+), 55 deletions(-)
 create mode 100644 nncf_bert_config_squad.json
 create mode 100644 nncf_bert_config_squad_magnitude_sparsity_cubic.json
 create mode 100644 nncf_bert_config_xnli.json
 create mode 100644 nncf_distilbert_config_sst2.json
 create mode 100644 nncf_gpt2_config_wikitext_hw_config.json
 create mode 100644 nncf_mobilebert_config_squad_int8.json
 create mode 100644 nncf_roberta_config_mnli.json

diff --git a/examples/language-modeling/run_language_modeling.py b/examples/language-modeling/run_language_modeling.py
index d9465c37..9da9d30f 100644
--- a/examples/language-modeling/run_language_modeling.py
+++ b/examples/language-modeling/run_language_modeling.py
@@ -26,6 +26,8 @@ import os
 from dataclasses import dataclass, field
 from typing import Optional
 
+import torch
+from torch import onnx
 from transformers import (
     CONFIG_MAPPING,
     MODEL_WITH_LM_HEAD_MAPPING,
@@ -43,8 +45,15 @@ from transformers import (
 )
 
 
-logger = logging.getLogger(__name__)
+from nncf import NNCFConfig
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
+from transformers.trainer import get_train_dataloader_for_init
 
+logger = logging.getLogger(__name__)
+# from nncf.torch import set_log_level
+# set_log_level(logging.DEBUG)
 
 MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())
 MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)
@@ -192,16 +200,60 @@ def main():
             "and load it from here, using --tokenizer_name"
         )
 
+
+    if data_args.block_size <= 0:
+        data_args.block_size = tokenizer.max_len
+        # Our input block size will be the max possible for the model
+    else:
+        data_args.block_size = min(data_args.block_size, tokenizer.max_len)
+
+
+    # Get datasets
+
+    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
+    eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None
+    data_collator = DataCollatorForLanguageModeling(
+        tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability
+    )
+
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset,
+                                                             data_collator)
+
+            class WikitextInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(WikitextInitializingDataLoader(train_dataloader)),
+                BNAdaptationInitArgs(WikitextInitializingDataLoader(train_dataloader)),
+            ])
+
     if model_args.model_name_or_path:
-        model = AutoModelWithLMHead.from_pretrained(
+        retval = AutoModelWithLMHead.from_pretrained(
             model_args.model_name_or_path,
             from_tf=bool(".ckpt" in model_args.model_name_or_path),
             config=config,
             cache_dir=model_args.cache_dir,
+            nncf_config=nncf_config,
+            nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
         )
     else:
         logger.info("Training new model from scratch")
-        model = AutoModelWithLMHead.from_config(config)
+        retval = AutoModelWithLMHead.from_config(config)
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
 
     model.resize_token_embeddings(len(tokenizer))
 
@@ -211,19 +261,16 @@ def main():
             "flag (masked language modeling)."
         )
 
-    if data_args.block_size <= 0:
-        data_args.block_size = tokenizer.max_len
-        # Our input block size will be the max possible for the model
-    else:
-        data_args.block_size = min(data_args.block_size, tokenizer.max_len)
-
-    # Get datasets
 
-    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
-    eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None
-    data_collator = DataCollatorForLanguageModeling(
-        tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability
-    )
+    if training_args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, config.n_positions], dtype=torch.long)
+            onnx.export(model, dummy_tensor, training_args.to_onnx)
 
     # Initialize our Trainer
     trainer = Trainer(
@@ -233,6 +280,7 @@ def main():
         train_dataset=train_dataset,
         eval_dataset=eval_dataset,
         prediction_loss_only=True,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/question-answering/run_squad.py b/examples/question-answering/run_squad.py
index 2bd4e90f..2b1a60e8 100644
--- a/examples/question-answering/run_squad.py
+++ b/examples/question-answering/run_squad.py
@@ -25,9 +25,13 @@ import timeit
 
 import numpy as np
 import torch
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
 from torch.utils.data.distributed import DistributedSampler
 from tqdm import tqdm, trange
+from torch import onnx
 
 from transformers import (
     MODEL_FOR_QUESTION_ANSWERING_MAPPING,
@@ -52,6 +54,8 @@ try:
 except ImportError:
     from tensorboardX import SummaryWriter
 
+from nncf import NNCFConfig
+from nncf.torch.initialization import PTInitializingDataLoader
 
 logger = logging.getLogger(__name__)
 
@@ -71,14 +75,19 @@ def to_list(tensor):
     return tensor.detach().cpu().tolist()
 
 
-def train(args, train_dataset, model, tokenizer):
+def get_train_dataloader(args, train_dataset):
+    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
+    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
+    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    return train_dataloader
+
+
+def train(args, train_dataset, model, tokenizer, compression_ctrl=None):
     """ Train the model """
     if args.local_rank in [-1, 0]:
         tb_writer = SummaryWriter()
 
-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    train_dataloader = get_train_dataloader(args, train_dataset)
 
     if args.max_steps > 0:
         t_total = args.max_steps
@@ -144,7 +153,7 @@ def train(args, train_dataset, model, tokenizer):
     epochs_trained = 0
     steps_trained_in_current_epoch = 0
     # Check if continuing training from a checkpoint
-    if os.path.exists(args.model_name_or_path):
+    if os.path.exists(args.model_name_or_path) and compression_ctrl is None:
         try:
             # set global_step to gobal_step of last saved checkpoint from model path
             checkpoint_suffix = args.model_name_or_path.split("-")[-1].split("/")[0]
@@ -200,6 +209,7 @@ def train(args, train_dataset, model, tokenizer):
                     )
 
             outputs = model(**inputs)
+
             # model outputs are always tuple in transformers (see doc)
             loss = outputs[0]
 
@@ -208,6 +218,10 @@ def train(args, train_dataset, model, tokenizer):
             if args.gradient_accumulation_steps > 1:
                 loss = loss / args.gradient_accumulation_steps
 
+            if compression_ctrl is not None:
+                compression_loss = compression_ctrl.loss()
+                loss += compression_loss
+
             if args.fp16:
                 with amp.scale_loss(loss, optimizer) as scaled_loss:
                     scaled_loss.backward()
@@ -215,6 +229,8 @@ def train(args, train_dataset, model, tokenizer):
                 loss.backward()
 
             tr_loss += loss.item()
+
+            epoch_iterator.set_postfix(loss=loss.item())
             if (step + 1) % args.gradient_accumulation_steps == 0:
                 if args.fp16:
                     torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
@@ -223,6 +239,10 @@ def train(args, train_dataset, model, tokenizer):
 
                 optimizer.step()
                 scheduler.step()  # Update learning rate schedule
+
+                if compression_ctrl is not None:
+                    compression_ctrl.scheduler.step()
+
                 model.zero_grad()
                 global_step += 1
 
@@ -237,12 +257,19 @@ def train(args, train_dataset, model, tokenizer):
                     tb_writer.add_scalar("loss", (tr_loss - logging_loss) / args.logging_steps, global_step)
                     logging_loss = tr_loss
 
+                    if compression_ctrl is not None:
+                        tb_writer.add_scalar("compression_loss", compression_loss.item())
+                        compression_stats = compression_ctrl.statistics()
+                        for key, value in prepare_for_tensorboard(compression_stats).items():
+                            tb_writer.add_scalar("compression/statistics/{0}".format(key), value,
+                                                 global_step)
+
                 # Save model checkpoint
                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
                     output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
-                    # Take care of distributed/parallel training
-                    model_to_save = model.module if hasattr(model, "module") else model
-                    model_to_save.save_pretrained(output_dir)
+
+                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training
+                    model_to_save.save_pretrained(output_dir, saved_module_override=model_to_save)
                     tokenizer.save_pretrained(output_dir)
 
                     torch.save(args, os.path.join(output_dir, "training_args.bin"))
@@ -255,10 +283,13 @@ def train(args, train_dataset, model, tokenizer):
             if args.max_steps > 0 and global_step > args.max_steps:
                 epoch_iterator.close()
                 break
+
+        if compression_ctrl is not None:
+            compression_ctrl.scheduler.epoch_step()
+
         if args.max_steps > 0 and global_step > args.max_steps:
             train_iterator.close()
             break
-
     if args.local_rank in [-1, 0]:
         tb_writer.close()
 
@@ -658,6 +689,11 @@ def main():
     parser.add_argument("--server_port", type=str, default="", help="Can be used for distant debugging.")
 
     parser.add_argument("--threads", type=int, default=1, help="multiple threads for converting example to features")
+    parser.add_argument('--to_onnx', type=str, metavar='PATH', default=None,
+                        help='Export to ONNX model by given path')
+    parser.add_argument('--nncf_config', type=str, help='path to NNCF config .json file to be used for compressed model'
+                                                        'fine-tuning')
+
     args = parser.parse_args()
 
     if args.doc_stride >= args.max_seq_length - args.max_query_length:
@@ -732,19 +768,69 @@ def main():
         do_lower_case=args.do_lower_case,
         cache_dir=args.cache_dir if args.cache_dir else None,
     )
-    model = AutoModelForQuestionAnswering.from_pretrained(
+
+    nncf_config = None
+    if args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = args.output_dir
+        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if args.do_train:
+            train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)
+            train_dataloader = get_train_dataloader(args, train_dataset)
+
+            class SquadInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    kwargs = {'attention_mask': dataloader_output[1],
+                              'start_positions': dataloader_output[3],
+                              'end_positions': dataloader_output[4]}
+                    if args.model_type != 'distilbert':
+                        kwargs['token_type_ids'] = None if args.model_type == 'xlm' else dataloader_output[2]
+                    if args.model_type in ['xlnet', 'xlm']:
+                        kwargs.update({'cls_index': dataloader_output[5],
+                                       'p_mask': dataloader_output[6]})
+                    return (dataloader_output[0],), kwargs
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(SquadInitializingDataloader(train_dataloader)),
+                BNAdaptationInitArgs(SquadInitializingDataloader(train_dataloader)),
+            ])
+
+    retval = AutoModelForQuestionAnswering.from_pretrained(
         args.model_name_or_path,
         from_tf=bool(".ckpt" in args.model_name_or_path),
         config=config,
         cache_dir=args.cache_dir if args.cache_dir else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and args.do_eval and not args.do_train
     )
 
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+
     if args.local_rank == 0:
         # Make sure only the first process in distributed training will download model & vocab
         torch.distributed.barrier()
 
+    if args.to_onnx:
+        if nncf_config is not None:
+            compression_ctrl.export_model(args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, args.max_seq_length], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), args.to_onnx)
+
     model.to(args.device)
 
+    if nncf_config is not None:
+        if not (args.local_rank == -1 or args.no_cuda):
+            compression_ctrl.distributed()
+
     logger.info("Training/evaluation parameters %s", args)
 
     # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.
@@ -761,7 +845,7 @@ def main():
     # Training
     if args.do_train:
         train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)
-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)
+        global_step, tr_loss = train(args, train_dataset, model, tokenizer, compression_ctrl)
         logger.info(" global_step = %s, average loss = %s", global_step, tr_loss)
 
     # Save the trained model and the tokenizer
@@ -771,14 +855,22 @@ def main():
         # They can then be reloaded using `from_pretrained()`
         # Take care of distributed/parallel training
         model_to_save = model.module if hasattr(model, "module") else model
-        model_to_save.save_pretrained(args.output_dir)
+        model_to_save.save_pretrained(args.output_dir, saved_module_override=model_to_save)
         tokenizer.save_pretrained(args.output_dir)
 
         # Good practice: save your training arguments together with the trained model
         torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
 
         # Load a trained model and vocabulary that you have fine-tuned
-        model = AutoModelForQuestionAnswering.from_pretrained(args.output_dir)  # , force_download=True)
+        retval = AutoModelForQuestionAnswering.from_pretrained(args.output_dir,
+                                                               nncf_config=nncf_config,
+                                                               nncf_eval=True if nncf_config is not None else False)
+
+        if nncf_config is None:
+            model = retval
+        else:
+            _, model = retval
+
         tokenizer = AutoTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)
         model.to(args.device)
 
@@ -803,7 +895,15 @@ def main():
         for checkpoint in checkpoints:
             # Reload the model
             global_step = checkpoint.split("-")[-1] if len(checkpoints) > 1 else ""
-            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)
+            retval = AutoModelForQuestionAnswering.from_pretrained(checkpoint,
+                                                                   nncf_config=nncf_config,
+                                                                   nncf_eval=True if nncf_config is not None else False)  # , force_download=True)
+
+            if nncf_config is None:
+                model = retval
+            else:
+                _, model = retval
+
             model.to(args.device)
 
             # Evaluate
diff --git a/examples/text-classification/run_glue.py b/examples/text-classification/run_glue.py
index cf9b765a..957cdba2 100644
--- a/examples/text-classification/run_glue.py
+++ b/examples/text-classification/run_glue.py
@@ -24,6 +24,10 @@ from dataclasses import dataclass, field
 from typing import Callable, Dict, Optional
 
 import numpy as np
+from nncf import NNCFConfig
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.torch.initialization import PTInitializingDataLoader
 
 from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset
 from transformers import GlueDataTrainingArguments as DataTrainingArguments
@@ -36,7 +39,7 @@ from transformers import (
     glue_tasks_num_labels,
     set_seed,
 )
-
+from transformers.trainer import get_train_dataloader_for_init
 
 logger = logging.getLogger(__name__)
 
@@ -126,14 +129,44 @@ def main():
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
+
+    # Get datasets
+    train_dataset = GlueDataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
+
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset)
+
+            class GlueInitializingDataLoader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(GlueInitializingDataLoader(train_dataloader)),
+                BNAdaptationInitArgs(GlueInitializingDataLoader(train_dataloader)),
+            ])
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
         model_args.model_name_or_path,
         from_tf=bool(".ckpt" in model_args.model_name_or_path),
         config=config,
         cache_dir=model_args.cache_dir,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
     )
 
-    # Get datasets
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+            
     train_dataset = (
         GlueDataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None
     )
@@ -165,6 +196,7 @@ def main():
         train_dataset=train_dataset,
         eval_dataset=eval_dataset,
         compute_metrics=build_compute_metrics_fn(data_args.task_name),
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/text-classification/run_xnli.py b/examples/text-classification/run_xnli.py
index c1db6fa3..ca44bd1c 100644
--- a/examples/text-classification/run_xnli.py
+++ b/examples/text-classification/run_xnli.py
@@ -25,8 +25,13 @@ import random
 
 import numpy as np
 import torch
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
 from torch.utils.data.distributed import DistributedSampler
+from torch import onnx
+import torch.distributed
 from tqdm import tqdm, trange
 
 from transformers import (
@@ -48,6 +51,8 @@ try:
 except ImportError:
     from tensorboardX import SummaryWriter
 
+from nncf import NNCFConfig
+from nncf.torch.initialization import PTInitializingDataLoader
 
 logger = logging.getLogger(__name__)
 
@@ -60,14 +65,19 @@ def set_seed(args):
         torch.cuda.manual_seed_all(args.seed)
 
 
-def train(args, train_dataset, model, tokenizer):
+def get_train_dataloader(args, train_dataset):
+    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
+    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
+    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    return train_dataloader
+
+
+def train(args, train_dataset, model, tokenizer, compression_ctrl=None):
     """ Train the model """
     if args.local_rank in [-1, 0]:
         tb_writer = SummaryWriter()
 
-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    train_dataloader = get_train_dataloader(args, train_dataset)
 
     if args.max_steps > 0:
         t_total = args.max_steps
@@ -132,7 +142,7 @@ def train(args, train_dataset, model, tokenizer):
     epochs_trained = 0
     steps_trained_in_current_epoch = 0
     # Check if continuing training from a checkpoint
-    if os.path.exists(args.model_name_or_path):
+    if os.path.exists(args.model_name_or_path) and compression_ctrl is None:
         # set global_step to gobal_step of last saved checkpoint from model path
         global_step = int(args.model_name_or_path.split("-")[-1].split("/")[0])
         epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)
@@ -164,7 +174,9 @@ def train(args, train_dataset, model, tokenizer):
                 inputs["token_type_ids"] = (
                     batch[2] if args.model_type in ["bert"] else None
                 )  # XLM and DistilBERT don't use segment_ids
+
             outputs = model(**inputs)
+
             loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
 
             if args.n_gpu > 1:
@@ -172,6 +184,10 @@ def train(args, train_dataset, model, tokenizer):
             if args.gradient_accumulation_steps > 1:
                 loss = loss / args.gradient_accumulation_steps
 
+            if compression_ctrl is not None:
+                compression_loss = compression_ctrl.loss()
+                loss += compression_loss
+
             if args.fp16:
                 with amp.scale_loss(loss, optimizer) as scaled_loss:
                     scaled_loss.backward()
@@ -179,6 +195,8 @@ def train(args, train_dataset, model, tokenizer):
                 loss.backward()
 
             tr_loss += loss.item()
+            epoch_iterator.set_postfix(loss=loss.item())
+
             if (step + 1) % args.gradient_accumulation_steps == 0:
                 if args.fp16:
                     torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
@@ -188,6 +206,10 @@ def train(args, train_dataset, model, tokenizer):
                 optimizer.step()
                 scheduler.step()  # Update learning rate schedule
                 model.zero_grad()
+
+                if compression_ctrl is not None:
+                    compression_ctrl.scheduler.step()
+
                 global_step += 1
 
                 if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:
@@ -202,13 +224,20 @@ def train(args, train_dataset, model, tokenizer):
                     tb_writer.add_scalar("loss", (tr_loss - logging_loss) / args.logging_steps, global_step)
                     logging_loss = tr_loss
 
+                    if compression_ctrl is not None:
+                        tb_writer.add_scalar("compression_loss", compression_loss.item())
+                        compression_stats = compression_ctrl.statistics()
+                        for key, value in prepare_for_tensorboard(compression_stats).items():
+                            tb_writer.add_scalar("compression/statistics/{0}".format(key), value,
+                                                 global_step)
+
                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
                     # Save model checkpoint
                     output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
                     if not os.path.exists(output_dir):
                         os.makedirs(output_dir)
                     model_to_save = (
-                        model.module if hasattr(model, "module") else model
+                        model.module if hasattr(model, "module") and compression_ctrl is None else model
                     )  # Take care of distributed/parallel training
                     model_to_save.save_pretrained(output_dir)
                     tokenizer.save_pretrained(output_dir)
@@ -223,6 +253,10 @@ def train(args, train_dataset, model, tokenizer):
             if args.max_steps > 0 and global_step > args.max_steps:
                 epoch_iterator.close()
                 break
+
+        if compression_ctrl is not None:
+            compression_ctrl.scheduler.epoch_step()
+
         if args.max_steps > 0 and global_step > args.max_steps:
             train_iterator.close()
             break
@@ -474,6 +508,10 @@ def main():
     parser.add_argument("--local_rank", type=int, default=-1, help="For distributed training: local_rank")
     parser.add_argument("--server_ip", type=str, default="", help="For distant debugging.")
     parser.add_argument("--server_port", type=str, default="", help="For distant debugging.")
+    parser.add_argument('--to_onnx', type=str, metavar='PATH', default=None,
+                        help='Export to ONNX model by given path')
+    parser.add_argument('--nncf_config', type=str, help='path to NNCF config .json file to be used for compressed model'
+                                                        'fine-tuning')
     args = parser.parse_args()
 
     if (
@@ -551,24 +589,74 @@ def main():
         do_lower_case=args.do_lower_case,
         cache_dir=args.cache_dir,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
+
+
+    nncf_config = None
+    if args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = args.output_dir
+        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if args.do_train:
+            train_dataset = load_and_cache_examples(args, args.task_name,  tokenizer, evaluate=False)
+            train_dataloader = get_train_dataloader(args, train_dataset)
+
+            class XnliInitializingDataloader(PTInitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    kwargs = {'attention_mask': dataloader_output[1],
+                              'labels': dataloader_output[3]}
+                    if args.model_type != 'distilbert':
+                        kwargs['token_type_ids'] = dataloader_output[2] if args.model_type in [
+                            'bert'] else None  # XLM and DistilBERT don't use segment_ids
+                    return (dataloader_output[0],), kwargs
+
+            nncf_config.register_extra_structs([
+                QuantizationRangeInitArgs(XnliInitializingDataloader(train_dataloader)),
+                BNAdaptationInitArgs(XnliInitializingDataloader(train_dataloader)),
+            ])
+
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
         args.model_name_or_path,
         from_tf=bool(".ckpt" in args.model_name_or_path),
         config=config,
         cache_dir=args.cache_dir,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and args.do_eval and not args.do_train
     )
 
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
     if args.local_rank == 0:
         torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
 
+    if args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, args.max_seq_length], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), args.to_onnx)
+
     model.to(args.device)
 
+    if nncf_config is not None:
+        if not (args.local_rank == -1 or args.no_cuda):
+            compression_ctrl.distributed()
+
     logger.info("Training/evaluation parameters %s", args)
 
     # Training
     if args.do_train:
         train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)
-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)
+        global_step, tr_loss = train(args, train_dataset, model, tokenizer, compression_ctrl)
         logger.info(" global_step = %s, average loss = %s", global_step, tr_loss)
 
     # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()
@@ -577,16 +663,25 @@ def main():
         # Save a trained model, configuration and tokenizer using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         model_to_save = (
-            model.module if hasattr(model, "module") else model
+            model.module if hasattr(model, "module") and nncf_config is None else model
         )  # Take care of distributed/parallel training
-        model_to_save.save_pretrained(args.output_dir)
+
+        model_to_save.save_pretrained(args.output_dir, saved_module_override=model_to_save)
         tokenizer.save_pretrained(args.output_dir)
 
         # Good practice: save your training arguments together with the trained model
         torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
 
         # Load a trained model and vocabulary that you have fine-tuned
-        model = AutoModelForSequenceClassification.from_pretrained(args.output_dir)
+        retval = AutoModelForSequenceClassification.from_pretrained(args.output_dir,
+                                                                   nncf_config=nncf_config,
+                                                                   nncf_eval=True if nncf_config is not None else False)
+
+        if nncf_config is None:
+            model = retval
+        else:
+            _, model = retval
+
         tokenizer = AutoTokenizer.from_pretrained(args.output_dir)
         model.to(args.device)
 
@@ -604,7 +699,15 @@ def main():
             global_step = checkpoint.split("-")[-1] if len(checkpoints) > 1 else ""
             prefix = checkpoint.split("/")[-1] if checkpoint.find("checkpoint") != -1 else ""
 
-            model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
+            retval = AutoModelForSequenceClassification.from_pretrained(checkpoint,
+                                                                        nncf_config=nncf_config,
+                                                                        nncf_eval=nncf_config is not None)
+
+            if nncf_config is None:
+                model = retval
+            else:
+                _, model = retval
+
             model.to(args.device)
             result = evaluate(args, model, tokenizer, prefix=prefix)
             result = dict((k + "_{}".format(global_step), v) for k, v in result.items())
diff --git a/nncf_bert_config_squad.json b/nncf_bert_config_squad.json
new file mode 100644
index 00000000..d3a34e92
--- /dev/null
+++ b/nncf_bert_config_squad.json
@@ -0,0 +1,46 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "{re}BertIntermediate\\[intermediate\\]/__mul___0",
+            "{re}BertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"],
+        "activations":
+        {
+            "mode": "symmetric",
+            "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/NNCFLinear\\[(key)|(query)|(value)\\].*",
+            "{re}BertForQuestionAnswering/BertModel\\[bert\\]/BertEmbeddings\\[embeddings\\]/__add___0",
+            "{re}BertForQuestionAnswering/BertModel\\[bert\\]/BertEmbeddings\\[embeddings\\]/__add___1",
+            "{re}BertOutput\\[output\\]/__add___0"]
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_bert_config_squad_magnitude_sparsity_cubic.json b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
new file mode 100644
index 00000000..b4452e8d
--- /dev/null
+++ b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
@@ -0,0 +1,31 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "magnitude_sparsity",
+        "params": {
+            "schedule": "polynomial",
+            "power": 3,
+            "sparsity_init": 0.0,
+            "sparsity_target": 0.8,
+            "sparsity_target_epoch": 40,
+            "sparsity_freeze_epoch": 60,
+            "update_per_optimizer_step": true,
+            "steps_per_epoch": 1109,
+            "weight_importance": "abs"
+        },
+        "ignored_scopes": ["{re}.*NNCFEmbedding"]
+    }
+}
diff --git a/nncf_bert_config_xnli.json b/nncf_bert_config_xnli.json
new file mode 100644
index 00000000..a21a522f
--- /dev/null
+++ b/nncf_bert_config_xnli.json
@@ -0,0 +1,36 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 96
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "{re}BertIntermediate\\[intermediate\\]/__mul___0",
+            "{re}BertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric"
+        }
+    }
+}
diff --git a/nncf_distilbert_config_sst2.json b/nncf_distilbert_config_sst2.json
new file mode 100644
index 00000000..6b648ca5
--- /dev/null
+++ b/nncf_distilbert_config_sst2.json
@@ -0,0 +1,33 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 32
+            }
+        },
+        "ignored_scopes": [
+            "{re}TransformerBlock\\[[0-9]*\\]/FFN\\[ffn\\]/__mul___0",
+            "{re}TransformerBlock\\[[0-9]*\\]/FFN\\[ffn\\]/NNCFLinear\\[lin1\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_gpt2_config_wikitext_hw_config.json b/nncf_gpt2_config_wikitext_hw_config.json
new file mode 100644
index 00000000..4b237613
--- /dev/null
+++ b/nncf_gpt2_config_wikitext_hw_config.json
@@ -0,0 +1,58 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 1024],
+            "type": "long"
+        }
+    ],
+    "hw_config_type": "cpu",
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 16,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            }
+        },
+        "ignored_scopes": [
+             //gelu_new with fusing into previous GEMM
+            "{re}.*MLP\\[mlp\\]/__rmul___0",
+            "{re}.*MLP\\[mlp\\]/__add___0",
+            "{re}.*MLP\\[mlp\\]/__rmul___1",
+            "{re}.*MLP\\[mlp\\]/tanh_0",
+            "{re}.*MLP\\[mlp\\]/__radd___0",
+            "{re}.*MLP\\[mlp\\]/__mul___0",
+
+            // Intermediate embedding sum results
+            "GPT2LMHeadModel/GPT2Model[transformer]/__add___0",
+            "GPT2LMHeadModel/GPT2Model[transformer]/__add___1",
+
+            // Scaling in attention
+            "{re}.*Attention\\[attn\\]/__truediv___0",
+
+            // Pre-LayerNorm additions
+            "{re}.*Block\\[[0-9]*\\]/__add___0",
+            "{re}.*Block\\[[0-9]*\\]/__add___1",
+
+            // Final LayerNorm inputs
+            "GPT2LMHeadModel/GPT2Model[transformer]/LayerNorm[ln_f]",
+
+            // LM head
+            "GPT2LMHeadModel/NNCFLinear[lm_head]"
+        ],
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_mobilebert_config_squad_int8.json b/nncf_mobilebert_config_squad_int8.json
new file mode 100644
index 00000000..89504d2c
--- /dev/null
+++ b/nncf_mobilebert_config_squad_int8.json
@@ -0,0 +1,46 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 64,
+                "type": "percentile",
+                "params":
+                {
+                    "min_percentile": 0.01,
+                    "max_percentile": 99.99
+                }
+            }
+        },
+        "ignored_scopes": ["{re}MobileBertSelfAttention\\[self\\]/__add___0",
+            "{re}MobileBertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"],
+        "activations":
+        {
+            "mode": "symmetric",
+            "ignored_scopes": [
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___0",
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___1",
+            "{re}MobileBertOutput\\[output\\]/__add___0",
+            "{re}NoNorm\\[LayerNorm\\]/__mul___0"]
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_roberta_config_mnli.json b/nncf_roberta_config_mnli.json
new file mode 100644
index 00000000..edbe0f84
--- /dev/null
+++ b/nncf_roberta_config_mnli.json
@@ -0,0 +1,36 @@
+{
+    "input_info": [
+        {
+            "keyword": "input_ids",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        },
+        {
+            "keyword": "attention_mask",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_samples": 24
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]",
+            "RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[dense]"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "asymmetric"
+        }
+    }
+}
diff --git a/src/transformers/modeling_gpt2.py b/src/transformers/modeling_gpt2.py
index b839cba1..1808e58b 100644
--- a/src/transformers/modeling_gpt2.py
+++ b/src/transformers/modeling_gpt2.py
@@ -119,6 +119,7 @@ class Attention(nn.Module):
         self.n_head = config.n_head
         self.split_size = n_state
         self.scale = scale
+        self.n_ctx = n_ctx
 
         self.c_attn = Conv1D(n_state * 3, nx)
         self.c_proj = Conv1D(n_state, nx)
@@ -147,7 +148,9 @@ class Attention(nn.Module):
         w = torch.matmul(q, k)
         if self.scale:
             w = w / (float(v.size(-1)) ** 0.5)
-        nd, ns = w.size(-2), w.size(-1)
+        # Had to comment this out and use static size - wouldn't let export to ONNX otherwise
+        # nd, ns = w.size(-2), w.size(-1)
+        nd, ns = self.n_ctx, self.n_ctx
         mask = self.bias[:, :, ns - nd : ns, :ns]
         w = torch.where(mask.bool(), w, self.masked_bias.to(w.dtype))
 
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 5b000c81..05df28a7 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -24,6 +24,9 @@ from torch import Tensor, device, dtype, nn
 from torch.nn import CrossEntropyLoss
 from torch.nn import functional as F
 
+from nncf.torch import create_compressed_model
+
+
 from .activations import get_activation
 from .configuration_utils import PretrainedConfig
 from .file_utils import (
@@ -471,7 +474,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
 
         self.base_model._prune_heads(heads_to_prune)
 
-    def save_pretrained(self, save_directory):
+    def save_pretrained(self, save_directory,
+                        saved_module_override=None):
         """ Save a model and its configuration file to a directory, so that it
             can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.
 
@@ -492,6 +496,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
         # If we save using the predefined names, we can load using `from_pretrained`
         output_model_file = os.path.join(save_directory, WEIGHTS_NAME)
 
+        module_to_save = model_to_save if saved_module_override is None else saved_module_override
+
         if getattr(self.config, "xla_device", False):
             import torch_xla.core.xla_model as xm
 
@@ -499,10 +505,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
                 # Save configuration file
                 model_to_save.config.save_pretrained(save_directory)
             # xm.save takes care of saving only from master
-            xm.save(model_to_save.state_dict(), output_model_file)
+            xm.save(module_to_save.state_dict(), output_model_file)
         else:
             model_to_save.config.save_pretrained(save_directory)
-            torch.save(model_to_save.state_dict(), output_model_file)
+            torch.save(module_to_save.state_dict(), output_model_file)
 
         logger.info("Model weights saved in {}".format(output_model_file))
 
@@ -588,6 +594,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
         output_loading_info = kwargs.pop("output_loading_info", False)
         local_files_only = kwargs.pop("local_files_only", False)
         use_cdn = kwargs.pop("use_cdn", True)
+        nncf_config = kwargs.pop("nncf_config", None)
+        nncf_eval = kwargs.pop("nncf_eval", False)
 
         # Load config if we don't provide a configuration
         if not isinstance(config, PretrainedConfig):
@@ -680,6 +688,11 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
                     "If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
                 )
 
+        if nncf_config is not None and nncf_eval:
+            compression_algo_controller, model = create_compressed_model(model, nncf_config,
+                                                                         resuming_state_dict=state_dict)
+            return compression_algo_controller, model
+
         missing_keys = []
         unexpected_keys = []
         error_msgs = []
@@ -786,6 +799,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
         # Set model in evaluation mode to deactivate DropOut modules by default
         model.eval()
 
+        if nncf_config is not None:
+            compression_algo_controller, model = create_compressed_model(model, nncf_config)
+            return compression_algo_controller, model
+
         if output_loading_info:
             loading_info = {
                 "missing_keys": missing_keys,
@@ -802,7 +819,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):
 
         return model
 
-
+import nncf
+@nncf.torch.register_module()
 class Conv1D(nn.Module):
     def __init__(self, nf, nx):
         """ Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)
diff --git a/src/transformers/tokenization_utils_base.py b/src/transformers/tokenization_utils_base.py
index 6a1219a4..ccb93309 100644
--- a/src/transformers/tokenization_utils_base.py
+++ b/src/transformers/tokenization_utils_base.py
@@ -1365,7 +1365,7 @@ class PreTrainedTokenizerBase(SpecialTokensMixin):
             write_dict = {}
             for key, value in self.special_tokens_map_extended.items():
                 if isinstance(value, AddedToken):
-                    write_dict[key] = value.__getstate__()
+                    write_dict[key] = value.content  # Remove this edit once the related AttributeError is fixed upstream
                 else:
                     write_dict[key] = value
             f.write(json.dumps(write_dict, ensure_ascii=False))
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 067f793d..f9ccb762 100644
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -10,6 +10,8 @@ from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import torch
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from packaging import version
 from torch import nn
 from torch.utils.data.dataloader import DataLoader
@@ -17,6 +18,7 @@ from torch.utils.data.dataset import Dataset
 from torch.utils.data.distributed import DistributedSampler
 from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler
 from tqdm.auto import tqdm, trange
+from transformers import is_torch_tpu_available
 
 from .data.data_collator import DataCollator, default_data_collator
 from .file_utils import is_apex_available, is_torch_tpu_available
@@ -184,6 +186,7 @@ class Trainer:
         prediction_loss_only=False,
         tb_writer: Optional["SummaryWriter"] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,
+        compression_ctrl: PTCompressionAlgorithmController = None
     ):
         self.model = model.to(args.device)
         self.args = args
@@ -226,6 +229,8 @@ class Trainer:
                 FutureWarning,
             )
 
+        self.compression_ctrl = compression_ctrl
+
     def get_train_dataloader(self) -> DataLoader:
         """
         Returns the training :class:`~torch.utils.data.DataLoader`.
@@ -417,6 +422,9 @@ class Trainer:
 
         # Distributed training (should be after apex fp16 initialization)
         if self.args.local_rank != -1:
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
+
             model = torch.nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank],
@@ -450,7 +458,7 @@ class Trainer:
         epochs_trained = 0
         steps_trained_in_current_epoch = 0
         # Check if continuing training from a checkpoint
-        if model_path is not None:
+        if model_path is not None and self.compression_ctrl is None:
             # set global_step to global_step of last saved checkpoint from model path
             try:
                 self.global_step = int(model_path.split("-")[-1].split("/")[0])
@@ -496,7 +504,9 @@ class Trainer:
                     steps_trained_in_current_epoch -= 1
                     continue
 
-                tr_loss += self._training_step(model, inputs, optimizer)
+                curr_loss = self._training_step(model, inputs, optimizer)
+                epoch_iterator.set_postfix(loss=curr_loss)
+                tr_loss += curr_loss
 
                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (
                     # last step in epoch but step is always smaller than gradient_accumulation_steps
@@ -515,6 +525,10 @@ class Trainer:
 
                     scheduler.step()
                     model.zero_grad()
+
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.scheduler.step()
+
                     self.global_step += 1
                     self.epoch = epoch + (step + 1) / len(epoch_iterator)
 
@@ -531,6 +545,12 @@ class Trainer:
                         )
                         logging_loss = tr_loss
 
+                        if self.compression_ctrl is not None:
+                            logs["compression_loss"] = self.compression_ctrl.loss()
+                            compression_stats = self.compression_ctrl.statistics()
+                            for key, value in prepare_for_tensorboard(compression_stats).items():
+                                logs["compression/statistics/{0}".format(key)] = value
+
                         self._log(logs)
 
                     if self.args.evaluate_during_training and self.global_step % self.args.eval_steps == 0:
@@ -630,6 +651,10 @@ class Trainer:
         if self.args.gradient_accumulation_steps > 1:
             loss = loss / self.args.gradient_accumulation_steps
 
+        if self.compression_ctrl is not None:
+            compression_loss = self.compression_ctrl.loss()
+            loss += compression_loss
+
         if self.args.fp16:
             with amp.scale_loss(loss, optimizer) as scaled_loss:
                 scaled_loss.backward()
@@ -688,9 +713,12 @@ class Trainer:
         logger.info("Saving model checkpoint to %s", output_dir)
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
-        if not isinstance(self.model, PreTrainedModel):
-            raise ValueError("Trainer.model appears to not be a PreTrainedModel")
-        self.model.save_pretrained(output_dir)
+
+        model_to_save = (
+            self.model.module if hasattr(self.model, "module") and self.compression_ctrl is None else self.model
+        )  # Take care of distributed/parallel training
+
+        model_to_save.save_pretrained(output_dir, saved_module_override=model_to_save)
 
         # Good practice: save your training arguments together with the trained model
         torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
@@ -884,3 +912,30 @@ class Trainer:
         # truncate the dummy elements added by SequentialDistributedSampler
         output = concat[:num_total_examples]
         return output
+
+
+def get_train_dataloader_for_init(args, train_dataset, data_collator=None):
+    if is_torch_tpu_available():
+        train_sampler = get_tpu_sampler(train_dataset)
+    else:
+        from torch.utils.data import RandomSampler
+        from torch.utils.data import DistributedSampler
+        train_sampler = (
+            RandomSampler(train_dataset)
+            if args.local_rank == -1
+            else DistributedSampler(train_dataset)
+        )
+
+    if data_collator is None:
+        from transformers.data.data_collator import default_data_collator
+        data_collator = default_data_collator
+
+    from torch.utils.data import DataLoader
+    data_loader = DataLoader(
+        train_dataset,
+        batch_size=args.train_batch_size,
+        sampler=train_sampler,
+        collate_fn=data_collator,
+        drop_last=args.dataloader_drop_last,
+    )
+    return data_loader
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 9609cc91..d5658d7e 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -6,6 +6,8 @@ from dataclasses import dataclass, field
 from typing import Any, Dict, Optional, Tuple
 
 from .file_utils import cached_property, is_torch_available, is_torch_tpu_available, torch_required
+from nncf import NNCFConfig
+
 
 
 if is_torch_available():
@@ -210,6 +212,12 @@ class TrainingArguments:
     )
     debug: bool = field(default=False, metadata={"help": "Whether to print debug metrics on TPU"})
 
+    nncf_config: str = field(default=None,
+                             metadata={"help": "NNCF configuration .json file for compression-enabled training"})
+
+    to_onnx: str = field(default=None,
+                             metadata={"help": "Name of the ONNX model file to export the model to."})
+
     dataloader_drop_last: bool = field(
         default=False, metadata={"help": "Drop the last incomplete batch if it is not divisible by the batch size."}
     )
-- 
2.17.1

