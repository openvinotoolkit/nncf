From a847e7fe93d7f22789cad8db4dc8309c2a22703e Mon Sep 17 00:00:00 2001
From: Vasily Shamporov <vasily.shamporov@intel.com>
Date: Fri, 29 Nov 2019 18:33:05 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 examples/question-answering/run_squad.py      | 128 ++++++++++++++++--
 examples/text-classification/run_glue.py      |  67 ++++++++-
 examples/text-classification/run_xnli.py      | 127 +++++++++++++++--
 nncf_bert_config_squad.json                   |  43 ++++++
 ...config_squad_magnitude_sparsity_cubic.json |  31 +++++
 nncf_bert_config_xnli.json                    |  36 +++++
 nncf_distilbert_config_sst2.json              |  33 +++++
 nncf_mobilebert_config_squad_int8.json        |  43 ++++++
 nncf_roberta_config_mnli.json                 |  42 ++++++
 src/transformers/modeling_utils.py            |  23 +++-
 src/transformers/trainer.py                   |  33 ++++-
 src/transformers/training_args.py             |   5 +
 12 files changed, 575 insertions(+), 36 deletions(-)
 create mode 100644 nncf_bert_config_squad.json
 create mode 100644 nncf_bert_config_squad_magnitude_sparsity_cubic.json
 create mode 100644 nncf_bert_config_xnli.json
 create mode 100644 nncf_distilbert_config_sst2.json
 create mode 100644 nncf_mobilebert_config_squad_int8.json
 create mode 100644 nncf_roberta_config_mnli.json

diff --git a/examples/question-answering/run_squad.py b/examples/question-answering/run_squad.py
index 3fe887d6..b119cc4f 100644
--- a/examples/question-answering/run_squad.py
+++ b/examples/question-answering/run_squad.py
@@ -25,9 +25,11 @@ import timeit
 
 import numpy as np
 import torch
+from nncf.structures import QuantizationRangeInitArgs
 from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
 from torch.utils.data.distributed import DistributedSampler
 from tqdm import tqdm, trange
+from torch import onnx
 
 from transformers import (
     MODEL_FOR_QUESTION_ANSWERING_MAPPING,
@@ -52,6 +54,8 @@ try:
 except ImportError:
     from tensorboardX import SummaryWriter
 
+from nncf import NNCFConfig
+from nncf.initialization import InitializingDataLoader
 
 logger = logging.getLogger(__name__)
 
@@ -71,14 +75,19 @@ def to_list(tensor):
     return tensor.detach().cpu().tolist()
 
 
-def train(args, train_dataset, model, tokenizer):
+def get_train_dataloader(args, train_dataset):
+    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
+    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
+    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    return train_dataloader
+
+
+def train(args, train_dataset, model, tokenizer, compression_ctrl=None):
     """ Train the model """
     if args.local_rank in [-1, 0]:
         tb_writer = SummaryWriter()
 
-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    train_dataloader = get_train_dataloader(args, train_dataset)
 
     if args.max_steps > 0:
         t_total = args.max_steps
@@ -144,7 +153,7 @@ def train(args, train_dataset, model, tokenizer):
     epochs_trained = 0
     steps_trained_in_current_epoch = 0
     # Check if continuing training from a checkpoint
-    if os.path.exists(args.model_name_or_path):
+    if os.path.exists(args.model_name_or_path) and compression_ctrl is None:
         try:
             # set global_step to gobal_step of last saved checkpoint from model path
             checkpoint_suffix = args.model_name_or_path.split("-")[-1].split("/")[0]
@@ -200,6 +209,7 @@ def train(args, train_dataset, model, tokenizer):
                     )
 
             outputs = model(**inputs)
+
             # model outputs are always tuple in transformers (see doc)
             loss = outputs[0]
 
@@ -208,6 +218,10 @@ def train(args, train_dataset, model, tokenizer):
             if args.gradient_accumulation_steps > 1:
                 loss = loss / args.gradient_accumulation_steps
 
+            if compression_ctrl is not None:
+                compression_loss = compression_ctrl.loss()
+                loss += compression_loss
+
             if args.fp16:
                 with amp.scale_loss(loss, optimizer) as scaled_loss:
                     scaled_loss.backward()
@@ -215,6 +229,8 @@ def train(args, train_dataset, model, tokenizer):
                 loss.backward()
 
             tr_loss += loss.item()
+
+            epoch_iterator.set_postfix(loss=loss.item())
             if (step + 1) % args.gradient_accumulation_steps == 0:
                 if args.fp16:
                     torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
@@ -223,6 +239,10 @@ def train(args, train_dataset, model, tokenizer):
 
                 optimizer.step()
                 scheduler.step()  # Update learning rate schedule
+
+                if compression_ctrl is not None:
+                    compression_ctrl.scheduler.step()
+
                 model.zero_grad()
                 global_step += 1
 
@@ -237,14 +257,22 @@ def train(args, train_dataset, model, tokenizer):
                     tb_writer.add_scalar("loss", (tr_loss - logging_loss) / args.logging_steps, global_step)
                     logging_loss = tr_loss
 
+                    if compression_ctrl is not None:
+                        tb_writer.add_scalar("compression_loss", compression_loss.item())
+                        compression_stats = compression_ctrl.statistics()
+                        for key, value in compression_stats.items():
+                            if isinstance(value, (int, float)):
+                                tb_writer.add_scalar("compression/statistics/{0}".format(key), value,
+                                                     global_step)
+
                 # Save model checkpoint
                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
                     output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
                     if not os.path.exists(output_dir):
                         os.makedirs(output_dir)
-                    # Take care of distributed/parallel training
-                    model_to_save = model.module if hasattr(model, "module") else model
-                    model_to_save.save_pretrained(output_dir)
+
+                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training
+                    model_to_save.save_pretrained(output_dir, saved_module_override=model_to_save)
                     tokenizer.save_pretrained(output_dir)
 
                     torch.save(args, os.path.join(output_dir, "training_args.bin"))
@@ -257,10 +285,13 @@ def train(args, train_dataset, model, tokenizer):
             if args.max_steps > 0 and global_step > args.max_steps:
                 epoch_iterator.close()
                 break
+
+        if compression_ctrl is not None:
+            compression_ctrl.scheduler.epoch_step()
+
         if args.max_steps > 0 and global_step > args.max_steps:
             train_iterator.close()
             break
-
     if args.local_rank in [-1, 0]:
         tb_writer.close()
 
@@ -661,6 +692,11 @@ def main():
     parser.add_argument("--server_port", type=str, default="", help="Can be used for distant debugging.")
 
     parser.add_argument("--threads", type=int, default=1, help="multiple threads for converting example to features")
+    parser.add_argument('--to_onnx', type=str, metavar='PATH', default=None,
+                        help='Export to ONNX model by given path')
+    parser.add_argument('--nncf_config', type=str, help='path to NNCF config .json file to be used for compressed model'
+                                                        'fine-tuning')
+
     args = parser.parse_args()
 
     if args.doc_stride >= args.max_seq_length - args.max_query_length:
@@ -735,19 +771,67 @@ def main():
         do_lower_case=args.do_lower_case,
         cache_dir=args.cache_dir if args.cache_dir else None,
     )
-    model = AutoModelForQuestionAnswering.from_pretrained(
+
+    nncf_config = None
+    if args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = args.output_dir
+        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if args.do_train:
+            train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)
+            train_dataloader = get_train_dataloader(args, train_dataset)
+
+            class SquadInitializingDataloader(InitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    kwargs = {'attention_mask': dataloader_output[1],
+                              'start_positions': dataloader_output[3],
+                              'end_positions': dataloader_output[4]}
+                    if args.model_type != 'distilbert':
+                        kwargs['token_type_ids'] = None if args.model_type == 'xlm' else dataloader_output[2]
+                    if args.model_type in ['xlnet', 'xlm']:
+                        kwargs.update({'cls_index': dataloader_output[5],
+                                       'p_mask': dataloader_output[6]})
+                    return (dataloader_output[0],), kwargs
+
+            initializing_data_loader = SquadInitializingDataloader(train_dataloader)
+            nncf_config.register_extra_structs([QuantizationRangeInitArgs(initializing_data_loader)])
+
+    retval = AutoModelForQuestionAnswering.from_pretrained(
         args.model_name_or_path,
         from_tf=bool(".ckpt" in args.model_name_or_path),
         config=config,
         cache_dir=args.cache_dir if args.cache_dir else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and args.do_eval and not args.do_train
     )
 
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+
     if args.local_rank == 0:
         # Make sure only the first process in distributed training will download model & vocab
         torch.distributed.barrier()
 
+    if args.to_onnx:
+        if nncf_config is not None:
+            compression_ctrl.export_model(args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, args.max_seq_length], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), args.to_onnx)
+
     model.to(args.device)
 
+    if nncf_config is not None:
+        if not (args.local_rank == -1 or args.no_cuda):
+            compression_ctrl.distributed()
+
     logger.info("Training/evaluation parameters %s", args)
 
     # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.
@@ -764,7 +848,7 @@ def main():
     # Training
     if args.do_train:
         train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)
-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)
+        global_step, tr_loss = train(args, train_dataset, model, tokenizer, compression_ctrl)
         logger.info(" global_step = %s, average loss = %s", global_step, tr_loss)
 
     # Save the trained model and the tokenizer
@@ -778,14 +862,22 @@ def main():
         # They can then be reloaded using `from_pretrained()`
         # Take care of distributed/parallel training
         model_to_save = model.module if hasattr(model, "module") else model
-        model_to_save.save_pretrained(args.output_dir)
+        model_to_save.save_pretrained(args.output_dir, saved_module_override=model_to_save)
         tokenizer.save_pretrained(args.output_dir)
 
         # Good practice: save your training arguments together with the trained model
         torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
 
         # Load a trained model and vocabulary that you have fine-tuned
-        model = AutoModelForQuestionAnswering.from_pretrained(args.output_dir)  # , force_download=True)
+        retval = AutoModelForQuestionAnswering.from_pretrained(args.output_dir,
+                                                               nncf_config=nncf_config,
+                                                               nncf_eval=True if nncf_config is not None else False)
+
+        if nncf_config is None:
+            model = retval
+        else:
+            _, model = retval
+
         tokenizer = AutoTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)
         model.to(args.device)
 
@@ -810,7 +902,15 @@ def main():
         for checkpoint in checkpoints:
             # Reload the model
             global_step = checkpoint.split("-")[-1] if len(checkpoints) > 1 else ""
-            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)
+            retval = AutoModelForQuestionAnswering.from_pretrained(checkpoint,
+                                                                   nncf_config=nncf_config,
+                                                                   nncf_eval=True if nncf_config is not None else False)  # , force_download=True)
+
+            if nncf_config is None:
+                model = retval
+            else:
+                _, model = retval
+
             model.to(args.device)
 
             # Evaluate
diff --git a/examples/text-classification/run_glue.py b/examples/text-classification/run_glue.py
index cf9b765a..a79a2edd 100644
--- a/examples/text-classification/run_glue.py
+++ b/examples/text-classification/run_glue.py
@@ -24,8 +24,12 @@ from dataclasses import dataclass, field
 from typing import Callable, Dict, Optional
 
 import numpy as np
+from nncf import NNCFConfig
+from nncf.structures import QuantizationRangeInitArgs
+from nncf.initialization import InitializingDataLoader
 
-from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset
+from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset, \
+    is_torch_tpu_available
 from transformers import GlueDataTrainingArguments as DataTrainingArguments
 from transformers import (
     HfArgumentParser,
@@ -41,6 +45,32 @@ from transformers import (
 logger = logging.getLogger(__name__)
 
 
+def get_train_dataloader_for_init(args, train_dataset, data_collator=None):
+    if is_torch_tpu_available():
+        from transformers.trainer import get_tpu_sampler
+        train_sampler = get_tpu_sampler(train_dataset)
+    else:
+        from torch.utils.data import RandomSampler
+        from torch.utils.data import DistributedSampler
+        train_sampler = (
+            RandomSampler(train_dataset)
+            if args.local_rank == -1
+            else DistributedSampler(train_dataset)
+        )
+
+    if data_collator is None:
+        from transformers.data.data_collator import default_data_collator
+        data_collator = default_data_collator
+
+    from torch.utils.data import DataLoader
+    data_loader = DataLoader(
+        train_dataset,
+        batch_size=args.train_batch_size,
+        sampler=train_sampler,
+        collate_fn=data_collator
+    )
+    return data_loader
+
 @dataclass
 class ModelArguments:
     """
@@ -126,14 +156,44 @@ def main():
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
+
+    # Get datasets
+    train_dataset = GlueDataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
+    eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode="dev") if training_args.do_eval else None
+    test_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode="test") if training_args.do_predict else None
+
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if training_args.do_train:
+            train_dataloader = get_train_dataloader_for_init(training_args, train_dataset)
+
+            class GlueInitializingDataLoader(InitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    return (), dataloader_output
+
+            nncf_config.register_extra_structs(
+                [QuantizationRangeInitArgs(GlueInitializingDataLoader(train_dataloader))])
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
         model_args.model_name_or_path,
         from_tf=bool(".ckpt" in model_args.model_name_or_path),
         config=config,
         cache_dir=model_args.cache_dir,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
     )
 
-    # Get datasets
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+            
     train_dataset = (
         GlueDataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None
     )
@@ -165,6 +225,7 @@ def main():
         train_dataset=train_dataset,
         eval_dataset=eval_dataset,
         compute_metrics=build_compute_metrics_fn(data_args.task_name),
+        compression_ctrl=compression_ctrl
     )
 
     # Training
diff --git a/examples/text-classification/run_xnli.py b/examples/text-classification/run_xnli.py
index f0ee5120..b29f56eb 100644
--- a/examples/text-classification/run_xnli.py
+++ b/examples/text-classification/run_xnli.py
@@ -25,8 +25,11 @@ import random
 
 import numpy as np
 import torch
+from nncf.structures import QuantizationRangeInitArgs
 from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
 from torch.utils.data.distributed import DistributedSampler
+from torch import onnx
+import torch.distributed
 from tqdm import tqdm, trange
 
 from transformers import (
@@ -48,6 +51,8 @@ try:
 except ImportError:
     from tensorboardX import SummaryWriter
 
+from nncf import NNCFConfig
+from nncf.initialization import InitializingDataLoader
 
 logger = logging.getLogger(__name__)
 
@@ -60,14 +65,19 @@ def set_seed(args):
         torch.cuda.manual_seed_all(args.seed)
 
 
-def train(args, train_dataset, model, tokenizer):
+def get_train_dataloader(args, train_dataset):
+    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
+    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
+    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    return train_dataloader
+
+
+def train(args, train_dataset, model, tokenizer, compression_ctrl=None):
     """ Train the model """
     if args.local_rank in [-1, 0]:
         tb_writer = SummaryWriter()
 
-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+    train_dataloader = get_train_dataloader(args, train_dataset)
 
     if args.max_steps > 0:
         t_total = args.max_steps
@@ -132,7 +142,7 @@ def train(args, train_dataset, model, tokenizer):
     epochs_trained = 0
     steps_trained_in_current_epoch = 0
     # Check if continuing training from a checkpoint
-    if os.path.exists(args.model_name_or_path):
+    if os.path.exists(args.model_name_or_path) and compression_ctrl is None:
         # set global_step to gobal_step of last saved checkpoint from model path
         global_step = int(args.model_name_or_path.split("-")[-1].split("/")[0])
         epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)
@@ -164,7 +174,9 @@ def train(args, train_dataset, model, tokenizer):
                 inputs["token_type_ids"] = (
                     batch[2] if args.model_type in ["bert"] else None
                 )  # XLM and DistilBERT don't use segment_ids
+
             outputs = model(**inputs)
+
             loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
 
             if args.n_gpu > 1:
@@ -172,6 +184,10 @@ def train(args, train_dataset, model, tokenizer):
             if args.gradient_accumulation_steps > 1:
                 loss = loss / args.gradient_accumulation_steps
 
+            if compression_ctrl is not None:
+                compression_loss = compression_ctrl.loss()
+                loss += compression_loss
+
             if args.fp16:
                 with amp.scale_loss(loss, optimizer) as scaled_loss:
                     scaled_loss.backward()
@@ -179,6 +195,8 @@ def train(args, train_dataset, model, tokenizer):
                 loss.backward()
 
             tr_loss += loss.item()
+            epoch_iterator.set_postfix(loss=loss.item())
+
             if (step + 1) % args.gradient_accumulation_steps == 0:
                 if args.fp16:
                     torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
@@ -188,6 +206,10 @@ def train(args, train_dataset, model, tokenizer):
                 optimizer.step()
                 scheduler.step()  # Update learning rate schedule
                 model.zero_grad()
+
+                if compression_ctrl is not None:
+                    compression_ctrl.scheduler.step()
+
                 global_step += 1
 
                 if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:
@@ -202,13 +224,21 @@ def train(args, train_dataset, model, tokenizer):
                     tb_writer.add_scalar("loss", (tr_loss - logging_loss) / args.logging_steps, global_step)
                     logging_loss = tr_loss
 
+                    if compression_ctrl is not None:
+                        tb_writer.add_scalar("compression_loss", compression_loss.item())
+                        compression_stats = compression_ctrl.statistics()
+                        for key, value in compression_stats.items():
+                            if isinstance(value, (int, float)):
+                                tb_writer.add_scalar("compression/statistics/{0}".format(key), value,
+                                                     global_step)
+
                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
                     # Save model checkpoint
                     output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
                     if not os.path.exists(output_dir):
                         os.makedirs(output_dir)
                     model_to_save = (
-                        model.module if hasattr(model, "module") else model
+                        model.module if hasattr(model, "module") and compression_ctrl is None else model
                     )  # Take care of distributed/parallel training
                     model_to_save.save_pretrained(output_dir)
                     tokenizer.save_pretrained(output_dir)
@@ -223,6 +253,10 @@ def train(args, train_dataset, model, tokenizer):
             if args.max_steps > 0 and global_step > args.max_steps:
                 epoch_iterator.close()
                 break
+
+        if compression_ctrl is not None:
+            compression_ctrl.scheduler.epoch_step()
+
         if args.max_steps > 0 and global_step > args.max_steps:
             train_iterator.close()
             break
@@ -474,6 +508,10 @@ def main():
     parser.add_argument("--local_rank", type=int, default=-1, help="For distributed training: local_rank")
     parser.add_argument("--server_ip", type=str, default="", help="For distant debugging.")
     parser.add_argument("--server_port", type=str, default="", help="For distant debugging.")
+    parser.add_argument('--to_onnx', type=str, metavar='PATH', default=None,
+                        help='Export to ONNX model by given path')
+    parser.add_argument('--nncf_config', type=str, help='path to NNCF config .json file to be used for compressed model'
+                                                        'fine-tuning')
     args = parser.parse_args()
 
     if (
@@ -551,24 +589,72 @@ def main():
         do_lower_case=args.do_lower_case,
         cache_dir=args.cache_dir,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
+
+
+    nncf_config = None
+    if args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = args.output_dir
+        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+        if args.do_train:
+            train_dataset = load_and_cache_examples(args, args.task_name,  tokenizer, evaluate=False)
+            train_dataloader = get_train_dataloader(args, train_dataset)
+
+            class XnliInitializingDataloader(InitializingDataLoader):
+                def get_inputs(self, dataloader_output):
+                    kwargs = {'attention_mask': dataloader_output[1],
+                              'labels': dataloader_output[3]}
+                    if args.model_type != 'distilbert':
+                        kwargs['token_type_ids'] = dataloader_output[2] if args.model_type in [
+                            'bert'] else None  # XLM and DistilBERT don't use segment_ids
+                    return (dataloader_output[0],), kwargs
+
+            initializing_data_loader = XnliInitializingDataloader(train_dataloader,)
+            nncf_config.register_extra_structs([QuantizationRangeInitArgs(initializing_data_loader)])
+
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
         args.model_name_or_path,
         from_tf=bool(".ckpt" in args.model_name_or_path),
         config=config,
         cache_dir=args.cache_dir,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and args.do_eval and not args.do_train
     )
 
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
     if args.local_rank == 0:
         torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
 
+    if args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, args.max_seq_length], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), args.to_onnx)
+
     model.to(args.device)
 
+    if nncf_config is not None:
+        if not (args.local_rank == -1 or args.no_cuda):
+            compression_ctrl.distributed()
+
     logger.info("Training/evaluation parameters %s", args)
 
     # Training
     if args.do_train:
         train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)
-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)
+        global_step, tr_loss = train(args, train_dataset, model, tokenizer, compression_ctrl)
         logger.info(" global_step = %s, average loss = %s", global_step, tr_loss)
 
     # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()
@@ -581,16 +667,25 @@ def main():
         # Save a trained model, configuration and tokenizer using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         model_to_save = (
-            model.module if hasattr(model, "module") else model
+            model.module if hasattr(model, "module") and nncf_config is None else model
         )  # Take care of distributed/parallel training
-        model_to_save.save_pretrained(args.output_dir)
+
+        model_to_save.save_pretrained(args.output_dir, saved_module_override=model_to_save)
         tokenizer.save_pretrained(args.output_dir)
 
         # Good practice: save your training arguments together with the trained model
         torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
 
         # Load a trained model and vocabulary that you have fine-tuned
-        model = AutoModelForSequenceClassification.from_pretrained(args.output_dir)
+        retval = AutoModelForSequenceClassification.from_pretrained(args.output_dir,
+                                                                   nncf_config=nncf_config,
+                                                                   nncf_eval=True if nncf_config is not None else False)
+
+        if nncf_config is None:
+            model = retval
+        else:
+            _, model = retval
+
         tokenizer = AutoTokenizer.from_pretrained(args.output_dir)
         model.to(args.device)
 
@@ -608,7 +703,15 @@ def main():
             global_step = checkpoint.split("-")[-1] if len(checkpoints) > 1 else ""
             prefix = checkpoint.split("/")[-1] if checkpoint.find("checkpoint") != -1 else ""
 
-            model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
+            retval = AutoModelForSequenceClassification.from_pretrained(checkpoint,
+                                                                        nncf_config=nncf_config,
+                                                                        nncf_eval=nncf_config is not None)
+
+            if nncf_config is None:
+                model = retval
+            else:
+                _, model = retval
+
             model.to(args.device)
             result = evaluate(args, model, tokenizer, prefix=prefix)
             result = dict((k + "_{}".format(global_step), v) for k, v in result.items())
diff --git a/nncf_bert_config_squad.json b/nncf_bert_config_squad.json
new file mode 100644
index 00000000..7b0b4c6e
--- /dev/null
+++ b/nncf_bert_config_squad.json
@@ -0,0 +1,43 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_steps": 4,
+                "type": "percentile",
+                "min_percentile": 0.01,
+                "max_percentile": 99.99
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "{re}BertIntermediate\\[intermediate\\]/__mul___0",
+            "{re}BertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"],
+        "activations":
+        {
+            "mode": "symmetric",
+            "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/NNCFLinear\\[(key)|(query)|(value)\\].*",
+            "{re}BertForQuestionAnswering/BertModel\\[bert\\]/BertEmbeddings\\[embeddings\\]/__add___0",
+            "{re}BertForQuestionAnswering/BertModel\\[bert\\]/BertEmbeddings\\[embeddings\\]/__add___1",
+            "{re}BertOutput\\[output\\]/__add___0"]
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_bert_config_squad_magnitude_sparsity_cubic.json b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
new file mode 100644
index 00000000..b4452e8d
--- /dev/null
+++ b/nncf_bert_config_squad_magnitude_sparsity_cubic.json
@@ -0,0 +1,31 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "magnitude_sparsity",
+        "params": {
+            "schedule": "polynomial",
+            "power": 3,
+            "sparsity_init": 0.0,
+            "sparsity_target": 0.8,
+            "sparsity_target_epoch": 40,
+            "sparsity_freeze_epoch": 60,
+            "update_per_optimizer_step": true,
+            "steps_per_epoch": 1109,
+            "weight_importance": "abs"
+        },
+        "ignored_scopes": ["{re}.*NNCFEmbedding"]
+    }
+}
diff --git a/nncf_bert_config_xnli.json b/nncf_bert_config_xnli.json
new file mode 100644
index 00000000..d916689b
--- /dev/null
+++ b/nncf_bert_config_xnli.json
@@ -0,0 +1,36 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_steps": 1
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "{re}BertIntermediate\\[intermediate\\]/__mul___0",
+            "{re}BertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric"
+        }
+    }
+}
diff --git a/nncf_distilbert_config_sst2.json b/nncf_distilbert_config_sst2.json
new file mode 100644
index 00000000..746b3e1b
--- /dev/null
+++ b/nncf_distilbert_config_sst2.json
@@ -0,0 +1,33 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_steps": 1
+            }
+        },
+        "ignored_scopes": [
+            "{re}TransformerBlock\\[[0-9]*\\]/FFN\\[ffn\\]/__mul___0",
+            "{re}TransformerBlock\\[[0-9]*\\]/FFN\\[ffn\\]/NNCFLinear\\[lin1\\]/linear_0"
+        ],
+        "activations":
+        {
+            "mode": "symmetric"
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_mobilebert_config_squad_int8.json b/nncf_mobilebert_config_squad_int8.json
new file mode 100644
index 00000000..107360f8
--- /dev/null
+++ b/nncf_mobilebert_config_squad_int8.json
@@ -0,0 +1,43 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_steps": 8,
+                "type": "percentile",
+                "min_percentile": 0.01,
+                "max_percentile": 99.99
+            }
+        },
+        "ignored_scopes": ["{re}MobileBertSelfAttention\\[self\\]/__add___0",
+            "{re}MobileBertIntermediate\\[intermediate\\]/NNCFLinear\\[dense\\]/linear_0"],
+        "activations":
+        {
+            "mode": "symmetric",
+            "ignored_scopes": [
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___0",
+            "{re}MobileBertForQuestionAnswering/MobileBertModel\\[mobilebert\\]/MobileBertEmbeddings\\[embeddings\\]/__add___1",
+            "{re}MobileBertOutput\\[output\\]/__add___0",
+            "{re}NoNorm\\[LayerNorm\\]/__mul___0"]
+        },
+        "weights":
+        {
+            "mode": "symmetric",
+            "signed": true
+        }
+    }
+}
diff --git a/nncf_roberta_config_mnli.json b/nncf_roberta_config_mnli.json
new file mode 100644
index 00000000..3f2eb189
--- /dev/null
+++ b/nncf_roberta_config_mnli.json
@@ -0,0 +1,42 @@
+{
+    "input_info": [
+        {
+            "keyword": "input_ids",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        },
+        {
+            "keyword": "attention_mask",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "ones"
+        },
+        {
+            "keyword": "token_type_ids",
+            "sample_size": [1, 128],
+            "type": "long",
+            "filler": "zeros"
+        }
+    ],
+    "compression": {
+        "algorithm": "quantization",
+        "initializer": {
+            "range": {
+                "num_init_steps": 1
+            }
+        },
+        "ignored_scopes": ["{re}BertSelfAttention\\[self\\]/__add___0",
+            "RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[out_proj]",
+            "RobertaForSequenceClassification/RobertaClassificationHead[classifier]/Linear[dense]"
+        ],
+        "activations":
+        {
+            "mode": "asymmetric"
+        },
+        "weights":
+        {
+            "mode": "asymmetric"
+        }
+    }
+}
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 3f84bb4b..6a2e1fa3 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -24,6 +24,9 @@ from torch import Tensor, device, dtype, nn
 from torch.nn import CrossEntropyLoss
 from torch.nn import functional as F
 
+from nncf import create_compressed_model
+
+
 from .activations import get_activation
 from .configuration_utils import PretrainedConfig
 from .file_utils import (
@@ -470,7 +473,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin):
 
         self.base_model._prune_heads(heads_to_prune)
 
-    def save_pretrained(self, save_directory):
+    def save_pretrained(self, save_directory,
+                        saved_module_override=None):
         """ Save a model and its configuration file to a directory, so that it
             can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.
 
@@ -490,6 +494,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin):
         # If we save using the predefined names, we can load using `from_pretrained`
         output_model_file = os.path.join(save_directory, WEIGHTS_NAME)
 
+        module_to_save = model_to_save if saved_module_override is None else saved_module_override
+
         if getattr(self.config, "xla_device", False):
             import torch_xla.core.xla_model as xm
 
@@ -497,10 +503,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin):
                 # Save configuration file
                 model_to_save.config.save_pretrained(save_directory)
             # xm.save takes care of saving only from master
-            xm.save(model_to_save.state_dict(), output_model_file)
+            xm.save(module_to_save.state_dict(), output_model_file)
         else:
             model_to_save.config.save_pretrained(save_directory)
-            torch.save(model_to_save.state_dict(), output_model_file)
+            torch.save(module_to_save.state_dict(), output_model_file)
 
         logger.info("Model weights saved in {}".format(output_model_file))
 
@@ -585,6 +591,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin):
         output_loading_info = kwargs.pop("output_loading_info", False)
         local_files_only = kwargs.pop("local_files_only", False)
         use_cdn = kwargs.pop("use_cdn", True)
+        nncf_config = kwargs.pop("nncf_config", None)
+        nncf_eval = kwargs.pop("nncf_eval", False)
 
         # Load config if we don't provide a configuration
         if not isinstance(config, PretrainedConfig):
@@ -677,6 +685,11 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin):
                     "If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
                 )
 
+        if nncf_config is not None and nncf_eval:
+            compression_algo_controller, model = create_compressed_model(model, nncf_config,
+                                                                         resuming_state_dict=state_dict)
+            return compression_algo_controller, model
+
         missing_keys = []
         unexpected_keys = []
         error_msgs = []
@@ -772,6 +785,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin):
         # Set model in evaluation mode to deactivate DropOut modules by default
         model.eval()
 
+        if nncf_config is not None:
+            compression_algo_controller, model = create_compressed_model(model, nncf_config)
+            return compression_algo_controller, model
+
         if output_loading_info:
             loading_info = {
                 "missing_keys": missing_keys,
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 0aa9cdf3..2f22364b 100644
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -10,6 +10,7 @@ from typing import Callable, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
+from nncf.compression_method_api import CompressionAlgorithmController
 from packaging import version
 from torch import nn
 from torch.utils.data.dataloader import DataLoader
@@ -178,6 +179,7 @@ class Trainer:
         prediction_loss_only=False,
         tb_writer: Optional["SummaryWriter"] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,
+        compression_ctrl: CompressionAlgorithmController = None
     ):
         """
         Trainer is a simple but feature-complete training and eval loop for PyTorch,
@@ -219,6 +221,8 @@ class Trainer:
             # We'll find a more elegant and not need to do this in the future.
             self.model.config.xla_device = True
 
+        self.compression_ctrl = compression_ctrl
+
     def get_train_dataloader(self) -> DataLoader:
         if self.train_dataset is None:
             raise ValueError("Trainer: training requires a train_dataset.")
@@ -395,6 +399,9 @@ class Trainer:
 
         # Distributed training (should be after apex fp16 initialization)
         if self.args.local_rank != -1:
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
+
             model = torch.nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank],
@@ -428,7 +435,7 @@ class Trainer:
         epochs_trained = 0
         steps_trained_in_current_epoch = 0
         # Check if continuing training from a checkpoint
-        if model_path is not None:
+        if model_path is not None and self.compression_ctrl is None:
             # set global_step to global_step of last saved checkpoint from model path
             try:
                 self.global_step = int(model_path.split("-")[-1].split("/")[0])
@@ -489,6 +496,10 @@ class Trainer:
 
                     scheduler.step()
                     model.zero_grad()
+
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.scheduler.step()
+
                     self.global_step += 1
                     self.epoch = epoch + (step + 1) / len(epoch_iterator)
 
@@ -505,6 +516,13 @@ class Trainer:
                         )
                         logging_loss = tr_loss
 
+                        if self.compression_ctrl is not None:
+                            logs["compression_loss"] = self.compression_ctrl.loss().item()
+                            compression_stats = self.compression_ctrl.statistics()
+                            for key, value in compression_stats.items():
+                                if isinstance(value, (int, float)):
+                                    logs["compression/statistics/{0}".format(key)] = value
+
                         self._log(logs)
 
                         if self.args.evaluate_during_training:
@@ -594,6 +612,10 @@ class Trainer:
         if self.args.gradient_accumulation_steps > 1:
             loss = loss / self.args.gradient_accumulation_steps
 
+        if self.compression_ctrl is not None:
+            compression_loss = self.compression_ctrl.loss()
+            loss += compression_loss
+
         if self.args.fp16:
             with amp.scale_loss(loss, optimizer) as scaled_loss:
                 scaled_loss.backward()
@@ -653,9 +675,12 @@ class Trainer:
         logger.info("Saving model checkpoint to %s", output_dir)
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
-        if not isinstance(self.model, PreTrainedModel):
-            raise ValueError("Trainer.model appears to not be a PreTrainedModel")
-        self.model.save_pretrained(output_dir)
+
+        model_to_save = (
+            self.model.module if hasattr(self.model, "module") and self.compression_ctrl is None else self.model
+        )  # Take care of distributed/parallel training
+
+        model_to_save.save_pretrained(output_dir, saved_module_override=model_to_save)
 
         # Good practice: save your training arguments together with the trained model
         torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index fc97e63d..225ebd25 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -6,6 +6,8 @@ from dataclasses import dataclass, field
 from typing import Any, Dict, Optional, Tuple
 
 from .file_utils import cached_property, is_torch_available, is_torch_tpu_available, torch_required
+from nncf import NNCFConfig
+
 
 
 if is_torch_available():
@@ -135,6 +137,9 @@ class TrainingArguments:
     )
     tpu_metrics_debug: bool = field(default=False, metadata={"help": "TPU: Whether to print debug metrics"})
 
+    nncf_config: str = field(default=None,
+                             metadata={"help": "NNCF configuration .json file for compression-enabled training"})
+
     dataloader_drop_last: bool = field(
         default=False, metadata={"help": "Drop the last incomplete batch if it is not divisible by the batch size."}
     )
-- 
2.17.1

