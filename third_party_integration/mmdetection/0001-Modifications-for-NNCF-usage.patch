From 87bcfbe8c054447153d37971fc3d9b2a4f8ff127 Mon Sep 17 00:00:00 2001
From: Aleksei Kashapov <aleksei.kashapov@intel.com>
Date: Mon, 14 Dec 2020 20:41:54 +0300
Subject: [PATCH] NNCF patch for mmdet

---
 .../mask_rcnn_r50_caffe_fpn_1x_coco_int8.py   |  62 +++++++++++
 configs/pascal_voc/ssd300_voc_int8.py         |  94 ++++++++++++++++
 .../retinanet/retinanet_r50_fpn_1x_int8.py    |  40 +++++++
 mmdet/apis/train.py                           |  35 +++++-
 mmdet/core/__init__.py                        |   1 +
 mmdet/core/nncf/__init__.py                   |   9 ++
 mmdet/core/nncf/hooks.py                      |  24 ++++
 mmdet/core/nncf/utils.py                      | 105 ++++++++++++++++++
 mmdet/models/dense_heads/anchor_head.py       |   1 +
 mmdet/models/dense_heads/base_dense_head.py   |   5 +-
 mmdet/models/detectors/base.py                |  17 ++-
 .../roi_heads/mask_heads/fcn_mask_head.py     |  13 ++-
 mmdet/models/roi_heads/standard_roi_head.py   |   8 +-
 tools/pytorch2onnx.py                         |  25 +++--
 tools/train.py                                |   6 +
 15 files changed, 416 insertions(+), 29 deletions(-)
 create mode 100644 configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py
 create mode 100644 configs/pascal_voc/ssd300_voc_int8.py
 create mode 100644 configs/retinanet/retinanet_r50_fpn_1x_int8.py
 create mode 100644 mmdet/core/nncf/__init__.py
 create mode 100644 mmdet/core/nncf/hooks.py
 create mode 100644 mmdet/core/nncf/utils.py

diff --git a/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py b/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py
new file mode 100644
index 0000000..81a90ba
--- /dev/null
+++ b/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py
@@ -0,0 +1,62 @@
+_base_ = './mask_rcnn_r50_fpn_1x_coco.py'
+model = dict(
+    pretrained='open-mmlab://detectron2/resnet50_caffe',
+    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'))
+# use caffe img_norm
+img_norm_cfg = dict(
+    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1333, 800),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    train=dict(pipeline=train_pipeline),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
+
+# overload some training params:
+optimizer = dict(type='SGD', lr=0.002, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(grad_clip=None)
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.001,
+    step=[3, 5])
+total_epochs = 6
+work_dir = './work_dirs/mask_rcnn_r50_caffe_fpn_1x_coco_int8'
+
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth
+load_from = './mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'
+
+find_unused_parameters=True
+
+# nncf config:
+input_size = 800
+nncf_config = dict(compression=[dict(algorithm="quantization",
+                                     weights=dict(per_channel=True),
+                                     initializer=dict(range=dict(num_init_steps=5,
+                                                                 type='min_max')))],
+                   log_dir=work_dir)
diff --git a/configs/pascal_voc/ssd300_voc_int8.py b/configs/pascal_voc/ssd300_voc_int8.py
new file mode 100644
index 0000000..4112ea7
--- /dev/null
+++ b/configs/pascal_voc/ssd300_voc_int8.py
@@ -0,0 +1,94 @@
+_base_ = [
+    '../_base_/models/ssd300.py', '../_base_/datasets/voc0712.py',
+    '../_base_/default_runtime.py'
+]
+model = dict(
+    bbox_head=dict(
+        num_classes=20, anchor_generator=dict(basesize_ratio_range=(0.2,
+                                                                    0.9))))
+# dataset settings
+dataset_type = 'VOCDataset'
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile', to_float32=True),
+    dict(type='LoadAnnotations', with_bbox=True),
+    dict(
+        type='PhotoMetricDistortion',
+        brightness_delta=32,
+        contrast_range=(0.5, 1.5),
+        saturation_range=(0.5, 1.5),
+        hue_delta=18),
+    dict(
+        type='Expand',
+        mean=img_norm_cfg['mean'],
+        to_rgb=img_norm_cfg['to_rgb'],
+        ratio_range=(1, 4)),
+    dict(
+        type='MinIoURandomCrop',
+        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
+        min_crop_size=0.3),
+    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(300, 300),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=False),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    samples_per_gpu=4,
+    workers_per_gpu=4,
+    train=dict(
+        type='RepeatDataset', times=10, dataset=dict(pipeline=train_pipeline)),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
+# optimizer
+optimizer = dict(type='SGD', lr=1e-4, momentum=0.9, weight_decay=5e-4)
+optimizer_config = dict()
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.1,
+    step=[16, 20])
+checkpoint_config = dict(interval=1)
+
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+
+# runtime settings
+total_epochs = 24
+
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/ssd300_voc_int8'
+workflow = [('train', 1)]
+
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth
+load_from = './ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth'
+resume_from = None
+
+# nncf config
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10,
+                                                                 type="threesigma")))],
+                   log_dir=work_dir)
diff --git a/configs/retinanet/retinanet_r50_fpn_1x_int8.py b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
new file mode 100644
index 0000000..7788165
--- /dev/null
+++ b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
@@ -0,0 +1,40 @@
+_base_ = [
+    '../_base_/models/retinanet_r50_fpn.py',
+    '../_base_/datasets/coco_detection.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+# optimizer
+optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
+
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 10,
+    step=[8, 11])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 3
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/retinanet_r50_fpn_1x_int8'
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/models/retinanet_r50_fpn_2x_20190616-75574209.pth
+load_from = './retinanet_r50_fpn_2x_20190616-75574209.pth'
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+input_size = 800
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10)))],
+                   log_dir=work_dir)
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index c314d42..a45ee5f 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -4,13 +4,16 @@ import numpy as np
 import torch
 from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
-                         OptimizerHook, build_optimizer)
+                         Fp16OptimizerHook, OptimizerHook, build_optimizer, load_checkpoint)
 from mmcv.utils import build_from_cfg
 
-from mmdet.core import DistEvalHook, EvalHook, Fp16OptimizerHook
+from mmdet.core import DistEvalHook, EvalHook, Fp16OptimizerHook, CompressionHook
 from mmdet.datasets import build_dataloader, build_dataset
 from mmdet.utils import get_root_logger
 
+from nncf.utils import get_all_modules
+from mmdet.core.nncf import wrap_nncf_model
+
 
 def set_random_seed(seed, deterministic=False):
     """Set random seed.
@@ -67,13 +70,25 @@ def train_detector(model,
             seed=cfg.seed) for ds in dataset
     ]
 
+    if cfg.load_from:
+        load_checkpoint(model=model, filename=cfg.load_from)
+
     # put model on gpus
+    model = model.cuda()
+
+    # nncf model wrapper
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_ctrl = wrap_nncf_model(model, cfg, data_loaders[0])
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_ctrl = None
+
     if distributed:
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
         model = MMDistributedDataParallel(
-            model.cuda(),
+            model,
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
@@ -81,6 +96,9 @@ def train_detector(model,
         model = MMDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
+    if cfg.ENABLE_COMPRESSION and distributed:
+        compression_ctrl.distributed()
+
     # build runner
     optimizer = build_optimizer(model, cfg.optimizer)
     runner = EpochBasedRunner(
@@ -108,6 +126,8 @@ def train_detector(model,
                                    cfg.get('momentum_config', None))
     if distributed:
         runner.register_hook(DistSamplerSeedHook())
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
 
     # register eval hooks
     if validate:
@@ -136,8 +156,11 @@ def train_detector(model,
             hook = build_from_cfg(hook_cfg, HOOKS)
             runner.register_hook(hook, priority=priority)
 
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
+
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
-    elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_ctrl=compression_ctrl)
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index f8eb6cb..14f2c89 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -5,3 +5,4 @@ from .fp16 import *  # noqa: F401, F403
 from .mask import *  # noqa: F401, F403
 from .post_processing import *  # noqa: F401, F403
 from .utils import *  # noqa: F401, F403
+from .nncf import *
diff --git a/mmdet/core/nncf/__init__.py b/mmdet/core/nncf/__init__.py
new file mode 100644
index 0000000..bc743b9
--- /dev/null
+++ b/mmdet/core/nncf/__init__.py
@@ -0,0 +1,9 @@
+from .hooks import CompressionHook
+from .utils import wrap_nncf_model
+from .utils import load_checkpoint
+
+__all__ = [
+    'CompressionHook',
+    'wrap_nncf_model',
+    'load_checkpoint',
+]
diff --git a/mmdet/core/nncf/hooks.py b/mmdet/core/nncf/hooks.py
new file mode 100644
index 0000000..d6b3887
--- /dev/null
+++ b/mmdet/core/nncf/hooks.py
@@ -0,0 +1,24 @@
+from texttable import Texttable
+from mmcv.runner.hooks.hook import Hook
+
+
+class CompressionHook(Hook):
+    def __init__(self, compression_ctrl=None):
+        self.compression_ctrl = compression_ctrl
+
+    def after_train_iter(self, runner):
+        self.compression_ctrl.scheduler.step()
+
+    def after_train_epoch(self, runner):
+        self.compression_ctrl.scheduler.epoch_step()
+
+    def before_run(self, runner):
+        runner.logger.info(get_printable_stats(self.compression_ctrl.statistics()))
+
+
+def get_printable_stats(stats):
+    for key, val in stats.items():
+        if isinstance(val, Texttable):
+            return '{}\n{}'.format(key, val.draw())
+        else:
+            return '{}: {}'.format(key, val)
diff --git a/mmdet/core/nncf/utils.py b/mmdet/core/nncf/utils.py
new file mode 100644
index 0000000..bb47455
--- /dev/null
+++ b/mmdet/core/nncf/utils.py
@@ -0,0 +1,105 @@
+import pathlib
+from collections import OrderedDict
+
+import torch
+from nncf.initialization import InitializingDataLoader
+from nncf.structures import QuantizationRangeInitArgs
+
+from nncf import NNCFConfig
+from nncf import load_state
+from nncf import create_compressed_model
+from nncf import nncf_model_input
+
+
+def wrap_nncf_model(model, cfg, data_loader_for_init=None):
+    pathlib.Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
+    nncf_config = NNCFConfig(cfg.nncf_config)
+
+    if data_loader_for_init is not None:
+        wrapped_loader = MMInitializeDataLoader(data_loader_for_init)
+
+        nncf_config.register_extra_structs([QuantizationRangeInitArgs(wrapped_loader)])
+
+    input_size = nncf_config.get(
+        'input_sample_size', (1, 3, cfg.input_size, cfg.input_size)
+    )
+
+    def wrap_inputs(args, kwargs):
+        # during model's forward
+        if 'img' in kwargs:
+            img = kwargs['img']
+            if isinstance(img, list):
+                assert len(img) == 1, 'Input list must have a length 1'
+                assert torch.is_tensor(img[0]), 'Input for a model must be a tensor'
+                img[0] = nncf_model_input(img[0])
+            else:
+                assert torch.is_tensor(img), 'Input for a model must be a tensor'
+                img = nncf_model_input(img)
+            kwargs['img'] = img
+            return args, kwargs
+
+        # during model's export
+        if isinstance(args, tuple):
+            assert torch.is_tensor(args[0][0]), 'Input for a model must be a tensor'
+            for img in args[0]:
+                img = nncf_model_input(img)
+
+        return args, kwargs
+
+    def dummy_forward(model):
+        device = next(model.parameters()).device
+        input_args = ([nncf_model_input(torch.randn(input_size).to(device)), ],)
+        input_kwargs = dict(return_loss=False, dummy_forward=True)
+        model(*input_args, **input_kwargs)
+
+    model.dummy_forward_fn = dummy_forward
+
+    checkpoint = None
+    if cfg.get('nncf_load_from', None):
+        checkpoint = load_checkpoint(model, cfg.nncf_load_from)
+
+    compression_ctrl, model = create_compressed_model(
+        model,
+        nncf_config,
+        resuming_state_dict=checkpoint,
+        dummy_forward_fn=dummy_forward,
+        wrap_inputs_fn=wrap_inputs,
+    )
+
+    return model, compression_ctrl
+
+
+def load_checkpoint(model, filename, map_location=None, strict=False):
+    """Load checkpoint from a file or URI.
+
+    Args:
+        model (Module): Module to load checkpoint.
+        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.
+        map_location (str): Same as :func:`torch.load`.
+        strict (bool): Whether to allow different params for the model and
+            checkpoint.
+
+    Returns:
+        dict or OrderedDict: The loaded checkpoint.
+    """
+    # load checkpoint from modelzoo or file or url
+    checkpoint = torch.load(filename, map_location=map_location)
+    # get state_dict from checkpoint
+    if isinstance(checkpoint, OrderedDict):
+        state_dict = checkpoint
+    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
+        state_dict = checkpoint['state_dict']
+    else:
+        raise RuntimeError('No state_dict found in checkpoint file {}'.format(filename))
+    _ = load_state(model, state_dict, strict)
+    return checkpoint
+
+
+class MMInitializeDataLoader(InitializingDataLoader):
+    def get_inputs(self, dataloader_output):
+        # redefined InitializingDataLoader because
+        # of DataContainer format in mmdet
+        kwargs = {k: v.data[0] for k, v in dataloader_output.items()}
+        return (), kwargs
+
+    # get_targets TBD
diff --git a/mmdet/models/dense_heads/anchor_head.py b/mmdet/models/dense_heads/anchor_head.py
index 2faec70..7d0676b 100644
--- a/mmdet/models/dense_heads/anchor_head.py
+++ b/mmdet/models/dense_heads/anchor_head.py
@@ -1,6 +1,7 @@
 import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import (anchor_inside_flags, build_anchor_generator,
                         build_assigner, build_bbox_coder, build_sampler,
diff --git a/mmdet/models/dense_heads/base_dense_head.py b/mmdet/models/dense_heads/base_dense_head.py
index de11e4a..0fcdc4d 100644
--- a/mmdet/models/dense_heads/base_dense_head.py
+++ b/mmdet/models/dense_heads/base_dense_head.py
@@ -1,6 +1,7 @@
 from abc import ABCMeta, abstractmethod
 
 import torch.nn as nn
+from nncf.dynamic_graph.context import no_nncf_trace
 
 
 class BaseDenseHead(nn.Module, metaclass=ABCMeta):
@@ -51,7 +52,9 @@ class BaseDenseHead(nn.Module, metaclass=ABCMeta):
             loss_inputs = outs + (gt_bboxes, img_metas)
         else:
             loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
-        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
+
+        with no_nncf_trace():
+            losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
         if proposal_cfg is None:
             return losses
         else:
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index 914f476..d82b8c1 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -131,6 +131,9 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             if not isinstance(var, list):
                 raise TypeError(f'{name} must be a list, but got {type(var)}')
 
+        if 'dummy_forward' in kwargs:
+            return self.forward_dummy(imgs[0])
+
         num_augs = len(imgs)
         if num_augs != len(img_metas):
             raise ValueError(f'num of augmentations ({len(imgs)}) '
@@ -153,8 +156,12 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             assert 'proposals' not in kwargs
             return self.aug_test(imgs, img_metas, **kwargs)
 
-    @auto_fp16(apply_to=('img', ))
-    def forward(self, img, img_metas, return_loss=True, **kwargs):
+    @abstractmethod
+    def forward_dummy(self, img, **kwargs):
+        pass
+
+    @auto_fp16(apply_to=('img',))
+    def forward(self, img, img_metas=[], return_loss=True, **kwargs):
         """Calls either :func:`forward_train` or :func:`forward_test` depending
         on whether ``return_loss`` is ``True``.
 
@@ -204,7 +211,7 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
 
         return loss, log_vars
 
-    def train_step(self, data, optimizer):
+    def train_step(self, data, optimizer, compression_ctrl=None):
         """The iteration step during training.
 
         This method defines an iteration step during training, except for the
@@ -234,6 +241,10 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
         losses = self(**data)
         loss, log_vars = self._parse_losses(losses)
 
+        if compression_ctrl is not None:
+            compression_loss = compression_ctrl.loss()
+            loss += compression_loss
+
         outputs = dict(
             loss=loss, log_vars=log_vars, num_samples=len(data['img_metas']))
 
diff --git a/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py b/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
index 1885f7c..0f02a9e 100644
--- a/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
+++ b/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
@@ -10,6 +10,8 @@ from torch.nn.modules.utils import _pair
 from mmdet.core import auto_fp16, force_fp32, mask_target
 from mmdet.models.builder import HEADS, build_loss
 
+from nncf.dynamic_graph.context import no_nncf_trace
+
 BYTES_PER_FLOAT = 4
 # TODO: This memory limit may be too much or too little. It would be better to
 # determine it based on available resources.
@@ -140,11 +142,12 @@ class FCNMaskHead(nn.Module):
         if mask_pred.size(0) == 0:
             loss_mask = mask_pred.sum() * 0
         else:
-            if self.class_agnostic:
-                loss_mask = self.loss_mask(mask_pred, mask_targets,
-                                           torch.zeros_like(labels))
-            else:
-                loss_mask = self.loss_mask(mask_pred, mask_targets, labels)
+            with no_nncf_trace():
+                if self.class_agnostic:
+                    loss_mask = self.loss_mask(mask_pred, mask_targets,
+                                               torch.zeros_like(labels))
+                else:
+                    loss_mask = self.loss_mask(mask_pred, mask_targets, labels)
         loss['loss_mask'] = loss_mask
         return loss
 
diff --git a/mmdet/models/roi_heads/standard_roi_head.py b/mmdet/models/roi_heads/standard_roi_head.py
index 8822f8f..ace3cbe 100644
--- a/mmdet/models/roi_heads/standard_roi_head.py
+++ b/mmdet/models/roi_heads/standard_roi_head.py
@@ -1,4 +1,5 @@
 import torch
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler
 from ..builder import HEADS, build_head, build_roi_extractor
@@ -153,9 +154,10 @@ class StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):
 
         bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,
                                                   gt_labels, self.train_cfg)
-        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],
-                                        bbox_results['bbox_pred'], rois,
-                                        *bbox_targets)
+        with no_nncf_trace():
+            loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],
+                                            bbox_results['bbox_pred'], rois,
+                                            *bbox_targets)
 
         bbox_results.update(loss_bbox=loss_bbox)
         return bbox_results
diff --git a/tools/pytorch2onnx.py b/tools/pytorch2onnx.py
index 232a2f4..30117ae 100644
--- a/tools/pytorch2onnx.py
+++ b/tools/pytorch2onnx.py
@@ -10,17 +10,17 @@ import torch
 from mmcv.runner import load_checkpoint
 
 from mmdet.models import build_detector
+from mmdet.core.nncf.utils import wrap_nncf_model
 
 try:
     from mmcv.onnx.symbolic import register_extra_symbolics
 except ModuleNotFoundError:
     raise NotImplementedError('please update mmcv to version>=v1.0.4')
 
-
 def pytorch2onnx(model,
                  input_img,
                  input_shape,
-                 opset_version=11,
+                 opset_version=10,
                  show=False,
                  output_file='tmp.onnx',
                  verify=False,
@@ -45,17 +45,19 @@ def pytorch2onnx(model,
     # onnx.export does not support kwargs
     origin_forward = model.forward
     model.forward = partial(
-        model.forward, img_metas=[[one_meta]], return_loss=False)
+        model.forward, img_metas=[[one_meta]], return_loss=False, dummy_forward=True)
     # pytorch has some bug in pytorch1.3, we have to fix it
     # by replacing these existing op
     register_extra_symbolics(opset_version)
-    torch.onnx.export(
-        model, ([one_img]),
-        output_file,
-        export_params=True,
-        keep_initializers_as_inputs=True,
-        verbose=show,
-        opset_version=opset_version)
+    with torch.no_grad():
+        torch.onnx.export(
+            model, ([one_img]),
+            output_file,
+            export_params=True,
+            keep_initializers_as_inputs=True,
+            verbose=show,
+            opset_version=opset_version,
+            enable_onnx_checker=False)
     model.forward = origin_forward
     print(f'Successfully exported ONNX model: {output_file}')
     if verify:
@@ -158,7 +160,8 @@ if __name__ == '__main__':
     model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)
     checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
 
-    # conver model to onnx file
+    cfg.nncf_load_from = args.checkpoint
+    model, compression_ctrl = wrap_nncf_model(model, cfg, None)
     pytorch2onnx(
         model,
         args.input_img,
diff --git a/tools/train.py b/tools/train.py
index c36b2a0..519e915 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -139,6 +139,12 @@ def main():
     logger.info(f'Distributed training: {distributed}')
     logger.info(f'Config:\n{cfg.pretty_text}')
 
+    if 'nncf_config' in cfg:
+        logger.info('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.ENABLE_COMPRESSION = True
+    else:
+        cfg.ENABLE_COMPRESSION = False
+
     # set random seeds
     if args.seed is not None:
         logger.info(f'Set random seed to {args.seed}, '
-- 
2.28.0.windows.1

