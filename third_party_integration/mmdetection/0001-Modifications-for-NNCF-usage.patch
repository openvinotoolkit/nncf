From 6ca30efa578d12df5e8e4368a08c91a2bcd24aef Mon Sep 17 00:00:00 2001
From: Ivan Lazarevich <ivan.lazarevich@intel.com>
Date: Tue, 28 Apr 2020 17:47:34 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 configs/pascal_voc/ssd300_voc_int8.py         | 95 +++++++++++++++++++
 .../retinanet/retinanet_r50_fpn_1x_int8.py    | 40 ++++++++
 mmdet/apis/train.py                           | 36 +++++--
 mmdet/core/__init__.py                        |  1 +
 mmdet/core/nncf/__init__.py                   |  9 ++
 mmdet/core/nncf/hooks.py                      | 24 +++++
 mmdet/core/nncf/utils.py                      | 72 ++++++++++++++
 mmdet/models/dense_heads/anchor_head.py       |  1 +
 mmdet/models/dense_heads/base_dense_head.py   |  5 +-
 mmdet/models/detectors/base.py                | 15 ++-
 mmdet/models/detectors/single_stage.py        |  1 +
 tools/train.py                                |  6 ++
 12 files changed, 296 insertions(+), 9 deletions(-)
 create mode 100644 configs/pascal_voc/ssd300_voc_int8.py
 create mode 100644 configs/retinanet/retinanet_r50_fpn_1x_int8.py
 create mode 100644 mmdet/core/nncf/__init__.py
 create mode 100644 mmdet/core/nncf/hooks.py
 create mode 100644 mmdet/core/nncf/utils.py

diff --git a/configs/pascal_voc/ssd300_voc_int8.py b/configs/pascal_voc/ssd300_voc_int8.py
new file mode 100644
index 0000000..cae6870
--- /dev/null
+++ b/configs/pascal_voc/ssd300_voc_int8.py
@@ -0,0 +1,95 @@
+_base_ = [
+    '../_base_/models/ssd300.py', '../_base_/datasets/voc0712.py',
+    '../_base_/default_runtime.py'
+]
+model = dict(
+    bbox_head=dict(
+        num_classes=20, anchor_generator=dict(basesize_ratio_range=(0.2,
+                                                                    0.9))))
+# dataset settings
+dataset_type = 'VOCDataset'
+data_root = 'voc/'
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile', to_float32=True),
+    dict(type='LoadAnnotations', with_bbox=True),
+    dict(
+        type='PhotoMetricDistortion',
+        brightness_delta=32,
+        contrast_range=(0.5, 1.5),
+        saturation_range=(0.5, 1.5),
+        hue_delta=18),
+    dict(
+        type='Expand',
+        mean=img_norm_cfg['mean'],
+        to_rgb=img_norm_cfg['to_rgb'],
+        ratio_range=(1, 4)),
+    dict(
+        type='MinIoURandomCrop',
+        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
+        min_crop_size=0.3),
+    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(300, 300),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=False),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    samples_per_gpu=4,
+    workers_per_gpu=4,
+    train=dict(
+        type='RepeatDataset', times=10, dataset=dict(pipeline=train_pipeline)),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
+# optimizer
+optimizer = dict(type='SGD', lr=1e-4, momentum=0.9, weight_decay=5e-4)
+optimizer_config = dict()
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.1,
+    step=[16, 20])
+checkpoint_config = dict(interval=1)
+
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+
+# runtime settings
+total_epochs = 24
+
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/ssd300_voc_int8'
+workflow = [('train', 1)]
+
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth
+load_from = './ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth'
+resume_from = None
+
+# nncf config
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10,
+                                                                 type="threesigma")))],
+                   log_dir=work_dir)
diff --git a/configs/retinanet/retinanet_r50_fpn_1x_int8.py b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
new file mode 100644
index 0000000..7788165
--- /dev/null
+++ b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
@@ -0,0 +1,40 @@
+_base_ = [
+    '../_base_/models/retinanet_r50_fpn.py',
+    '../_base_/datasets/coco_detection.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+# optimizer
+optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
+
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 10,
+    step=[8, 11])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 3
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/retinanet_r50_fpn_1x_int8'
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/models/retinanet_r50_fpn_2x_20190616-75574209.pth
+load_from = './retinanet_r50_fpn_2x_20190616-75574209.pth'
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+input_size = 800
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10)))],
+                   log_dir=work_dir)
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index c314d42..1361682 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -4,13 +4,16 @@ import numpy as np
 import torch
 from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
-                         OptimizerHook, build_optimizer)
+                         build_optimizer, load_checkpoint)
 from mmcv.utils import build_from_cfg
 
-from mmdet.core import DistEvalHook, EvalHook, Fp16OptimizerHook
+from mmdet.core import DistEvalHook, EvalHook, Fp16OptimizerHook, CompressionHook
 from mmdet.datasets import build_dataloader, build_dataset
 from mmdet.utils import get_root_logger
 
+from nncf.utils import get_all_modules
+from mmdet.core.nncf import wrap_nncf_model
+
 
 def set_random_seed(seed, deterministic=False):
     """Set random seed.
@@ -67,13 +70,26 @@ def train_detector(model,
             seed=cfg.seed) for ds in dataset
     ]
 
+    if cfg.load_from:
+        load_checkpoint(model=model,
+                        filename=cfg.load_from)
+
     # put model on gpus
+    model = model.cuda()
+
+    # nncf model wrapper
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_ctrl = wrap_nncf_model(model, cfg, data_loaders[0])
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_ctrl = None
+
     if distributed:
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
         model = MMDistributedDataParallel(
-            model.cuda(),
+            model,
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
@@ -81,6 +97,9 @@ def train_detector(model,
         model = MMDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
+    if cfg.ENABLE_COMPRESSION and distributed:
+        compression_ctrl.distributed()
+
     # build runner
     optimizer = build_optimizer(model, cfg.optimizer)
     runner = EpochBasedRunner(
@@ -108,6 +127,8 @@ def train_detector(model,
                                    cfg.get('momentum_config', None))
     if distributed:
         runner.register_hook(DistSamplerSeedHook())
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
 
     # register eval hooks
     if validate:
@@ -136,8 +157,11 @@ def train_detector(model,
             hook = build_from_cfg(hook_cfg, HOOKS)
             runner.register_hook(hook, priority=priority)
 
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
+
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
-    elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_ctrl=compression_ctrl)
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index f8eb6cb..14f2c89 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -5,3 +5,4 @@ from .fp16 import *  # noqa: F401, F403
 from .mask import *  # noqa: F401, F403
 from .post_processing import *  # noqa: F401, F403
 from .utils import *  # noqa: F401, F403
+from .nncf import *
diff --git a/mmdet/core/nncf/__init__.py b/mmdet/core/nncf/__init__.py
new file mode 100644
index 0000000..bc743b9
--- /dev/null
+++ b/mmdet/core/nncf/__init__.py
@@ -0,0 +1,9 @@
+from .hooks import CompressionHook
+from .utils import wrap_nncf_model
+from .utils import load_checkpoint
+
+__all__ = [
+    'CompressionHook',
+    'wrap_nncf_model',
+    'load_checkpoint',
+]
diff --git a/mmdet/core/nncf/hooks.py b/mmdet/core/nncf/hooks.py
new file mode 100644
index 0000000..d6b3887
--- /dev/null
+++ b/mmdet/core/nncf/hooks.py
@@ -0,0 +1,24 @@
+from texttable import Texttable
+from mmcv.runner.hooks.hook import Hook
+
+
+class CompressionHook(Hook):
+    def __init__(self, compression_ctrl=None):
+        self.compression_ctrl = compression_ctrl
+
+    def after_train_iter(self, runner):
+        self.compression_ctrl.scheduler.step()
+
+    def after_train_epoch(self, runner):
+        self.compression_ctrl.scheduler.epoch_step()
+
+    def before_run(self, runner):
+        runner.logger.info(get_printable_stats(self.compression_ctrl.statistics()))
+
+
+def get_printable_stats(stats):
+    for key, val in stats.items():
+        if isinstance(val, Texttable):
+            return '{}\n{}'.format(key, val.draw())
+        else:
+            return '{}: {}'.format(key, val)
diff --git a/mmdet/core/nncf/utils.py b/mmdet/core/nncf/utils.py
new file mode 100644
index 0000000..ae0d6f7
--- /dev/null
+++ b/mmdet/core/nncf/utils.py
@@ -0,0 +1,72 @@
+import pathlib
+from collections import OrderedDict
+
+import torch
+from nncf.initialization import InitializingDataLoader
+from nncf.structures import QuantizationRangeInitArgs
+
+from nncf import NNCFConfig
+from nncf import load_state
+from nncf import create_compressed_model
+
+
+def wrap_nncf_model(model, cfg, data_loader_for_init=None):
+    pathlib.Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
+    nncf_config = NNCFConfig(cfg.nncf_config)
+
+    if data_loader_for_init is not None:
+        wrapped_loader = MMInitializeDataLoader(data_loader_for_init)
+
+        nncf_config.register_extra_structs([QuantizationRangeInitArgs(wrapped_loader)])
+
+    input_size = nncf_config.get(
+        'input_sample_size', (1, 3, cfg.input_size, cfg.input_size)
+    )
+
+    def dummy_forward(model):
+        device = next(model.parameters()).device
+        input_args = ([torch.randn(input_size).to(device),],)
+        input_kwargs = dict(return_loss=False, dummy_forward=True)
+        model(*input_args, **input_kwargs)
+
+    model.dummy_forward_fn = dummy_forward
+    compression_ctrl, model = create_compressed_model(
+        model, nncf_config, dummy_forward_fn=dummy_forward
+    )
+    return model, compression_ctrl
+
+
+def load_checkpoint(model, filename, map_location=None, strict=False):
+    """Load checkpoint from a file or URI.
+
+    Args:
+        model (Module): Module to load checkpoint.
+        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.
+        map_location (str): Same as :func:`torch.load`.
+        strict (bool): Whether to allow different params for the model and
+            checkpoint.
+
+    Returns:
+        dict or OrderedDict: The loaded checkpoint.
+    """
+    # load checkpoint from modelzoo or file or url
+    checkpoint = torch.load(filename, map_location=map_location)
+    # get state_dict from checkpoint
+    if isinstance(checkpoint, OrderedDict):
+        state_dict = checkpoint
+    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
+        state_dict = checkpoint['state_dict']
+    else:
+        raise RuntimeError('No state_dict found in checkpoint file {}'.format(filename))
+    _ = load_state(model, state_dict, strict)
+    return checkpoint
+
+
+class MMInitializeDataLoader(InitializingDataLoader):
+    def get_inputs(self, dataloader_output):
+        # redefined InitializingDataLoader because
+        # of DataContainer format in mmdet
+        kwargs = {k: v.data[0] for k, v in dataloader_output.items()}
+        return (), kwargs
+
+    # get_targets TBD
diff --git a/mmdet/models/dense_heads/anchor_head.py b/mmdet/models/dense_heads/anchor_head.py
index 2faec70..7d0676b 100644
--- a/mmdet/models/dense_heads/anchor_head.py
+++ b/mmdet/models/dense_heads/anchor_head.py
@@ -1,6 +1,7 @@
 import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import (anchor_inside_flags, build_anchor_generator,
                         build_assigner, build_bbox_coder, build_sampler,
diff --git a/mmdet/models/dense_heads/base_dense_head.py b/mmdet/models/dense_heads/base_dense_head.py
index de11e4a..0fcdc4d 100644
--- a/mmdet/models/dense_heads/base_dense_head.py
+++ b/mmdet/models/dense_heads/base_dense_head.py
@@ -1,6 +1,7 @@
 from abc import ABCMeta, abstractmethod
 
 import torch.nn as nn
+from nncf.dynamic_graph.context import no_nncf_trace
 
 
 class BaseDenseHead(nn.Module, metaclass=ABCMeta):
@@ -51,7 +52,9 @@ class BaseDenseHead(nn.Module, metaclass=ABCMeta):
             loss_inputs = outs + (gt_bboxes, img_metas)
         else:
             loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
-        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
+
+        with no_nncf_trace():
+            losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
         if proposal_cfg is None:
             return losses
         else:
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index 914f476..d6672ca 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -131,6 +131,9 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             if not isinstance(var, list):
                 raise TypeError(f'{name} must be a list, but got {type(var)}')
 
+        if 'dummy_forward' in kwargs:
+            return self.forward_dummy(imgs[0])
+
         num_augs = len(imgs)
         if num_augs != len(img_metas):
             raise ValueError(f'num of augmentations ({len(imgs)}) '
@@ -153,8 +156,12 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             assert 'proposals' not in kwargs
             return self.aug_test(imgs, img_metas, **kwargs)
 
+    @abstractmethod
+    def forward_dummy(self, img, **kwargs):
+        pass
+
     @auto_fp16(apply_to=('img', ))
-    def forward(self, img, img_metas, return_loss=True, **kwargs):
+    def forward(self, img, img_metas=[], return_loss=True, **kwargs):
         """Calls either :func:`forward_train` or :func:`forward_test` depending
         on whether ``return_loss`` is ``True``.
 
@@ -204,7 +211,7 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
 
         return loss, log_vars
 
-    def train_step(self, data, optimizer):
+    def train_step(self, data, optimizer, compression_ctrl=None):
         """The iteration step during training.
 
         This method defines an iteration step during training, except for the
@@ -234,6 +241,10 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
         losses = self(**data)
         loss, log_vars = self._parse_losses(losses)
 
+        if compression_ctrl is not None:
+            compression_loss = compression_ctrl.loss()
+            loss += compression_loss
+
         outputs = dict(
             loss=loss, log_vars=log_vars, num_samples=len(data['img_metas']))
 
diff --git a/mmdet/models/detectors/single_stage.py b/mmdet/models/detectors/single_stage.py
index 3977bdf..4159ea4 100644
--- a/mmdet/models/detectors/single_stage.py
+++ b/mmdet/models/detectors/single_stage.py
@@ -1,5 +1,6 @@
 import torch
 import torch.nn as nn
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import bbox2result
 from ..builder import DETECTORS, build_backbone, build_head, build_neck
diff --git a/tools/train.py b/tools/train.py
index c36b2a0..519e915 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -139,6 +139,12 @@ def main():
     logger.info(f'Distributed training: {distributed}')
     logger.info(f'Config:\n{cfg.pretty_text}')
 
+    if 'nncf_config' in cfg:
+        logger.info('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.ENABLE_COMPRESSION = True
+    else:
+        cfg.ENABLE_COMPRESSION = False
+
     # set random seeds
     if args.seed is not None:
         logger.info(f'Set random seed to {args.seed}, '
-- 
2.17.1

