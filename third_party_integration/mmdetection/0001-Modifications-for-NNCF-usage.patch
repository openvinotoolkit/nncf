From a884a13ca8be97a8ba7686df9b64dbaabb1ee04e Mon Sep 17 00:00:00 2001
From: Ivan Lazarevich <ivan.lazarevich@intel.com>
Date: Tue, 28 Apr 2020 17:47:34 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 configs/pascal_voc/ssd300_voc_int8.py         | 95 +++++++++++++++++++
 .../retinanet/retinanet_r50_fpn_1x_int8.py    | 40 ++++++++
 mmdet/apis/train.py                           | 42 ++++++--
 mmdet/core/__init__.py                        |  1 +
 mmdet/core/nncf/__init__.py                   |  9 ++
 mmdet/core/nncf/hooks.py                      | 25 +++++
 mmdet/core/nncf/utils.py                      | 86 +++++++++++++++++
 mmdet/models/dense_heads/anchor_head.py       |  1 +
 mmdet/models/detectors/base.py                |  9 +-
 mmdet/models/detectors/single_stage.py        |  8 +-
 tools/train.py                                |  6 ++
 11 files changed, 311 insertions(+), 11 deletions(-)
 create mode 100644 configs/pascal_voc/ssd300_voc_int8.py
 create mode 100644 configs/retinanet/retinanet_r50_fpn_1x_int8.py
 create mode 100644 mmdet/core/nncf/__init__.py
 create mode 100644 mmdet/core/nncf/hooks.py
 create mode 100644 mmdet/core/nncf/utils.py

diff --git a/configs/pascal_voc/ssd300_voc_int8.py b/configs/pascal_voc/ssd300_voc_int8.py
new file mode 100644
index 0000000..0fff6fc
--- /dev/null
+++ b/configs/pascal_voc/ssd300_voc_int8.py
@@ -0,0 +1,95 @@
+_base_ = [
+    '../_base_/models/ssd300.py', '../_base_/datasets/voc0712.py',
+    '../_base_/default_runtime.py'
+]
+model = dict(
+    bbox_head=dict(
+        num_classes=20, anchor_generator=dict(basesize_ratio_range=(0.2,
+                                                                    0.9))))
+# dataset settings
+dataset_type = 'VOCDataset'
+data_root = 'VOCdevkit/'
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile', to_float32=True),
+    dict(type='LoadAnnotations', with_bbox=True),
+    dict(
+        type='PhotoMetricDistortion',
+        brightness_delta=32,
+        contrast_range=(0.5, 1.5),
+        saturation_range=(0.5, 1.5),
+        hue_delta=18),
+    dict(
+        type='Expand',
+        mean=img_norm_cfg['mean'],
+        to_rgb=img_norm_cfg['to_rgb'],
+        ratio_range=(1, 4)),
+    dict(
+        type='MinIoURandomCrop',
+        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
+        min_crop_size=0.3),
+    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(300, 300),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=False),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    samples_per_gpu=4,
+    workers_per_gpu=4,
+    train=dict(
+        type='RepeatDataset', times=10, dataset=dict(pipeline=train_pipeline)),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
+# optimizer
+optimizer = dict(type='SGD', lr=1e-4, momentum=0.9, weight_decay=5e-4)
+optimizer_config = dict()
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.1,
+    step=[16, 20])
+checkpoint_config = dict(interval=1)
+
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+
+# runtime settings
+total_epochs = 24
+
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/ssd300_voc_int8'
+workflow = [('train', 1)]
+
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth
+load_from = './ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth'
+resume_from = None
+
+# nncf config
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10,
+                                                                 type="threesigma")))],
+                   log_dir=work_dir)
diff --git a/configs/retinanet/retinanet_r50_fpn_1x_int8.py b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
new file mode 100644
index 0000000..7788165
--- /dev/null
+++ b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
@@ -0,0 +1,40 @@
+_base_ = [
+    '../_base_/models/retinanet_r50_fpn.py',
+    '../_base_/datasets/coco_detection.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+# optimizer
+optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
+
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 10,
+    step=[8, 11])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 3
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/retinanet_r50_fpn_1x_int8'
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/models/retinanet_r50_fpn_2x_20190616-75574209.pth
+load_from = './retinanet_r50_fpn_2x_20190616-75574209.pth'
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+input_size = 800
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10)))],
+                   log_dir=work_dir)
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index 39982d0..d38285b 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -5,13 +5,16 @@ import numpy as np
 import torch
 import torch.distributed as dist
 from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
-from mmcv.runner import DistSamplerSeedHook, Runner
+from mmcv.runner import DistSamplerSeedHook, Runner, load_checkpoint
 
 from mmdet.core import (DistEvalHook, DistOptimizerHook, EvalHook,
-                        Fp16OptimizerHook, build_optimizer)
+                        Fp16OptimizerHook, CompressionHook, build_optimizer)
 from mmdet.datasets import build_dataloader, build_dataset
 from mmdet.utils import get_root_logger
 
+from nncf.utils import get_all_modules
+from mmdet.core.nncf import wrap_nncf_model
+
 
 def set_random_seed(seed, deterministic=False):
     """Set random seed.
@@ -55,7 +58,7 @@ def parse_losses(losses):
     return loss, log_vars
 
 
-def batch_processor(model, data, train_mode):
+def batch_processor(model, data, compression_ctrl, train_mode):
     """Process a data batch.
 
     This method is required as an argument of Runner, which defines how to
@@ -74,6 +77,10 @@ def batch_processor(model, data, train_mode):
     losses = model(**data)
     loss, log_vars = parse_losses(losses)
 
+    if compression_ctrl is not None:
+        compression_loss = compression_ctrl.loss()
+        loss += compression_loss
+
     outputs = dict(
         loss=loss, log_vars=log_vars, num_samples=len(data['img'].data))
 
@@ -102,13 +109,26 @@ def train_detector(model,
             seed=cfg.seed) for ds in dataset
     ]
 
+    if cfg.load_from:
+        load_checkpoint(model=model,
+                        filename=cfg.load_from)
+
     # put model on gpus
+    model = model.cuda()
+
+    # nncf model wrapper
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_ctrl = wrap_nncf_model(model, cfg, data_loaders[0])
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_ctrl = None
+
     if distributed:
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
         model = MMDistributedDataParallel(
-            model.cuda(),
+            model,
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
@@ -116,6 +136,9 @@ def train_detector(model,
         model = MMDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
+    if cfg.ENABLE_COMPRESSION and distributed:
+        compression_ctrl.distributed()
+
     # build runner
     optimizer = build_optimizer(model, cfg.optimizer)
     runner = Runner(
@@ -144,6 +167,8 @@ def train_detector(model,
                                    cfg.get('momentum_config', None))
     if distributed:
         runner.register_hook(DistSamplerSeedHook())
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
 
     # register eval hooks
     if validate:
@@ -158,8 +183,11 @@ def train_detector(model,
         eval_hook = DistEvalHook if distributed else EvalHook
         runner.register_hook(eval_hook(val_dataloader, **eval_cfg))
 
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
+
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
-    elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_ctrl=compression_ctrl)
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index 675c7b9..6efa709 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -6,3 +6,4 @@ from .mask import *  # noqa: F401, F403
 from .optimizer import *  # noqa: F401, F403
 from .post_processing import *  # noqa: F401, F403
 from .utils import *  # noqa: F401, F403
+from .nncf import *
diff --git a/mmdet/core/nncf/__init__.py b/mmdet/core/nncf/__init__.py
new file mode 100644
index 0000000..bc743b9
--- /dev/null
+++ b/mmdet/core/nncf/__init__.py
@@ -0,0 +1,9 @@
+from .hooks import CompressionHook
+from .utils import wrap_nncf_model
+from .utils import load_checkpoint
+
+__all__ = [
+    'CompressionHook',
+    'wrap_nncf_model',
+    'load_checkpoint',
+]
diff --git a/mmdet/core/nncf/hooks.py b/mmdet/core/nncf/hooks.py
new file mode 100644
index 0000000..9f8dbc3
--- /dev/null
+++ b/mmdet/core/nncf/hooks.py
@@ -0,0 +1,25 @@
+from texttable import Texttable
+from mmcv.runner.hooks.hook import Hook
+
+
+class CompressionHook(Hook):
+    def __init__(self, compression_ctrl=None):
+        self.compression_ctrl = compression_ctrl
+
+    def after_train_iter(self, runner):
+        self.compression_ctrl.scheduler.step()
+
+    def after_train_epoch(self, runner):
+        self.compression_ctrl.scheduler.epoch_step()
+
+    def before_run(self, runner):
+        runner.logger.info(print_statistics(self.compression_ctrl.statistics()))
+
+
+def print_statistics(stats):
+    for key, val in stats.items():
+        if isinstance(val, Texttable):
+            logger.info(key)
+            logger.info(val.draw())
+        else:
+            logger.info('{}: {}'.format(key, val))
diff --git a/mmdet/core/nncf/utils.py b/mmdet/core/nncf/utils.py
new file mode 100644
index 0000000..f1513b1
--- /dev/null
+++ b/mmdet/core/nncf/utils.py
@@ -0,0 +1,86 @@
+import pathlib
+from collections import OrderedDict
+
+import torch
+from nncf.structures import QuantizationRangeInitArgs
+
+from nncf import NNCFConfig
+from nncf import load_state
+from nncf import create_compressed_model
+
+
+def wrap_nncf_model(model, cfg, data_loader_for_init=None):
+    pathlib.Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
+    nncf_config = NNCFConfig(cfg.nncf_config)
+
+    if data_loader_for_init is not None:
+        mmdet_kwargs = dict(return_loss=False, dummy_forward=True)
+        device = next(model.parameters()).device
+        wrapped_loader = MMInitializeDataLoader(
+            data_loader=data_loader_for_init, kwargs=mmdet_kwargs, device=device
+        )
+
+        nncf_config.register_extra_structs([QuantizationRangeInitArgs(wrapped_loader)])
+
+    input_size = nncf_config.get(
+        'input_sample_size', (1, 3, cfg.input_size, cfg.input_size)
+    )
+
+    def dummy_forward(model):
+        device = next(model.parameters()).device
+        input_args = ([torch.randn(input_size).to(device),],)
+        input_kwargs = dict(return_loss=False, dummy_forward=True)
+        model(*input_args, **input_kwargs)
+
+    model.dummy_forward_fn = dummy_forward
+    compression_ctrl, model = create_compressed_model(
+        model, nncf_config, dummy_forward_fn=dummy_forward
+    )
+    return model, compression_ctrl
+
+
+def load_checkpoint(model, filename, map_location=None, strict=False):
+    """Load checkpoint from a file or URI.
+
+    Args:
+        model (Module): Module to load checkpoint.
+        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.
+        map_location (str): Same as :func:`torch.load`.
+        strict (bool): Whether to allow different params for the model and
+            checkpoint.
+
+    Returns:
+        dict or OrderedDict: The loaded checkpoint.
+    """
+    # load checkpoint from modelzoo or file or url
+    checkpoint = torch.load(filename, map_location=map_location)
+    # get state_dict from checkpoint
+    if isinstance(checkpoint, OrderedDict):
+        state_dict = checkpoint
+    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
+        state_dict = checkpoint['state_dict']
+    else:
+        raise RuntimeError('No state_dict found in checkpoint file {}'.format(filename))
+    _ = load_state(model, state_dict, strict)
+    return checkpoint
+
+
+class MMInitializeDataLoader:
+    def __init__(self, data_loader, kwargs, device):
+        self.data_loader = data_loader
+        self.data_iterator = self.data_loader.__iter__()
+        self.kwargs = kwargs
+        self.device = device
+        self.num_workers = data_loader.num_workers
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        # redefined InitializeDataLoader because
+        # of DataContainer format in mmdet
+        input = self.data_iterator.__next__()
+        input_tensor = input['img']._data[0].to(self.device)
+        return ([input_tensor,], None, self.kwargs)
+
+
diff --git a/mmdet/models/dense_heads/anchor_head.py b/mmdet/models/dense_heads/anchor_head.py
index 2d8f515..21c8115 100644
--- a/mmdet/models/dense_heads/anchor_head.py
+++ b/mmdet/models/dense_heads/anchor_head.py
@@ -1,6 +1,7 @@
 import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import (anchor_inside_flags, build_anchor_generator,
                         build_assigner, build_bbox_coder, build_sampler,
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index bd2d76d..6c01a89 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -111,6 +111,9 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             if not isinstance(var, list):
                 raise TypeError(f'{name} must be a list, but got {type(var)}')
 
+        if kwargs['dummy_forward']:
+            return self.forward_dummy(imgs[0])
+
         num_augs = len(imgs)
         if num_augs != len(img_metas):
             raise ValueError(f'num of augmentations ({len(imgs)}) '
@@ -134,8 +137,12 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             assert 'proposals' not in kwargs
             return self.aug_test(imgs, img_metas, **kwargs)
 
+    @abstractmethod
+    def forward_dummy(self, img, **kwargs):
+        pass
+
     @auto_fp16(apply_to=('img', ))
-    def forward(self, img, img_metas, return_loss=True, **kwargs):
+    def forward(self, img, img_metas=[], return_loss=True, **kwargs):
         """
         Calls either forward_train or forward_test depending on whether
         return_loss=True. Note this setting will change the expected inputs.
diff --git a/mmdet/models/detectors/single_stage.py b/mmdet/models/detectors/single_stage.py
index 78509f3..04d1129 100644
--- a/mmdet/models/detectors/single_stage.py
+++ b/mmdet/models/detectors/single_stage.py
@@ -1,4 +1,5 @@
 import torch.nn as nn
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import bbox2result
 from ..builder import DETECTORS, build_backbone, build_head, build_neck
@@ -85,9 +86,10 @@ class SingleStageDetector(BaseDetector):
         """
         x = self.extract_feat(img)
         outs = self.bbox_head(x)
-        loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
-        losses = self.bbox_head.loss(
-            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
+        with no_nncf_trace():
+            loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
+            losses = self.bbox_head.loss(
+                *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
         return losses
 
     def simple_test(self, img, img_metas, rescale=False):
diff --git a/tools/train.py b/tools/train.py
index 4acd3a4..6a4bfff 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -120,6 +120,12 @@ def main():
     logger.info(f'Distributed training: {distributed}')
     logger.info(f'Config:\n{cfg.pretty_text}')
 
+    if 'nncf_config' in cfg:
+        logger.info('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.ENABLE_COMPRESSION = True
+    else:
+        cfg.ENABLE_COMPRESSION = False
+
     # set random seeds
     if args.seed is not None:
         logger.info(f'Set random seed to {args.seed}, '
-- 
2.17.1

