{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b60d044-b033-4219-af05-b934e34fda4a",
   "metadata": {},
   "source": [
    "# Quantization of TIMM models with NNCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9eecd-ef1c-4b7c-a547-690fc35af318",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to apply INT8 quantization to models in [TIMM](https://github.com/rwightman/pytorch-image-models), which is a prominent repository for computer vision, with NNCF.  \n",
    "It also assumes that OpenVINO™ is already installed and it uses `resnet18` and `mobilenetv2_050` in TIMM for simplicity.  \n",
    "Other models' list can be checked by using `timm.list_models()`.\n",
    "\n",
    "This tutorial consists of the following steps:\n",
    "- Preparation\n",
    "- Set analysis tool\n",
    "- Load TIMM models\n",
    "- Set NNCF config\n",
    "- Export the model to onnx\n",
    "- Run quantization & Check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdaacf1-48e4-47b0-8bcd-4a364f645c13",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814188b-317d-429e-818a-1d2b0d17fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0310c7ef-51e3-4996-bf55-82357d9c2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import create_compressed_model\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786aa6e0-b981-44a1-9755-9e06a24bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc5aa3-6d3b-4bc8-aaef-6582dde1789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_location = './models'\n",
    "os.makedirs(dump_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c75806-96b9-4716-bbea-21181e61e549",
   "metadata": {},
   "source": [
    "## Set analysis tool\n",
    "To obtain performance, [OpenVINO™ Model Analyzer](https://github.com/openvinotoolkit/model_analyzer) will be used.  \n",
    "So, if `model_analyzer` is not in the working directory, it should be cloned in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d13ea-08dc-41d9-ad41-d308fd8d37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone OpenVINO Model Analyzer in the working directory\n",
    "!git clone https://github.com/openvinotoolkit/model_analyzer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44483fbb-398b-432e-96fc-040812fdd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_openvino(model_path):\n",
    "    command_line = 'benchmark_app -m {} -d CPU '.format(model_path)\n",
    "    output = os.popen(command_line).read()\n",
    "\n",
    "    match = re.search(\"Throughput\\: (.+?) FPS\", output)\n",
    "    if match != None:\n",
    "        fps = match.group(1)\n",
    "        return float(fps), output\n",
    "\n",
    "    return None, output\n",
    "\n",
    "def analyze_model(model_path):\n",
    "    command_line = 'python model_analyzer/model_analyzer.py --model {} --ignore-unknown-layer'.format(model_path)\n",
    "    output = os.popen(command_line).read()\n",
    "\n",
    "    match1 = re.search(\"GFLOPs\\: (.+?)\\n\", output)\n",
    "    match2 = re.search(\"GIOPs\\: (.+?)\\n\", output)\n",
    "    if match1 != None and match2 != None:\n",
    "        flops = float(match1.group(1))\n",
    "        iops = float(match2.group(1))\n",
    "        return iops/(flops+iops), output\n",
    "    \n",
    "    return None, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c2671-6456-4ea8-9176-770e49805c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Texttable()\n",
    "table.header([\"Model\", \"Methods\", \"Ops Ratio\", \"FP32 FPS\", \"Opt FPS\", \"Speedup\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889d697-05d6-450e-aabc-35d6e4c83bdf",
   "metadata": {},
   "source": [
    "## Load models\n",
    "To load or check the whole TIMM model(s)'s name, use `timm.list_models()`.  \n",
    "Otherwise, to load the specific TIMM model(s), set the model(s)'s name in List.  \n",
    "For simplicity, in this tutorial, `resnet18` and `mobilenetv2_050` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1dd14-a231-47db-b3e3-03d972ee6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_list():\n",
    "    ## full_list = timm.list_models()\n",
    "    model_list = ['resnet18', 'mobilenetv2_050']\n",
    "    return model_list\n",
    "\n",
    "def create_timm_model(name):\n",
    "    model = timm.create_model(name, num_classes=1000, in_chans=3, pretrained=True, checkpoint_path='')\n",
    "    return model\n",
    "\n",
    "def cleanup(files):\n",
    "    # remove dump files\n",
    "    for file in files:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4a8ad-5571-4b84-b16b-c207c6807eae",
   "metadata": {},
   "source": [
    "## Set NNCF configs\n",
    "For simplicity and quantization only for now, NNCF configs are set briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a128aef-b370-4ff2-a02d-9996690ed03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_nncf(model, save_here):\n",
    "    # Quantize only for now\n",
    "    nncf_config_dict = {\n",
    "        \"input_info\": {\n",
    "        \"sample_size\": [1, 3, 224, 224]\n",
    "        },\n",
    "        \"compression\": {\n",
    "            \"algorithm\": \"quantization\",\n",
    "            'quantize_inputs': True,\n",
    "            'initializer': {\n",
    "                'range': {\n",
    "                    'num_init_samples': 0\n",
    "                },\n",
    "                'batchnorm_adaptation': {\n",
    "                    'num_bn_adaptation_samples': 0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "    compression_ctrl, model = create_compressed_model(model, nncf_config)\n",
    "    compression_ctrl.export_model(save_here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cbaa9-94e8-4e51-a331-42e5f7fa950f",
   "metadata": {},
   "source": [
    "## Export the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3a9ca-be94-41bc-a36c-032591a340ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, save_here):\n",
    "    x = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
    "    torch.onnx.export(model,\n",
    "                      x,\n",
    "                      save_here,\n",
    "                      export_params=True,\n",
    "                      opset_version=13,\n",
    "                      do_constant_folding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0044cf-3176-4254-9a02-a2bd4ebea138",
   "metadata": {},
   "source": [
    "## Run quantization & Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929b512-e2f9-4697-8732-69ad64398116",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = get_model_list()\n",
    "print(\"Optimizing models from the list: {}\".format(model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b8068-8005-4665-a644-097d97ad4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_list:\n",
    "    orig_model_path = os.path.join(dump_location, '{}_fp32.onnx'.format(model_name))\n",
    "    opt_model_path = os.path.join(dump_location, '{}_opt.onnx'.format(model_name))\n",
    "\n",
    "    result = ['N/A'] * len(table._header)\n",
    "    result[0] = model_name\n",
    "    result[1] = 'quantization'\n",
    "\n",
    "    try:\n",
    "        # set timm model\n",
    "        model = create_timm_model(model_name)\n",
    "        \n",
    "        # export the model to onnx\n",
    "        export_to_onnx(model,orig_model_path)\n",
    "        \n",
    "        # quantize the model with NNCF\n",
    "        optimize_with_nncf(model, opt_model_path)\n",
    "\n",
    "        # Analyze quantized model\n",
    "        ops_ratio, ouptut = analyze_model(opt_model_path)\n",
    "        if ops_ratio != None:\n",
    "            result[2] = ops_ratio\n",
    "\n",
    "        # Benchmark original model\n",
    "        orig_model_perf, orig_bench_output = benchmark_with_openvino(orig_model_path)\n",
    "        if orig_model_perf == None:\n",
    "            print(\"Cannot measure performance for original model: {}\\nDetails: {}\\n\".format(model_name, orig_bench_output))\n",
    "            table.add_row(result)\n",
    "            continue\n",
    "\n",
    "        result[3] = orig_model_perf\n",
    "\n",
    "        # Benchmark optimized model\n",
    "        opt_model_perf, opt_becnh_output = benchmark_with_openvino(opt_model_path)\n",
    "        if opt_model_perf == None:\n",
    "            print(\"Cannot measure performance for optimized model: {}\\nDetails: {}\\n\".format(model_name, opt_becnh_output))\n",
    "            table.add_row(result)\n",
    "            continue\n",
    "        result[4] = opt_model_perf\n",
    "        \n",
    "        # Organize performance\n",
    "        speedup = opt_model_perf / orig_model_perf\n",
    "        print(\"Performance gain after applying optimizations to {}: {}\".format(model_name, opt_model_perf / orig_model_perf))\n",
    "\n",
    "        result[5] = '{:.2f}x'.format(speedup)\n",
    "\n",
    "        cleanup([orig_model_path, opt_model_path]) # Comment this to keep the resulted models\n",
    "        \n",
    "    except BaseException as error:\n",
    "        print(\"Unexpected error when optimizing model: {}. Details: {}\".format(model_name, error))\n",
    "\n",
    "    table.add_row(result)\n",
    "\n",
    "print(table.draw())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timm_integration",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
