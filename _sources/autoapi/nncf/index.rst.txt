:py:mod:`nncf`
==============

.. py:module:: nncf

.. autoapi-nested-parse::

   Neural Network Compression Framework (NNCF) for enhanced OpenVINOâ„¢ inference.



Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   api/index.rst
   common/index.rst
   config/index.rst
   data/index.rst
   experimental/index.rst
   quantization/index.rst
   torch/index.rst



Classes
~~~~~~~

.. autoapisummary::

   nncf.NNCFConfig
   nncf.Dataset
   nncf.BackupMode
   nncf.CompressionFormat
   nncf.CompressWeightsMode
   nncf.DropType
   nncf.ModelType
   nncf.PruneMode
   nncf.QuantizationMode
   nncf.SensitivityMetric
   nncf.StripFormat
   nncf.TargetDevice
   nncf.QuantizationPreset
   nncf.OverflowFix
   nncf.IgnoredScope
   nncf.Subgraph



Functions
~~~~~~~~~

.. autoapisummary::

   nncf.strip
   nncf.compress_weights
   nncf.quantize
   nncf.quantize_with_accuracy_control



.. py:function:: strip(model, *, do_copy = True, strip_format = StripFormat.NATIVE, example_input = None)

   Removes auxiliary layers and operations added during the compression process, resulting in a clean
   model ready for deployment. The functionality of the model object is still preserved as a compressed model.

   :param model: The compressed model.
   :param do_copy: If True (default), will return a copy of the currently associated model object. If False,
     will return the currently associated model object "stripped" in-place.
   :param strip format: Describes the format in which model is saved after strip.
   :param example_input: An example input tensor to be used for tracing the model.
   :return: The stripped model.


.. py:class:: NNCFConfig(*args, **kwargs)

   Bases: :py:obj:`dict`\ [\ :py:obj:`str`\ , :py:obj:`Any`\ ]

   Contains the configuration parameters required for NNCF to apply the selected algorithms.

   This is a regular dictionary object extended with some utility functions, such as the ability to attach well-defined
   structures to pass non-serializable objects as parameters. It is primarily built from a .json file, or from a
   Python JSON-like dictionary - both data types will be checked against a JSONSchema. See the definition of the
   schema at https://openvinotoolkit.github.io/nncf/schema/, or by calling NNCFConfig.schema().

   .. py:method:: from_dict(nncf_dict)
      :classmethod:

      Load NNCF config from a Python dictionary. The dict must contain only JSON-supported primitives.

      :param nncf_dict: A Python dict with the JSON-style configuration for NNCF.


   .. py:method:: from_json(path)
      :classmethod:

      Load NNCF config from a JSON file at `path`.

      :param path: Path to the .json file containing the NNCF configuration.


   .. py:method:: register_extra_structs(struct_list)

      Attach the supplied list of extra configuration structures to this configuration object.

      :param struct_list: List of extra configuration structures.


   .. py:method:: get_redefinable_global_param_value_for_algo(param_name, algo_name)

      Some parameters can be specified both on the global NNCF config .json level (so that they apply
      to all algos), and at the same time overridden in the algorithm-specific section of the .json.
      This function returns the value that should apply for a given algorithm name, considering the
      exact format of this config.

      :param param_name: The name of a parameter in the .json specification of the NNCFConfig, that may
        be present either at the top-most level of the .json, or at the top level of the algorithm-specific
        subdict.
      :param algo_name: The name of the algorithm (among the allowed algorithm names in the .json) for which
        the resolution of the redefinable parameter should occur.
      :return: The value of the parameter that should be applied for the algo specified by `algo_name`.


   .. py:method:: schema()
      :staticmethod:

      Returns the JSONSchema against which the input data formats (.json or Python dict) are validated.



.. py:class:: Dataset(data_source, transform_func = None)

   Wrapper for passing custom user datasets into NNCF algorithms.

   This class defines the interface by which compression algorithms
   retrieve data items from the passed data source object. These data items are used
   for different purposes, for example, model inference and model validation, based
   on the choice of the exact compression algorithm.

   If the data item has been returned from the data source per iteration and it cannot be
   used as input for model inference, the transformation function is used to extract the
   model's input from this data item. For example, in supervised learning, the data item
   usually contains both examples and labels. So transformation function should extract
   the examples from the data item.

   A special input key nncf.definitions.NNCF_DATASET_RESET_STATE_KEY can be used by OpenVINO backend to control
   resetting of internal model state between model inferences. This key can be added to a dataset sample input
   dictionary with either `True` or `False` value. With `True` value, the model state will be reset before inference
   on the corresponding sample, and with `False` the state will not be reset.

   :param data_source: The iterable object serving as the source of data items.
   :param transform_func: The function that is used to extract the model's input
       from the data item. The data item here is the data item that is returned from
       the data source per iteration. This function should be passed when
       the data item cannot be directly used as model's input. If this is not specified, then the data item
       will be passed into the model as-is.

   .. py:method:: get_data(indices = None)

      Returns the iterable object that contains selected data items from the data source as-is.

      :param indices: The zero-based indices of data items that should be selected from
          the data source. The indices should be sorted in ascending order. If indices are
          not passed all data items are selected from the data source.
      :return: The iterable object that contains selected data items from the data source as-is.


   .. py:method:: get_inference_data(indices = None)

      Returns the iterable object that contains selected data items from the data source, for which
      the transformation function was applied. The item, which was returned per iteration from this
      iterable, can be used as the model's input for model inference.

      :param indices: The zero-based indices of data items that should be selected from
          the data source. The indices should be sorted in ascending order. If indices are
          not passed all data items are selected from the data source.
      :return: The iterable object that contains selected data items from the data source, for which
          the transformation function was applied.


   .. py:method:: get_length()

      Tries to fetch length of the underlying dataset.
      :return: The length of the data_source if __len__() is implemented for it, and None otherwise.


   .. py:method:: get_batch_size()

      Tries to fetch batch size of the underlying dataset.
      :return: The value of batch_size or _batch_size attributes of the data_source if exist, and None otherwise.



.. py:class:: BackupMode

   Bases: :py:obj:`StrEnum`

   Defines a backup mode for weight compression.
   :param NONE: Stands for original floating-point precision of the model weights.
       In this mode, weights are retained in their original precision without any quantization.
   :param INT8_SYM: Stands for 8-bit integer symmetric quantization without zero point.
       https://github.com/openvinotoolkit/nncf/blob/develop/docs/usage/training_time_compression/other_algorithms/LegacyQuantization.md#symmetric-quantization
   :param INT8_ASYM: Stands for 8-bit integer asymmetric quantization with a typical non-fixed zero point.
       https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md#asymmetric-quantization


.. py:class:: CompressionFormat

   Bases: :py:obj:`StrEnum`

   Describes the format in which the model is saved after weight compression.

   :param DQ: Represents the 'dequantize' format, where weights are stored in low-bit precision,
       and a dequantization subgraph is added to the model. This is the default format for post-training weight
       compression methods.
   :param FQ: Represents the 'fake_quantize' format, where quantization is simulated by applying
       quantization and dequantization operations. Weights remain in the same precision. This format is
       suitable for quantization-aware training (QAT).
   :param FQ_LORA: Represents the 'fake_quantize_with_lora' format, which combines fake quantization
       with absorbable low-rank adapters (LoRA). Quantization is applied to the sum of weights and
       the multiplication of adapters. This makes quantization-aware training (QAT) more efficient in terms of
       accuracy, as adapters can also be tuned and remain computationally affordable during training due to their
       small dimensions.
   :param FQ_LORA_NLS: Represents the 'fake_quantize_with_lora_nls' format, which extends FQ_LORA with elastic
       absorbable low-rank adapters (LoRA). Quantization is applied similarly to FQ_LORA, and utilizing NLS often
       results in better performance for downstream task fine-tuning.


.. py:class:: CompressWeightsMode

   Bases: :py:obj:`StrEnum`

   Defines a mode for weight compression.
   :param INT8_SYM: Stands for 8-bit integer symmetric quantization of all weights.
       Weights are quantized symmetrically without zero point.
       https://github.com/openvinotoolkit/nncf/blob/develop/docs/usage/training_time_compression/other_algorithms/LegacyQuantization.md#symmetric-quantization
   :param INT8_ASYM: The same as INT8_SYM mode, but weights are quantized to a primary precision asymmetrically
       with a typical non-fixed zero point.
       https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md#asymmetric-quantization
   :param INT4_SYM: Stands for a mixed-precision weights quantization with 4-bit integer as a primary precision.
       Weights are quantized to a primary precision symmetrically without zero point.
       All embeddings and the last layer are always compressed to a backup precision, which is INT8_ASYM,
       by default. All others are quantized whether to 4-bit integer or to a backup precision depending on
       criteria and the given ratio.
       https://github.com/openvinotoolkit/nncf/blob/develop/docs/usage/training_time_compression/other_algorithms/LegacyQuantization.md#symmetric-quantization
   :param INT4_ASYM: The same as INT4_SYM mode, but weights are quantized to a primary precision asymmetrically
       with a typical non-fixed zero point.
       https://github.com/openvinotoolkit/nncf/blob/develop/docs/usage/training_time_compression/other_algorithms/LegacyQuantization.md#asymmetric-quantization
   :param NF4: The the same as INT4_SYM mode, but primary precision is NF4 data type without zero point.
   :param INT8: Mode is deprecated and will be removed in future releases. Please use `INT8_ASYM` instead.
   :param MXFP4: MX-compliant FP4 format with E2M1 values sharing group-level E8M0 scale. The size of group is 32.
   :param MXFP8_E4M3: MX-compliant FP8 format with E4M3 values sharing group-level E8M0 scale. The size of group is 32.
   :param FP8_E4M3: A FP8 format with E4M3 values sharing group-level fp16 scale.
   :param FP4: A FP4 format with E2M1 values sharing group-level fp16 scale.
   :param CODEBOOK: Codebook (LUT) quantization format.
   :param CB4_F8E4M3: Codebook (LUT) format with 16 fixed fp8 values in E4M3 format.


.. py:class:: DropType

   Bases: :py:obj:`StrEnum`

   Describes the accuracy drop type, which determines how the accuracy drop between
   the original model and the compressed model is calculated.

   :param ABSOLUTE: The accuracy drop is calculated as the absolute drop with respect
       to the results of the original model.
   :param RELATIVE: The accuracy drop is calculated relative to the results of
       the original model.


.. py:class:: ModelType

   Bases: :py:obj:`StrEnum`

   Describes the model type the specificity of which will be taken into account during compression.

   :param TRANSFORMER: Transformer-based models
       (https://arxiv.org/pdf/1706.03762.pdf)


.. py:class:: PruneMode

   Bases: :py:obj:`StrEnum`

   str(object='') -> str
   str(bytes_or_buffer[, encoding[, errors]]) -> str

   Create a new string object from the given object. If encoding or
   errors is specified, then the object must expose a data buffer
   that will be decoded using the given encoding and error handler.
   Otherwise, returns the result of object.__str__() (if defined)
   or repr(object).
   encoding defaults to sys.getdefaultencoding().
   errors defaults to 'strict'.


.. py:class:: QuantizationMode

   Bases: :py:obj:`StrEnum`

   Defines special modes.
   Currently contains only FP8-related modes (https://arxiv.org/pdf/2209.05433.pdf).

   :param FP8_E4M3: Mode with 4-bit exponent and 3-bit mantissa.
   :param FP8_E5M2: Mode with 5-bit exponent and 2-bit mantissa.


.. py:class:: SensitivityMetric

   Bases: :py:obj:`StrEnum`

   Defines a sensitivity metric for assigning quantization precision to layers. In order to
       preserve the accuracy of the model, the more sensitive layers receives a higher precision.

   :param WEIGHT_QUANTIZATION_ERROR: The inverted 8-bit quantization noise. Weights with highest value
       of this metric can be accurately quantized channel-wise to 8-bit. The idea is to leave these weights in 8bit,
       and quantize the rest of layers to 4-bit group-wise. Since group-wise is more accurate than per-channel,
       accuracy should not degrade.
   :param HESSIAN_INPUT_ACTIVATION: The average Hessian trace of weights with respect to the layer-wise quantization
       error multiplied by L2 norm of 8-bit quantization noise.
   :param MEAN_ACTIVATION_VARIANCE: The mean variance of the layers' inputs
       multiplied by inverted 8-bit quantization noise.
   :param MAX_ACTIVATION_VARIANCE: The maximum variance of the layers' inputs
       multiplied by inverted 8-bit quantization noise.
   :param MEAN_ACTIVATION_MAGNITUDE: The mean magnitude of the layers' inputs
       multiplied by inverted 8-bit quantization noise.


.. py:class:: StripFormat

   Bases: :py:obj:`StrEnum`

   Describes the format in which model is saved after strip: operation that removes auxiliary layers and
   operations added during the compression process, resulting in a clean model ready for deployment.
   The functionality of the model object is still preserved as a compressed model.

   :param NATIVE: Preserves as many custom NNCF additions as possible in the model.
   :param DQ: Replaces FakeQuantize operations with a dequantization subgraph and stores compressed weights
       in low-bit precision using fake quantize parameters. This is the default format for deploying models
       with compressed weights.
   :param IN_PLACE: Directly applies NNCF operations to the weights, replacing the original weights.


.. py:class:: TargetDevice

   Bases: :py:obj:`StrEnum`

   Target device architecture for compression.

   Compression will take into account the value of this parameter in order to obtain the best performance
   for this type of device.


.. py:class:: QuantizationPreset

   Bases: :py:obj:`nncf.parameters.StrEnum`

   An enum with values corresponding to the available quantization presets.


.. py:function:: compress_weights(model, *, mode = CompressWeightsMode.INT8_ASYM, ratio = None, group_size = None, ignored_scope = None, all_layers = None, dataset = None, sensitivity_metric = None, subset_size = 128, awq = None, scale_estimation = None, gptq = None, lora_correction = None, backup_mode = None, compression_format = CompressionFormat.DQ, advanced_parameters = None)

   Compress model weights.

   :param model: A model to be compressed.
   :type model: TModel
   :param mode: Defines a mode for weight compression.
       INT8_SYM stands for 8-bit integer symmetric quantization of all weights without zero point.
       INT8_ASYM is the same as INT8_SYM mode, but weights are quantized to a primary precision asymmetrically
           with a typical non-fixed zero point.
       INT4_SYM stands for a mixed-precision weights quantization with 4-bit integer as a primary precision.
           Weights are quantized to a primary precision symmetrically without zero point.
           All embeddings and the last layer are always compressed to a backup precision, which is INT8_ASYM,
           by default. All others are quantized whether to 4-bit integer or to a backup precision depending on
           criteria and the given ratio.
       INT4_ASYM is the same as INT4_SYM mode, but weights are quantized to a primary precision asymmetrically
           with a typical non-fixed zero point.
       NF4 is the same as INT4_SYM mode, but primary precision is NF4 data type without zero point.
       MXFP4 is MX-compliant FP4 format with E2M1 values sharing group-level E8M0 scale. The size of group is 32.
       MXFP8_E4M3 - is MX-compliant FP8 format with E4M3 values sharing a group-level E8M0 scale.
           The size of group is 32.
       FP8_E4M3 - is FP8 format with E4M3 values sharing a group-level FP16 scale.
       FP4 - is FP4 format with E2M1 values sharing a group-level FP16 scale.
   :type mode: nncf.CompressWeightsMode
   :param ratio: the ratio between baseline and backup precisions (e.g. 0.9 means 90% of layers quantized to NF4
       and the rest to INT8_ASYM).
   :type ratio: float
   :param group_size: number of weights (e.g. 128) in the channel dimension that share quantization parameters (scale).
       The value -1 means no grouping.
   :type group_size: int
   :param ignored_scope: An ignored scope that defined the list of model control
       flow graph nodes to be ignored during quantization.
   :type ignored_scope: nncf.IgnoredScope
   :param all_layers: Indicates whether embeddings and last MatMul layers should be compressed to a primary
       precision. By default, the backup precision is assigned for the embeddings and last MatMul layers.
   :type all_layers: bool
   :param dataset: Dataset used for assigning different quantization precision by finding outliers in activations.
   :type dataset: nncf.Dataset
   :param sensitivity_metric: The sensitivity metric for assigning quantization precision to layers. In order to
       preserve the accuracy of the model, the more sensitive layers receives a higher precision.
   :type sensitivity_metric: nncf.SensitivityMetric
   :param subset_size: Number of data samples to calculate activation statistics used for assigning different
       quantization precision. Defaults to 128.
   :type subset_size: int
   :param awq: Indicates whether use AWQ weights correction.
   :type awq: bool
   :param scale_estimation: Indicates whether a scale estimation algorithm is used that minimizes the L2 error
       between the original and compressed layers.
   :type scale_estimation: bool
   :param gptq: Indicates whether to use GPTQ algorithm.
   :type gptq: bool
   :param lora_correction: Indicates whether to use Lora Correction algorithm.
   :type lora_correction: bool
   :param backup_mode: Defines a backup mode for mixed-precision weight compression.
       NONE stands for original floating-point precision of the model weights.
           In this mode, weights are retained in their original precision without any quantization.
       INT8_SYM stands for 8-bit integer symmetric quantization without zero point.
       INT8_ASYM stands for 8-bit integer asymmetric quantization with a typical non-fixed zero point.
   :type backup_mode: nncf.BackupMode
   :param compression_format: Describes the format in which the model is saved after weight compression.
       Defaults to nncf.CompressionFormat.DQ.
   :type compression_format: nncf.CompressionFormat
   :param advanced_parameters: Advanced parameters for compression algorithms.
   :type advanced_parameters: nncf.AdvancedCompressionParameters
   :return: The non-trainable model with compressed weights.


.. py:function:: quantize(model, calibration_dataset, *, mode = None, preset = None, target_device = TargetDevice.ANY, subset_size = 300, fast_bias_correction = True, model_type = None, ignored_scope = None, advanced_parameters = None)

   Applies post-training quantization to the provided model.

   :param model: A model to be quantized.
   :type  model: TModel
   :param calibration_dataset: A representative dataset for the
       calibration process.
   :type  calibration_dataset: nncf.Dataset
   :param mode: Special quantization mode that specify different ways of the optimization.
   :type mode: Optional[nncf.QuantizationMode]
   :param preset: A preset controls the quantization mode (symmetric and asymmetric).
       It can take the following values:
       - `performance`: Symmetric quantization of weights and activations.
       - `mixed`: Symmetric quantization of weights and asymmetric quantization of activations.
       Default value is None. In this case, `mixed` preset is used for `transformer`
       model type otherwise `performance`.
   :type  preset: nncf.QuantizationPreset
   :param target_device: A target device the specificity of which will be taken
       into account while compressing in order to obtain the best performance
       for this type of device.
   :type  target_device: nncf.TargetDevice
   :param subset_size: Size of a subset to calculate activations statistics used for quantization.
       Must be positive.
   :param fast_bias_correction: Setting this option to `False` enables a different
       bias correction method which is more accurate, in general, and takes
       more time but requires less memory.
   :param model_type: Model type is needed to specify additional patterns
       in the model. Supported only `transformer` now.
   :type  model_type: Optional[nncf.ModelType]
   :param ignored_scope: An ignored scope that defined the list of model control
       flow graph nodes to be ignored during quantization.
   :type  ignored_scope: Optional[nncf.IgnoredScope]
   :param advanced_parameters: Advanced quantization parameters for
       fine-tuning the quantization algorithm.
   :return: The quantized model.
   :rtype: TModel


.. py:function:: quantize_with_accuracy_control(model, calibration_dataset, validation_dataset, validation_fn, *, max_drop = 0.01, drop_type = DropType.ABSOLUTE, preset = None, target_device = TargetDevice.ANY, subset_size = 300, fast_bias_correction = True, model_type = None, ignored_scope = None, advanced_quantization_parameters = None, advanced_accuracy_restorer_parameters = None)

   Applies post-training quantization algorithm with accuracy control to provided model.

   :param model: A model to be quantized.
   :type model: TModel
   :param calibration_dataset: A representative dataset for the calibration process.
   :type calibration_dataset: nncf.Dataset
   :param validation_dataset: A dataset for the validation process.
   :type validation_dataset: nncf.Dataset
   :param validation_fn: A validation function to validate the model. It should take two arguments:
       - `model`: model to be validate.
       - `validation_dataset`: dataset that provides data items to
             validate the provided model.
       The function should return the value of the metric with the following meaning:
       A higher value corresponds to better performance of the model.
   :param max_drop: The maximum accuracy drop that should be achieved after the quantization.
   :param drop_type: The accuracy drop type, which determines how the maximum accuracy
       drop between the original model and the compressed model is calculated.
   :param preset: A preset controls the quantization mode (symmetric and asymmetric).
       It can take the following values:
       - `performance`: Symmetric quantization of weights and activations.
       - `mixed`: Symmetric quantization of weights and asymmetric quantization of activations.
       Default value is None. In this case, `mixed` preset is used for `transformer`
       model type otherwise `performance`.
   :type preset: nncf.QuantizationPreset
   :param target_device: A target device the specificity of which will be taken
       into account while compressing in order to obtain the best performance
       for this type of device.
   :type target_device: nncf.TargetDevice
   :param subset_size: Size of a subset to calculate activations
       statistics used for quantization.
   :param fast_bias_correction: Setting this option to `False` enables a different
       bias correction method which is more accurate, in general, and takes
       more time but requires less memory.
   :param model_type: Model type is needed to specify additional patterns
       in the model. Supported only `transformer` now.
   :type model_type: nncf.ModelType
   :param ignored_scope: An ignored scope that defined the list of model control
       flow graph nodes to be ignored during quantization.
   :type ignored_scope: nncf.IgnoredScope
   :param advanced_quantization_parameters: Advanced quantization parameters for
       fine-tuning the quantization algorithm.
   :param advanced_accuracy_restorer_parameters: Advanced parameters for fine-tuning
       the accuracy restorer algorithm.
   :type advanced_accuracy_restorer_parameters: Optional[AdvancedAccuracyRestorerParameters]
   :return: The quantized model.
   :rtype: TModel


.. py:class:: OverflowFix

   Bases: :py:obj:`nncf.parameters.StrEnum`

   This option controls whether to apply the overflow issue fix for the 8-bit
   quantization.

   8-bit instructions of older Intel CPU generations (based on SSE, AVX-2, and AVX-512
   instruction sets) suffer from the so-called saturation (overflow) issue: in some
   configurations, the output does not fit into an intermediate buffer and has to be
   clamped. This can lead to an accuracy drop on the aforementioned architectures.
   The fix set to use only half a quantization range to avoid overflow for specific
   operations.

   If you are going to infer the quantized model on the architectures with AVX-2, and
   AVX-512 instruction sets, we recommend using FIRST_LAYER option as lower aggressive
   fix of the overflow issue. If you still face significant accuracy drop, try using
   ENABLE, but this may get worse the accuracy.

   :param ENABLE: All weights of all types of Convolutions and MatMul operations
       are be quantized using a half of the 8-bit quantization range.
   :param FIRST_LAYER: Weights of the first Convolutions of each model inputs
       are quantized using a half of the 8-bit quantization range.
   :param DISABLE: All weights are quantized using the full 8-bit quantization range.


.. py:class:: IgnoredScope

   Provides an option to specify portions of model to be excluded from compression.

   The ignored scope defines model sub-graphs that should be excluded from the compression process such as
   quantization, pruning and etc.

   Example:

   ..  code-block:: python

           import nncf

           # Exclude by node name:
           node_names = ['node_1', 'node_2', 'node_3']
           ignored_scope = nncf.IgnoredScope(names=node_names)

           # Exclude using regular expressions:
           patterns = ['node_\\d']
           ignored_scope = nncf.IgnoredScope(patterns=patterns)

           # Exclude by operation type:

           # OpenVINO opset https://docs.openvino.ai/latest/openvino_docs_ops_opset.html
           operation_types = ['Multiply', 'GroupConvolution', 'Interpolate']
           ignored_scope = nncf.IgnoredScope(types=operation_types)

           # ONNX opset https://github.com/onnx/onnx/blob/main/docs/Operators.md
           operation_types = ['Mul', 'Conv', 'Resize']
           ignored_scope = nncf.IgnoredScope(types=operation_types)

   **Note:** Operation types must be specified according to the model framework.

   :param names: List of ignored node names.
   :type names: List[str]
   :param patterns: List of regular expressions that define patterns for names of ignored nodes.
   :type patterns: List[str]
   :param types: List of ignored operation types.
   :type types: List[str]
   :param subgraphs: List of ignored subgraphs.
   :type subgraphs: List[Subgraph]
   :param validate: If set to True, then a RuntimeError will be raised if any ignored scope does not match
     in the model graph.
   :type types: bool


.. py:class:: Subgraph

   Defines the ignored subgraph as follows: A subgraph comprises all nodes along
   all simple paths in the graph from input to output nodes.

   :param inputs: Input node names.
   :type inputs: List[str]
   :param outputs: Output node names.
   :type outputs: List[str]


