:py:mod:`nncf.torch.structures`
===============================

.. py:module:: nncf.torch.structures

.. autoapi-nested-parse::

   PyTorch-specific structure definitions for passing arguments into certain NNCF calls.




Classes
~~~~~~~

.. autoapisummary::

   nncf.torch.structures.QuantizationPrecisionInitArgs
   nncf.torch.structures.AutoQPrecisionInitArgs
   nncf.torch.structures.LeGRInitArgs
   nncf.torch.structures.DistributedCallbacksArgs
   nncf.torch.structures.ExecutionParameters




.. py:class:: QuantizationPrecisionInitArgs(criterion_fn, criterion, data_loader, device = None)

   Bases: :py:obj:`nncf.config.structures.NNCFExtraConfigStruct`

   Stores arguments for initialization of quantization's bitwidth.
   Initialization is based on calculating a measure reflecting layers' sensitivity to perturbations. The measure is
   calculated by estimation of average trace of Hessian for modules using the Hutchinson algorithm.

   :param criterion_fn: callable object, that implements calculation of loss by given outputs of the model, targets,
     and loss function. It's not needed when the calculation of loss is just a direct call of the criterion with 2
     arguments: outputs of model and targets. For all other specific cases, the callable object should be provided.
     E.g. for inception-v3, the losses for two outputs of the model are combined with different weight.
   :param criterion: loss function, instance of descendant of `torch.nn.modules.loss._Loss`,
   :param data_loader: 'data_loader' - provides an iterable over the given dataset. Instance of
     nncf.initialization.PTInitializingDataLoader; a regular 'torch.utils.data.DataLoader' may
     also be passed, but only in the simple case when it returns a tuple of (input, target) tensors.
   .. WARNING:: The final quantizer setup of the created compressed model is dependent on the data
     provided by the data_loader. When using PyTorch's DistributedDataParallel with precision
     initialization, make sure that each process in the distributed group receives the same data
     from the data_loader as the other processes, otherwise the create_compressed_model call may
     create different compressed model objects for each distributed process and the distributed training
     will fail.
   :param device: Device to perform initialization at. Either 'cpu', 'cuda', or None (default); if None, will
     use the device of the model's parameters.


.. py:class:: AutoQPrecisionInitArgs(data_loader, eval_fn, nncf_config)

   Bases: :py:obj:`nncf.config.structures.NNCFExtraConfigStruct`

   :param data_loader: 'data_loader' - provides an iterable over the given dataset. Instance of
     nncf.initialization.PTInitializingDataLoader; a regular 'torch.utils.data.DataLoader' may
     also be passed, but only in the simple case when it returns a tuple of (input, target) tensors.
    .. WARNING:: The final quantizer setup of the created compressed model is dependent on the data
     provided by the data_loader. When using PyTorch's DistributedDataParallel with precision
     initialization, make sure that each process in the distributed group receives the same data
     from the data_loader as the other processes, otherwise the create_compressed_model call may
     create different compressed model objects for each distributed process and the distributed training
     will fail.


.. py:class:: LeGRInitArgs(train_loader, train_fn, val_loader, val_fn, train_optimizer, nncf_config)

   Bases: :py:obj:`nncf.config.structures.NNCFExtraConfigStruct`

   Stores arguments for learning global ranking in pruning algorithm.

   :param train_loader: provides an iterable over the given training (or initialising) dataset.
   :param train_fn: callable for training compressed model. Train model for one epoch or train_steps (if specified) by
     given args: [dataloader, model, optimizer, compression algorithm controller, train_steps number].
   :param val_loader: provides an iterable over the given validation dataset.
   :param val_fn: callable to validate model, calculates pair of validation [acc, loss] by given model and dataloader.
   :param train_optimizer: optional, optimizer for model training.
   :param nncf_config: NNCF config for compression.


.. py:class:: DistributedCallbacksArgs(wrapping_callback, unwrapping_callback)

   Bases: :py:obj:`nncf.config.structures.NNCFExtraConfigStruct`

   A pair of callbacks that is needed for distributed training of the model: wrapping model with wrapping_callback for
   distributed training, and after all training steps unwrapping model to the initial not-distributed state with
   unwrapping_callback.

   :param wrapping_callback: Callback that wraps the model for distributed training with any necessary structure (for
     example, torch.nn.DataParallel or any custom class), returns wrapped model ready for distributed training
   :param unwrapping_callback: Callback for unwrapping the model wrapped with wrapping_callback, returns original model


.. py:class:: ExecutionParameters(cpu_only, current_gpu)

   Parameters that are necessary for distributed training of the model.

   :param cpu_only: whether cpu-only mode is using for training
   :param current_gpu: id of GPU that should be used for training (if only one of all is used)


