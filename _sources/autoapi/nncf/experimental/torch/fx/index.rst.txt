:py:mod:`nncf.experimental.torch.fx`
====================================

.. py:module:: nncf.experimental.torch.fx



Classes
~~~~~~~

.. autoapisummary::

   nncf.experimental.torch.fx.OpenVINOQuantizer



Functions
~~~~~~~~~

.. autoapisummary::

   nncf.experimental.torch.fx.compress_pt2e
   nncf.experimental.torch.fx.quantize_pt2e



.. py:function:: compress_pt2e(model, quantizer, *, dataset = None, awq = False, scale_estimation = False, gptq = False, lora_correction = False, subset_size = 128, ratio = 1, sensitivity_metric = None, advanced_parameters = None)

   Applies Weight Compression to the torch.fx.GraphModule model using provided torch.ao quantizer.

   :param model: A torch.fx.GraphModule instance to be quantized.
   :param quantizer: Torch ao quantizer to annotate nodes in the graph with quantization setups
       to convey the desired way of quantization.
   :param dataset: A representative dataset for the calibration process.
   :param awq: Determines whether to use or not the modified AWQ algorithm.
   :param scale_estimation: Determines whether to use or not scale estimation for 4-bit layers.
   :param gptq: Determines whether to use or not GPTQ algorithm.
   :param lora_correction: Determines whether to use or not LoRA Correction algorithm.
   :param subset_size: Number of data samples to calculate activation statistics used for assigning different
       quantization precision.
   :param ratio: the ratio between baseline and backup precisions (e.g. 0.9 means 90% of layers quantized to NF4
       and the rest to INT8_ASYM).
   :param sensitivity_metric: The sensitivity metric for assigning quantization precision to layers. In order to
       preserve the accuracy of the model, the more sensitive layers receive a higher precision.
   :param advanced_parameters: Advanced parameters for algorithms in the compression pipeline.


.. py:function:: quantize_pt2e(model, quantizer, calibration_dataset, subset_size = 300, fast_bias_correction = True, smooth_quant = False, bias_correction_params = None, smooth_quant_params = None, activations_range_estimator_params = None, weights_range_estimator_params = None, batchwise_statistics = None, fold_quantize = True, do_copy = False)

   Applies post-training quantization to the torch.fx.GraphModule provided model
   using provided torch.ao quantizer.

   :param model: A torch.fx.GraphModule instance to be quantized.
   :param quantizer: Torch ao quantizer to annotate nodes in the graph with quantization setups
       to convey the desired way of quantization.
   :param calibration_dataset: A representative dataset for the
       calibration process.
   :param subset_size: Size of a subset to calculate activations
       statistics used for quantization.
   :param fast_bias_correction: Setting this option to `False` enables a different
       bias correction method which is more accurate, in general, and takes
       more time but requires less memory. None disables the bias correction algorithm.
   :param smooth_quant: Setting this option to `True` enables the SmoothQuant algorithm.
   :param bias_correction_params: Contains advanced parameters for fine-tuning bias correction algorithm.
   :param smooth_quant_params: Contains advanced alpha parameters for SmoothQuant algorithm.
   :param activations_range_estimator_params: Contains parameters for estimating the range
       of activations of the model.
   :param weights_range_estimator_params: Contains parameters for estimating the range
       of weights of the model.
   :param batchwise_statistics: Determines whether quantizer statistics should be calculated
       for each item of the batch or for the entire batch, default is None, which means
       it set True if batch_size > 1 otherwise False.
   :param fold_quantize: Boolean flag for whether fold the quantize op or not. The value is True by default.
   :param do_copy: The copy of the given model is being quantized if do_copy == True,
       otherwise the model is quantized inplace. Default value is False.
   :return: The quantized torch.fx.GraphModule instance.


.. py:class:: OpenVINOQuantizer(*, mode = None, preset = None, target_device = TargetDevice.ANY, model_type = None, ignored_scope = None, overflow_fix = None, quantize_outputs = False, activations_quantization_params = None, weights_quantization_params = None, quantizer_propagation_rule = QuantizerPropagationRule.MERGE_ALL_IN_ONE)

   Bases: :py:obj:`torch.ao.quantization.quantizer.quantizer.Quantizer`

   Implementation of the Torch AO quantizer which annotates models with quantization annotations
   optimally for the inference via OpenVINO.

   :param mode: Defines optimization mode for the algorithm. None by default.
   :param preset: A preset controls the quantization mode (symmetric and asymmetric).
       It can take the following values:
       - `performance`: Symmetric quantization of weights and activations.
       - `mixed`: Symmetric quantization of weights and asymmetric quantization of activations.
       Default value is None. In this case, `mixed` preset is used for `transformer`
       model type otherwise `performance`.
   :param target_device: A target device the specificity of which will be taken
       into account while compressing in order to obtain the best performance
       for this type of device, defaults to TargetDevice.ANY.
   :param model_type: Model type is needed to specify additional patterns
       in the model. Supported only `transformer` now.
   :param ignored_scope: An ignored scope that defined the list of model control
       flow graph nodes to be ignored during quantization.
   :param overflow_fix: This option controls whether to apply the overflow issue
       fix for the 8-bit quantization.
   :param quantize_outputs: Whether to insert additional quantizers right before
       each of the model outputs.
   :param activations_quantization_params: Quantization parameters for model
       activations.
   :param weights_quantization_params: Quantization parameters for model weights.
   :param quantizer_propagation_rule: The strategy to be used while propagating and merging quantizers.
       MERGE_ALL_IN_ONE by default.

   .. py:method:: set_ignored_scope(names = None, patterns = None, types = None, subgraphs = None, validate = True)

      Provides an option to specify portions of model to be excluded from compression.
      The ignored scope defines model sub-graphs that should be excluded from the quantization process.

      :param names: List of ignored node names.
      :param patterns: List of regular expressions that define patterns for names of ignored nodes.
      :param types: List of ignored operation types.
      :param subgraphs: List of ignored subgraphs.
      :param validate: If set to True, then a RuntimeError will be raised if any ignored scope does not match
          in the model graph.


   .. py:method:: annotate(model)

      Adds quantization annotations to the nodes in the model graph in-place.

      :param model: A torch.fx.GraphModule to annotate.
      :return: The torch.fx.GraphModule with updated annotations.


   .. py:method:: validate(model)

      Validates the annotated model before the insertion of FakeQuantizers / observers.

      :param model: Annotated torch.fx.GraphModule to validate after the  annotation.


   .. py:method:: transform_for_annotation(model)

      Allows for user defined transforms to run before annotating the graph.
      This allows quantizer to allow quantizing part of the model that are otherwise not quantizable.
      For example quantizer can
      a) decompose a compound operator like scaled dot product attention,
      into bmm and softmax if quantizer knows how to quantize bmm/softmax but not sdpa
      or b) transform scalars to tensor to allow quantizing scalars.

      Note: this is an optional method

      :param model: Given torch.fx.GraphModule to transform before the annotation.
      :return: The transformed torch.fx.GraphModule ready for the annotation.



